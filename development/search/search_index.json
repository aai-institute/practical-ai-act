{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#_1","title":"Home","text":"<p>Welcome to the trustworthy-AI pipeline (twai) project!</p> <p>The introduction of the EU AI Act adds a new dimension of requirements to the implementation of high-risk artificial intelligence systems. At first glance, the legal text may not clearly outline how these compliance requirements affect the practical aspects of AI system development.</p> <p>This project aims to bridge that gap by offering valuable insights and practical examples to help accelerate the integration of compliance requirements into real-world projects. It is designed as an entry point for professionals approaching the AI Act from a technical perspective. A key message we want to convey is that adhering to software engineering best practices already provides a strong foundation for achieving compliance.</p> <p>To support this goal, the project will present an exemplary system architecture for a trustworthy AI system, offer concrete implementations of essential software components, and establish clear connections between engineering best practices and the specific requirements for high-risk systems under the AI Act.</p> <p>This is a living document and will be continuously updated as more legal and technical information becomes available. Your feedback and discussions are highly encouraged, as they will help us refine the content and ensure its relevance to the community.</p> <ul> <li> <p> Reader's Guide</p> <p>Find out how to navigate the resources on this website</p> </li> <li> <p> Showcase</p> <p>A worked end-to-end ML pipeline for a (hypothetical) high-risk AI system</p> </li> <li> <p> Engineering Practice</p> <p>Engineering techniques for trustworthy AI systems</p> </li> <li> <p> AI Act Conformity</p> <p>Background information regarding the EU AI Act from a technical view point</p> </li> </ul>"},{"location":"acknowledgments/","title":"Acknowledgments","text":"<p>TODO</p> <ul> <li>appliedAI Institute funding note</li> <li>Project funding note</li> </ul>"},{"location":"readers-guide/","title":"Reader's Guide","text":"<p>Legal Disclaimer</p> <p>The information provided on this website is for informational purposes only and does not constitute legal advice. The tools, practices, and mappings presented here reflect our interpretation of the EU AI Act and are intended to support understanding and implementation of trustworthy AI principles. Following this guidance does not guarantee compliance with the EU AI Act or any other legal or regulatory framework.</p> <p>We are not affiliated with, nor do we endorse, any of the tools listed on this website.</p>"},{"location":"readers-guide/#who-is-this-website-for","title":"Who is this website for?","text":"<p>This project is intended for AI practitioners, data scientists, and engineers who are interested in implementing trustworthy AI systems under the European AI Act.</p> <p>The information is applicable to low- and high-risk AI systems, since it provides a sound engineering foundation for building trustworthy AI systems in general.</p>"},{"location":"readers-guide/#syllabus","title":"Syllabus","text":"<p>The project is structured into three key areas, each addressing different aspects of AI system development and compliance with the EU AI Act. The content is not meant to be consumed in linear order.</p> <ul> <li>Showcase: This section introduces a demonstrational use-case that we use to develop the content. It also includes a risk classification according to the EU AI Act.</li> <li>Engineering Practice: Following software engineering best practice lay the foundation for achieving compliance.</li> <li>AI Act Conformity: Background information connecting technical artifacts to the obligations of the AI Act</li> </ul>"},{"location":"readers-guide/#what-is-out-of-scope","title":"What is out of scope?","text":""},{"location":"readers-guide/#assessing-your-use-cases-risk-category","title":"Assessing your use case's risk category","text":"<p>If you are unsure how the AI Act applies to your use case, you should first try to determine the appropriate risk classification for your use case.</p> <p>The following resources can help you with that:</p> <ul> <li>The appliedAI Institute Risk Classification Database, a comprehensive list of practical examples of high-risk and non high-risk use cases on AI systems under the EU AI Act</li> <li>The EU Act Compliance Checker, an interactive tool that helps you assess the risk category of your AI system and applicable requirements from the AI Act</li> </ul>"},{"location":"readers-guide/#systems-covered-under-european-harmonized-standards","title":"Systems covered under European Harmonized Standards","text":"<p>If your system is covered under European Harmonized Standards (see Annex I), you should refer to the relevant standards for guidance on compliance.</p>"},{"location":"readers-guide/#general-purpose-ai-systems","title":"General-purpose AI systems","text":"<p>If you are building a general-purpose AI system, you will still find the engineering resources useful, since they mirror good software engineering practices.</p> <p>However, the compliance resources are specifically tailored to the requirements for high-risk AI systems under Chapter III of the AI Act.</p> <p>For guidance on how to comply with Chapter V of the AI Act, you might find the following resources helpful:</p> <ul> <li>The European AI Office, which oversees the implementation of the AI Act, while ensuring compliance, fostering innovation, and coordinating AI governance across EU Member States.</li> <li>The General-Purpose AI Code of Practice, and an explorer for the draft Code of Practice.</li> </ul>"},{"location":"readers-guide/#relationship-to-privacy-and-data-protection","title":"Relationship to Privacy and Data Protection","text":"<p>The project does not cover the relationship between the AI Act and the General Data Protection Regulation (GDPR), or the question of privacy in Machine Learning more broadly.</p> <p>The AI Act and the GDPR are separate legal frameworks, and while they may overlap in some areas, they have different objectives and requirements. Although some of the engineering practices on this website also help you comply with the GDPR, we do not explicitly consider these privacy and data protection aspects.</p>"},{"location":"readers-guide/#recommended-knowledge","title":"Recommended Knowledge","text":"<ul> <li>A basic understanding of machine learning and AI concepts</li> <li>An understanding of the terminology in Art. 3 of the AI Act</li> <li>Familiarity with Python programming, if you want to follow along with the code examples</li> <li>Software engineering best practices for ML:<ul> <li>See the Beyond Jupyter series for an introduction</li> </ul> </li> </ul>"},{"location":"conformity/documents/","title":"Documents to be submitted","text":"<ul> <li>Technical Documentation</li> <li>Instructions for Use</li> </ul>"},{"location":"conformity/documents/instructions-for-use/","title":"Transparency and Provision of Information","text":"<p>To get a more hands-on reading of Article 13, we shall assume, that there is a fix use-case to solve and the input data have a specified format (does this makes sense?).</p> <p>The first thing to notice is, that information must be provided to the user even on an event level. This means, that there must be an identifier for each event (inference call, failure, ...), which can be used to trace back the collected information. This is tightly connected to Article 12, which handles the requirements regarding record-keeping. We can elaborate on this by showing a simple event-based architecture using databases to persist the information and make them available to the user via an API.</p> <p>Let's consider which kind of information must be provided to the user (in the context of the AI act this is the deployer). The requirements described in Article 13 of the EU AI Act can be broadly seperated into two categories. The first category is static information about the system, which is not input dependent. The second category is the information about the output for a specific input (this is a point of discussion!).</p>"},{"location":"conformity/documents/instructions-for-use/#static-information-about-the-system","title":"Static information about the system","text":"<p>This includes all information about the system, which is not input dependent. There seem to be quite an overlap with Hugging Face model cards.</p> <p>The static information can be further subdivided into those which are model-independent and those which are model-dependent.</p>"},{"location":"conformity/documents/instructions-for-use/#model-independent-information","title":"Model-independent information","text":"<p>All information, which is not specific for a single model or may be stable over a long time:</p> <ul> <li> <p>identity and contact details of provider; Who is responsible for the system?</p> <ul> <li>13.3 a</li> <li>This does not need much of an explanation, but we can include it in our showcase implementation as well.</li> </ul> </li> <li> <p>intended purpose of a system; Which problem will be solved?</p> <ul> <li>13.3 b(i)</li> <li>This needs clarification about how detailed the description of the system   has to be and which language (in the sense of using machine learning terminology) should be used. We can provide an example for the simple case of classification and regression with the census data.</li> <li>A still vague definition is given in article 3.12.</li> </ul> </li> <li> <p>input data scheme; How to get an output of the model?</p> <ul> <li>13.2, 13.3 b(vi)</li> <li>Although this is not mentioned explicitly, it belongs to the general instructions of use; it has to be clear and accessible to the user, how the input schema has to look like to run inference of the model.</li> <li>Again, this information can be made accessible via an API, which provides the schema of the input data, and we can easily showcase this.</li> </ul> </li> </ul>"},{"location":"conformity/documents/instructions-for-use/#model-dependent-information","title":"Model-dependent information","text":"<ul> <li> <p>description of the model; What kind of model is used?</p> <ul> <li>13.3 b(iv)</li> <li>this includes the description of the model type/architecture, hyperparameters and preprocessing. Here the task will be to show how to make all this information amenable for logging, e.g. automatically create a string representation.</li> </ul> </li> <li> <p>evaluation metrics for the model to show the performance to expect</p> <ul> <li>13.3 b(ii)</li> <li>I think which metrics (in the mathematical sense) to use for which ml problem   is pretty standard, e.g. accuracy, precision, recall, F1-score for a binary classification; Maybe it is best to not elaborate on this too much but reference a   good source for this?</li> <li>More important than the metrics themselves is the question of how the evaluation   was done, e.g. how the data was split or if cross-validation was used. This   is also tightly connected to the information about the training data. There   should be transparency about how to reproduce the evaluation result.</li> </ul> </li> <li> <p>statistics about restricted performance, e.g. subgroup performance;</p> <ul> <li>13.3 b(v)</li> <li>This is a point of discussion. We should explain in which scenario an isolated   view on a specific subgroup (may it be a group having a specific feature or   in a binary problem looking only at the positive labeled group, i.e. precision and recall) makes sense.</li> <li>We should make the connection to Article 11 and Annex IV</li> </ul> </li> <li> <p>information about the training data;</p> <ul> <li>13.3 b(vi)</li> <li>this information might be about the data collection process, any data transformation, basically the information collected from the data governance view point. Here it might be more challenging to find a good format for providing it   to the user, but maybe it also makes sense to make it available via an API?</li> <li>We should make the connection to Article 11 and Annex IV</li> </ul> </li> </ul>"},{"location":"conformity/documents/instructions-for-use/#information-about-the-output-for-a-specific-input","title":"Information about the output for a specific input","text":"<p>In contrast to the information discussed so far, this includes all information which is specific to a single input. * logs for a specific inference call;     * 13.3 f     * based on an event-based architecture, we can provide a simple API to access the logs for a specific inference call and an instruction how to use the endpoints should be available.     * again, this is tightly connected to article 12 and the record-keeping requirements. * additional information, which allow for interpretation of the output.     * 13.3 b(vii), 13.3 d (connection to human oversight)     * the AI act is extremely vague in regard to this point. It uses phrase like       \"where applicable\" and \"enable deployers to interpret the output\".     * We should elaborate on how to make this more concrete. One way could be to speak       about the terms interpretability and explainability how they are used in the technical community, i.e. a discussion about intrinsically interpretable models and post-hoc interpretability methods for black-box models.</p>"},{"location":"conformity/documents/instructions-for-use/#open-questions","title":"Open questions","text":"<ul> <li>since explainability (XAI) and interpretability are not mentioned explicitly in article 13, but might be helpful to enable the user to handle the output (\"interpret/ explain the output\") in a better way, should we include it in our explanations and showcase?<ul> <li>I think we can keep the explanation part short and reference to a good source for this.</li> <li>It should be included into the showcase, because the costs to actually implement it are not high and the information gain for the user is high.</li> <li>it is not clear how to deal with the terms \"where applicable\" and \"when appropriate\".</li> </ul> </li> <li>what part of the information collected for data governance should be made available to the user and how?<ul> <li>Can Hugging Face model cards be used as a template for this?</li> </ul> </li> <li>the term \"reasonably foreseeable misuse\" is used in article 13.b (iii) and defined in article 3.13.<ul> <li>In short, any possible not intended use due to the fact that humans are humans. E.g. they are lacking skills to use the system appropriately, I think of the typical it-support scenario or the user has evil intentions.</li> <li>Still, it is unclear how to include it in our showcase. Besides the explainability part, this will is the biggest open question.</li> </ul> </li> <li>Art. 13(3), (b)(iv): Get a sense for \"technical characteristics and capabilities\" - what's the intention and rationale behind this phrasing?</li> <li>What is the connection between Article 11, Annex IV and Article 13.3 b(v, vi)? Is it instruction for use vs. technical documentation (what is the difference)?</li> <li>The meaning and intention of Article 13.3 c is completely unclear (is this something like a \"diff\")</li> <li>What is the difference between the terms \"misuse\", \"abuse\" and \"attack\" in the context of the AI act?  </li> </ul>"},{"location":"conformity/documents/instructions-for-use/#potential-tasks","title":"Potential tasks","text":"<ul> <li>Set up an event logging system and document its architecture in line with Art. 13(3), (f) and Art. 12</li> <li>starting from our use-case, think about potential misuses.<ul> <li>e.g. use the model for a different age span than in the training,   so the model will still give an output, but it is not semantically meaning full.</li> </ul> </li> <li>set up an automatic way to build the (provenance) information about the model, preprocessing and training data.</li> <li>the goal should be to create transparency and traceability about the complete processing pipeline</li> <li>e.g. sklearn pipeline steps documented included parameterization</li> <li>implement a basic API to provide the static information about the system.</li> <li>implement the usage of intrinsically explainable models and/or usage of post-hoc methods, have a look at the interpret package.</li> </ul> <p>Related Norms: * ISO/IEC FDIS 12792 (Transparency taxonomy of AI systems) * ISO/IEC TS 4213 * ISO/IEC AWI 4213 * ISO/IEC DTS 6254</p>"},{"location":"conformity/documents/technical-documentation/","title":"Technical Documentation","text":"<p>Compliance Info</p> <pre><code>|Art. 11|\n|Art. 10(2)(b)|\n</code></pre> <p>Providers of high-risk AI systems are required to prepare detailed technical documentation before placing the system on the market. This documentation must be kept up to date and should demonstrate the system's compliance with the AI Act's requirements. It should include a general description of the AI system, its intended purpose, design specifications, and information on performance evaluation. Small and medium-sized enterprises (SMEs) may provide this information in a simplified form, as specified by the EU.</p>"},{"location":"conformity/documents/technical-documentation/#data-documentation","title":"Data Documentation","text":"<p>See also below, under \"Dataset Information\"</p> <ul> <li>Data selection process</li> <li>Quality improvement measures</li> <li>Data owners</li> <li>Description</li> <li>Classification (applicable glossary terms)</li> <li>Fields (name, type, description, classification) for tabular data</li> <li>Data properties (e.g., format, resolution) for non-tabular data</li> </ul> <p>Art. 10(2)(a) the relevant design choices;</p> <p>Any specific decisions made during the design and development of a high-risk AI system must be documented. Such decisions can include:</p> <ul> <li>Architecture specific: These are choices related to the architecture, algorithms, data handling, and setup of the AI System. Depending on the architecture, different preprocessing techniques might need to be used.</li> <li>Impact on performance: These choices influence how well the AI system performs. Techniques can be applied to increase accuracy, reliability, or to reduce bias.</li> <li>Compliance-specific decisions: These choices are made to ensure compliance with laws and ethical guidelines. For example, anonymizing PIIs.</li> </ul> <p>Ensuring Data Relevance</p> <p>To ensure the dataset is relevant to the AI system's intended purpose, consider the following:</p> <ul> <li> <p>Clearly Define the AI System's Purpose</p> <ul> <li>What problem is the AI solving?</li> <li>Who are the end-users?</li> <li>What decisions will be influenced by the AI?</li> </ul> </li> <li> <p>Specify Key Attributes &amp; Variables</p> <ul> <li>Identify the critical features required for accurate predictions.</li> <li>Ensure the data captures the necessary demographic, contextual, or domain-specific factors.</li> </ul> </li> <li> <p>Check for Domain Alignment</p> <ul> <li>Ensure the data is collected from sources that reflect the real-world environment of deployment.</li> </ul> </li> <li> <p>Assess Temporal &amp; Geographic Relevance</p> <ul> <li>Is the data up-to-date?</li> <li>Is it from the correct geographical region?</li> </ul> </li> <li> <p>Validate Coverage of Target Population</p> <ul> <li>Ensure the dataset represents the groups the AI will serve.</li> <li>Avoid underrepresentation of critical demographics.</li> </ul> </li> <li> <p>Assess Data Labeling and Context</p> <ul> <li>Verify if the labels or classifications align with domain expertise.</li> <li>Ensure human annotations are validated by experts where necessary.</li> </ul> </li> <li> <p>Establish data lineage (i.e., define upstream or downstream data sets) on data set level or on column level for tabular data.     Before model training, data often undergoes various preprocessing steps. For example, datasets may be created by merging multiple datasets or combining different features. Simply documenting the final dataset is insufficient, as it does not allow the origin of the data to be traced. Proper documentation of data lineage is essential to maintain traceability and accountability.</p> <ul> <li>Define lineage in a data catalog, or have it automatically represented in a workflow/data orchestrator<ul> <li>Keep records of the data lifecycle, including the sources of data, selection criteria, and preprocessing steps (all steps to model training).</li> </ul> </li> </ul> </li> </ul> <p>Art. 10(2)(b) data collection processes and the origin of data, and in the case of personal data, the original purpose of the data collection;</p> <p>From the documentation, it shall be understood:</p> <ul> <li>How the data is collected. Which methods and procedures where used to gather the data (survey, sensor, scraping, etc.)? Which conditions apply to the data collection (consent, regulatory compliance)? How does it align with best practices and ethical standards.</li> <li>What is the source of the data? Is it publicly available, proprietary, collected from third parties (e.g., via an API)? What are the geographical, cultural, and demographic origins of the data?</li> <li> <p>To ensure compliance with the GDPR, personal data can only be collected for a specific purpose and cannot be repurposed.</p> </li> <li> <p>Establish an organization-wide glossary index</p> <ul> <li>Define terms (including definition/description) and use those to annotate data set and fields in the data sets. Single source of truth for interpreting data sets and fields.</li> </ul> </li> </ul> <p>Art. 10(2)(c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and aggregation;</p> <p>The documentation of all data preparation is essential for traceability and transparency. See the point above on implementing data lineage.</p> <p>Being specific and precise when documenting datasets is essential to minimize the risk of misinterpretation, both of the documentation itself and when using the data. A common approach to reduce ambiguity is to create a company-wide glossary that provides detailed definitions for terms. Datasets or fields linked to a glossary term should adhere strictly to the defined meaning, ensuring consistency and clarity.</p> <p>(d) the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and represent;</p> <p>The article mandates that any assumptions made are explicitly documented. The goal is to capture how the data aligns with the purpose of the AI system.</p> <ul> <li>Understanding what the data represents: Identify and define specific concepts that the data is intended to measure. Especially important when data used as a proxy (e.g., income as a proxy for economic status).</li> <li>Assumptions: Define the beliefs, whether they are implicit or explicit, about the dataset. This may include assumptions about the accuracy, scope, and relevance of the dataset.</li> <li>Purpose alignment: Verify that the understanding (definitions) and the assumptions align with the intended use of the AI system.</li> <li>Misinterpretation: Recognize and document any (potential) limitations (e.g., underrepresented classes).</li> </ul> <p>(e) an assessment of the availability, quantity and suitability of the data sets that are needed;</p> <p>This assessment ensures that the data meets the quality standards such that it can be used.</p> <ul> <li>Availability: Does the data exist? Is the data accessible (both technically and from the perspective of access rights)? Are there any practical, legal, or ethical restrictions that have to be resolved before accessing the data.</li> <li>Quantity: Assess whether the dataset contains enough data points for training and validation. Is the data set large enough that is represents all relevant groups or scenarios (i.e., system boundaries)?</li> <li>Suitability: Is the data representative of the target population and for the application/use? Is the data quality and accuracy fitting the goals of the AI System?</li> </ul>"},{"location":"conformity/documents/technical-documentation/#model-cards","title":"Model cards","text":"<p>A good model card is a structured document that provides clear, concise, and comprehensive information about a machine learning model:</p> <ul> <li>Model Overview:<ul> <li>Model Name and Version</li> <li>Model Description: A brief overview of the model's purpose, capabilities, and intended use.</li> <li>Contact Information: Details of the organization, department, team or individual responsible for the model, including contact information.</li> </ul> </li> <li>Intended Use:<ul> <li>Primary Use Cases: Description of the specific tasks the model is designed for.</li> <li>Out-of-Scope Use Cases: Explicitly state where the model should not be used</li> </ul> </li> <li>Dataset Information:<ul> <li>Training Data: Details of the dataset used to train the model, including its source, size, and key characteristics.</li> <li>Validation and Test Data: Information about datasets used for validation and testing (similar to training data).</li> <li>Preprocessing: Description of any preprocessing steps applied to the data.<ul> <li>normalization, encoding, handling missing values, feature engineering, etc.</li> </ul> </li> </ul> </li> <li>Performance Metrics:<ul> <li>Overall Performance: See metrics for Accuracy</li> <li>Subgroup Performance: Performance metrics broken down by demographic or contextual subgroups (e.g., age, gender, race, etc.).</li> <li>Benchmarks: Comparison against other models (or older versions of the model)</li> </ul> </li> <li>Fairness and Bias Analysis:<ul> <li>Evaluation: Results of bias testing across demographic groups, including fairness metrics used</li> <li>Mitigation: Actions taken to address identified biases in the model or training data.</li> </ul> </li> <li> <p>Limitations:</p> <ul> <li>Known Limitations: Description of scenarios where the model may not perform well or could produce unreliable results.</li> <li>Uncertainties: Aspects of the model's behavior that are not well understood or tested.</li> </ul> </li> <li> <p>Ethical Considerations:</p> <ul> <li>Potential Harms: Risks or harms that may arise from misuse or unintended use of the model.</li> <li>Privacy Concerns: Details on how the model handles sensitive data, compliance with GDPR.</li> </ul> </li> <li>Risk Management:<ul> <li>Risk Assessment: Identification of risks associated with the model and measures taken to mitigate them.</li> <li>Fail-Safes and Controls: Mechanisms for monitoring and managing model outputs, including fallback procedures.</li> </ul> </li> <li>Technical Specifications:<ul> <li>Model Architecture: A description of the underlying algorithm or architecture.</li> <li>Input and Output: Details of the expected input formats and output types</li> <li>Dependencies: Required software, libraries, or hardware for using the model.</li> </ul> </li> <li>Transparency and Explainability:<ul> <li>Explainability Techniques: Methods used to make the model's decision-making interpretable (e.g., SHAP, LIME).</li> <li>Interpretation Guidelines: Instructions for understanding and using model outputs responsibly.</li> </ul> </li> <li>Maintenance and Updates:\ud83c\udf4f<ul> <li>Update Schedule: Information about planned updates or retraining of the model.</li> <li>Changelog: A log of changes made to the model, datasets, or documentation over time.</li> <li>Use case dependent: what are example use cases? Can you group tasks?</li> </ul> </li> <li>Compliance Information:<ul> <li>Regulatory Compliance: Statement of compliance with relevant regulations (e.g., the EU AI Act, GDPR)</li> <li>Standards and Certifications: Details of standards followed (e.g., Code of Practice)</li> </ul> </li> <li>Usage Guidelines:<ul> <li>Installation and Deployment: Steps for deploying and using the model in various environments.</li> <li>Monitoring and Evaluation: Recommendations for ongoing performance monitoring and evaluation.</li> <li>Decommissioning: Guidance for safely retiring the model when it's no longer in use. State when the model has to be decommissioned.</li> </ul> </li> <li>Licensing:<ul> <li>Usage License: The terms under which the model can be used, modified, or distributed.</li> <li>Third-Party Content: Attribution and licensing for any third-party datasets, libraries, or tools used.</li> </ul> </li> </ul>"},{"location":"conformity/documents/technical-documentation/#annex-iv-technical-documentation-details","title":"Annex IV: Technical Documentation Details","text":"<p>Annex IV provides a comprehensive list of elements that must be included in the technical documentation referred to in Article 11. This includes detailed descriptions of the AI system's design specifications, algorithms, training data sets, risk management systems, validation and testing procedures, performance metrics, and cybersecurity measures. The annex ensures that all relevant information is available to assess the system's compliance with the AI Act.</p>"},{"location":"conformity/documents/technical-documentation/#readings","title":"Readings:","text":"<ul> <li>Datasheets for Datasets</li> <li>The Data Cards Playbook</li> </ul>"},{"location":"engineering-practice/accuracy/","title":"Accuracy","text":"<p>This article requires that high-risk AI systems be designed to achieve appropriate levels of accuracy, robustness, and cybersecurity. Systems should perform consistently throughout their lifecycle, be resilient to errors and faults, and have measures in place to mitigate risks associated with cybersecurity threats. The accuracy of AI systems should be declared in their instructions, and systems should be designed to minimize risks of producing biased outputs.</p>"},{"location":"engineering-practice/accuracy/#continuous-testing-and-benchmarking-of-models","title":"Continuous testing and benchmarking of models","text":"<ul> <li>Regularly test datasets to validate their quality, accuracy, and relevance to the AI system's intended use.<ul> <li>Data quality:<ul> <li>Missing Values Analysis: Identify and handle missing or incomplete data using imputation, removal, or domain-specific methods.</li> <li>Outlier Detection: Use statistical or machine learning techniques to detect and address outliers (e.g., Z-scores).</li> <li>Duplicate Records: Regularly check for and remove duplicate entries to maintain dataset integrity.</li> <li>Noise Reduction: Identify noisy or erroneous data points using heuristics, domain knowledge, or automated tools.</li> </ul> </li> <li>Relevance:<ul> <li>Feature Relevance Analysis: Perform feature selection or importance ranking to ensure all features contribute meaningfully to the AI model. (SHAP)</li> <li>Temporal Relevance: Test datasets periodically to ensure they remain current and relevant (e.g., avoid outdated information).</li> </ul> </li> <li>Drift detection:<ul> <li>Concept Drift: Regularly test for shifts in the relationship between input features and target labels over time.</li> <li>Data Distribution Drift: Monitor changes in feature distributions compared to baseline distributions<ul> <li>Kolmogorov-Smirnov test</li> <li>Earth Mover\u2019s Distance</li> </ul> </li> </ul> </li> </ul> </li> <li>Metrics:<ul> <li>Regression:<ul> <li>Mean Absolute Error (MAE)</li> <li>Median Absolute Error (MedAE)</li> <li>Mean Squared Error (MSE)</li> <li>Root Mean Squared Error (RMSE)</li> <li>R-Squared</li> </ul> </li> <li>Classification:<ul> <li>Accuracy (Balanced accuracy)</li> <li>Logarithmic Loss</li> <li>Precision</li> <li>Recall</li> <li>F1-score</li> <li>Area Under the ROC Curve (AUC-ROC)</li> </ul> </li> </ul> </li> <li>Tools:<ul> <li>Great Expectations, Pandas, Evidently AI, NannyML</li> </ul> </li> <li>Ensure that datasets are updated as necessary to reflect current conditions or contexts.</li> </ul>"},{"location":"engineering-practice/accuracy/#reading","title":"Reading","text":"<p>From the book Trustworthy Machine Learning</p> <ul> <li>Chapter 5.1</li> </ul>"},{"location":"engineering-practice/containerization/","title":"Containerization","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <p>TODO: This list is incomplete</p> <ul> <li>Art. 15 (Accuracy, Robustness and Cybersecurity), in particular:<ul> <li>Art. 15(1)</li> <li>Art. 15(4), containers provide a consistent runtime environment, allow for redundancy</li> </ul> </li> </ul>","tags":["robustness","deployment"]},{"location":"engineering-practice/containerization/#motivation","title":"Motivation","text":"<p>Deploying and running applications in a reproducible and reliable manner is a crucial part of the software development lifecycle. Inconsistencies in the runtime environment can lead to unexpected behavior, bugs, and security vulnerabilities. AI systems prove no exception to this rule, as they often rely on complex dependencies and configurations that can vary across different runtime environments.</p> <p>Containerization provides a solution to these challenges by encapsulating applications and their runtime dependencies into isolated environments, called containers.</p> <p>By controlling the runtime environment, containerization can improve the resilience of AI systems against inconsistencies, as required by Art. 15(4).</p> <p>Since containers provide a uniform execution model, monitoring their lifecycle and performance is easier. Failed containers can be restarted automatically, and their logs can be collected and analyzed to identify potential issues, in line with the fault tolerance requirements of the AI Act.</p> <p>Scaling of AI systems is also simplified with containerization, as containers can be easily replicated and distributed across multiple nodes. This addresses the redundancy requirements of Art. 15(4), as multiple instances of a containerized application can be run in parallel to ensure high availability and fault tolerance.</p>","tags":["robustness","deployment"]},{"location":"engineering-practice/containerization/#implementation-notes","title":"Implementation Notes","text":"","tags":["robustness","deployment"]},{"location":"engineering-practice/containerization/#applicability","title":"Applicability","text":"<p>Containerization can be applied at multiple stages of the machine learning lifecycle, including:</p> <ul> <li>MLOps infrastructure: tooling for data versioning, model training, and deployment</li> <li>Data processing: running data pipelines in isolated environments</li> <li>Model training: training models in isolated environments</li> <li>Model serving: deploying models as microservices</li> </ul>","tags":["robustness","deployment"]},{"location":"engineering-practice/containerization/#traceability-and-reproducibility","title":"Traceability and Reproducibility","text":"<p>Special care should be exercises to ensure the traceability and reproducibility of the containerized applications by always using versioned container images, if possible with immutable version tags or image digests (e.g., <code>my-image:1.0.0</code> or even <code>my-image@sha256:...</code>, instead of <code>my-image:latest</code> or <code>my-image</code>). This practice, akin to version locking for package dependencies, ensures that the same version of the container image is used for each deployment, reducing the risk of inconsistencies, especially in distributed environments.</p> <p>Avoid updating existing tags when building container images, as this can lead to confusion and inconsistencies.</p> <p>Similarly, container builds should be as reproducible as possible. The following techniques can help you to achieve this goal:</p> <ul> <li>Use versioned base images, ideally pinning to a specific image digest.</li> <li>Specify exact versions of dependencies (e.g., by using lock files or version constraints, if supported by your package manager).</li> <li>Avoid using <code>latest</code> tags for dependencies, as they can lead to unexpected changes in behavior.</li> <li>Use build arguments to parameterize the build process, if needed.</li> <li>Use multi-stage builds to separate build and runtime dependencies, if applicable.</li> <li>Use labels/annotations to add metadata to the image, e.g., the source code revision identifier, build date, and maintainers (the OpenContainers annotations spec provides guidance on predefined annotation keys).</li> </ul>","tags":["robustness","deployment"]},{"location":"engineering-practice/containerization/#security-considerations","title":"Security Considerations","text":"<p>While this document does not cover security aspects of containerization in detail, it is important to consider the following points:</p> <ul> <li>Use minimal base images to reduce the attack surface.</li> <li>Use container vulnerability scanning tools to identify known vulnerabilities in the base images and dependencies.</li> <li>Regularly update base images and dependencies to patch known vulnerabilities.</li> <li>Minimize runtime privileges by using non-root users and limiting container capabilities.</li> </ul>","tags":["robustness","deployment"]},{"location":"engineering-practice/containerization/#key-technologies","title":"Key Technologies","text":"<p>Container Engines:</p> <ul> <li>Docker</li> <li>Podman</li> </ul> <p>Container Orchestration:</p> <ul> <li>Docker Compose for small-scale deployments</li> <li>Kubernetes or OpenShift for large-scale, multi-node deployments</li> </ul>","tags":["robustness","deployment"]},{"location":"engineering-practice/containerization/#container-security","title":"Container Security","text":"<p>Note</p> <p>This list only includes open-source software.</p> <p>The field of container security is rapidly evolving, and new tools and techniques are constantly being developed, also as part of commercial solutions.</p> <ul> <li>Snyk Open Source</li> <li>Clair</li> <li>Grype</li> <li>Trivy</li> </ul>","tags":["robustness","deployment"]},{"location":"engineering-practice/experiment-tracking/","title":"Experiment tracking","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <p>TODO: Add article references to provisions addressed by experiment tracking</p> <ul> <li>Art. 15(3), experiment tracking captures used performance metrics and levels of accuracy</li> </ul>"},{"location":"engineering-practice/experiment-tracking/#motivation","title":"Motivation","text":"<p>AI models are typically trained on large amounts of data, using lots of compute resources. The outputs that a trained model generates depend on many factors, like which version of a dataset was used (for more information, refer to the data versioning section), what configurable attributes (also called hyperparameters) were used for the training, but also training techniques used like batching, which optimization target, and many more.</p> <p>The sum of all choices for the training of an AI model is often called an experiment. Consequently, with so many moving parts and choices in an experiment, extensive documentation is needed to make training workflows transparent to practitioners, decision makers, and users alike. A lot of ML/AI projects therefore use some form of experiment tracking solution to help with visualizing experiments, evaluate model performance, and compare the performance in different experiments.</p>"},{"location":"engineering-practice/experiment-tracking/#implementation-notes","title":"Implementation notes","text":"<p>A vital aspect of experiment tracking software is to diligently mark the outputs (artifacts) of your training workflows to ensure reproducibility and a good overview on the situation. This typically includes, but is not limited to,</p> <ul> <li>versions of training and evaluation data, either raw or pre-processed, ideally in conjunction with data versioning,</li> <li>hyperparameters giving as much information as possible in order to accurately reproduce experiments across platforms,</li> <li>metrics and statistics giving information about the performance of the newly trained model.</li> </ul>"},{"location":"engineering-practice/experiment-tracking/#key-technologies","title":"Key technologies","text":"<ul> <li> <p>MLflow</p> <p>MLflow is an open-source experiment tracking platform that stores data and model artifacts, (hyper)parameters, and visualizes model performance in different stages of the ML training lifecycle. It features a number of pre-configured tracking plugins for popular machine learning libraries called autologgers, which allow the collection of metrics and configuration with minimal setup. In addition, MLflow comes with a UI that can be used to visualize metadata and results across experiments.</p> </li> <li> <p>Weights &amp; Biases</p> <p>Weights &amp; Biases (or WandB) is a managed service for experiment tracking, metrics and metadata logging, and storing model and data artifacts.</p> </li> <li> <p>neptune.ai</p> <p>neptune.ai is another managed experiment tracking service for logging, visualizing, and monitoring metrics both in a training run and across multiple runs. It supports both managed and on-premise deployments, and offers special functionality for large language models (LLMs).</p> </li> <li> <p>ClearML</p> <p>ClearML is an open-source experiment tracking and orchestration platform that allows for the management of experiments, data, and models. It features a web-based UI for visualizing metrics and metadata, and supports integration with popular machine learning libraries.</p> </li> <li> <p>Comet</p> <p>Comet offers a managed experiment tracking service that allows for the logging and visualization of metrics, hyperparameters, and artifacts. It features a web-based UI for visualizing metrics and metadata, and supports integration with popular machine learning libraries.</p> </li> </ul>"},{"location":"engineering-practice/explainability/","title":"Explainability","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <ul> <li>Art. 13 (Transparency and Provision of Information to Deployers), in particular:<ul> <li>Art. 13(1): Explainability techniques contribute to a transparent system operation</li> <li>Art. 13(3)(b)(iv): Explainability techniques directly provide information relevant to explain the system's output</li> <li>Art. 13(3)(d): Explainability techniques can be used in the human oversight process to interpret the system's behavior</li> </ul> </li> <li>Art. 14 (Human Oversight), in particular:<ul> <li>Art. 14(4)(c), since explainability approaches enable human interpretation of the system output</li> <li>Art. 14(4)(d), explainability helps to decide if the output of the system should be disregarded</li> </ul> </li> <li>Art. 86(3) (Right to Explanation of Individual Decision-Making)</li> <li>Art. 11(1) in conjunction with Annex IV (Technical Documentation), in particular:<ul> <li>Annex IV(3): Explainability techniques can be part of the instructions for use to the deployer</li> </ul> </li> </ul>"},{"location":"engineering-practice/explainability/#motivation","title":"Motivation","text":"<p>Although there is no explicit demand for using explainability methods within the AI Act, they can aid compliance with the regulation, by providing insights into the system's decision-making process.</p> <p>In particular, these explanation approaches can assist the operator of the system in interpreting the system's behavior, as part of the human oversight process (see Art. 14) and allow affected third parties to request an explanation of the system's output (see Art. 86(3)). As part of the instructions for use to the deployer (see Art. 13(3)(b)(iv)), they can be used to provide information relevant to explain the system's output.</p> <p>Furthermore, specific methods are easily available through existing software packages and utilizing them could be considered as best practice (see upcoming ISO norm).</p> <p>Our showcase implementation focusses on a single explainability technique, the SHAP (SHapley Additive exPlanations) approach. This game-theoretic approach allows for post-hoc explanations in the form of highlighting each input feature's contribution to the model prediction.</p>"},{"location":"engineering-practice/explainability/#implementation-notes","title":"Implementation Notes","text":"<p>A custom inference server runtime implementation in MLserver is used to attach SHAP explanations to every model prediction. This design ensures that every prediction made by the system can be explained at a later point in time, without the need for the original input data or model revision at the time of the explanation. Contrast this with an implementation that would require access to the deployed model revision when an explanation is requested. In such a case, the model revision would need to be available (or deployed on demand) to generate the explanation.</p> <p>The system exposes an <code>/model/explain</code> API endpoint, that allows a user to request explanations for a given model prediction (as identified by its request ID, which is automatically attached to any prediction output through an HTTP <code>X-Request-ID</code> header).</p> <p>For a given inference request ID, the system retrieves the corresponding input, output, and explanation data from the inference log, and returns a visual representation of the SHAP explanation for the prediction to the user:</p> <p></p>"},{"location":"engineering-practice/explainability/#key-technologies","title":"Key Technologies","text":"<ul> <li>The <code>shap</code> Python package, implements the SHAP (SHapley Additive exPlanations) method</li> <li>The <code>lime</code> Python package, another popular model-agnostic explainability method (Local Interpretable Model-agnostic Explanations)</li> <li>Use of intrinsically explainable models:<ul> <li>The <code>interpret</code> Python package providing implementation for such glassbox models</li> </ul> </li> </ul>"},{"location":"engineering-practice/explainability/#resources","title":"Resources","text":"<ul> <li>As a primer, appliedAI TransferLab series on Explainable AI     and the accompanying training</li> <li>From the book Trustworthy Machine Learning, Chapter about Explainability</li> <li> The upcoming revision of the ISO/IEC DTS 6254 standard will describe approaches and methods used to achieve explainability objectives.</li> </ul>"},{"location":"engineering-practice/inference-log/","title":"Inference Log","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <p>Implementing an inference log will help you in achieving compliance with the following regulations:</p> <ul> <li>Art. 12 (Record-Keeping), in particular:<ul> <li>Art. 12(1), since the inference log enables the recording of events</li> <li>Art. 12(2), since the inference log allows the identification of potentially harmful situations and facilitates the post-market monitoring</li> </ul> </li> <li>Art. 19 (Automatically Generated Logs)</li> <li>Art. 26 (Obligations of Deployers of High-Risk AI Systems), in particular:<ul> <li>Art. 26(5) (Monitoring of the AI system's operation by the deployer)</li> <li>Art. 26(6) (Keeping of system logs by the deployer)</li> </ul> </li> <li>Art. 72 (Post-Market Monitoring)</li> </ul>"},{"location":"engineering-practice/inference-log/#motivation","title":"Motivation","text":"<p>An inference log is a permanent record of all inferences made by the AI system, including the input and output data, the model used, and relevant additional metadata.</p> <p>The inference log serves as the basis for monitoring the AI system's operation, ensuring that it behaves as intended and complies with legal and ethical requirements. As such, it enables post-deployment monitoring activities, such as model performance monitoring.</p> <p>Logging of inference data should allow for the reconstruction of the AI system's decision-making process, including the input data, the model used, and the output data. This is essential for understanding the AI system's behavior and for identifying and addressing any issues that may arise.</p>"},{"location":"engineering-practice/inference-log/#implementation-notes","title":"Implementation Notes","text":""},{"location":"engineering-practice/inference-log/#database-schema","title":"Database Schema","text":"<p>The database schema for the inference log should include tables for storing the following information:</p> <ul> <li>Requests: Information about the inference requests, including the input data, model used, and timestamp.</li> <li>Responses: Information about the inference responses, including the output data and timestamp.</li> <li>Errors: Information about any errors that occurred during inference, including the error message and the response data.</li> <li>Metadata: Additional metadata about the inference requests and responses.</li> </ul> <p>In order to allow for a variety of data types and structures, the input and output data should be stored as JSON or JSONB fields. The data types from the standardized Open Inference Protocol v2 REST API specification can be used as a reference for the structure of the input and output data. Using a standardized data structure will make it easier to integrate the inference log with other components of the AI system, such as the model performance monitoring component.</p> <pre><code>erDiagram\n    REQUESTS {\n        TEXT id PK\n        TIMESTAMPTZ timestamp\n        JSONB parameters\n        JSONB inputs\n        JSONB outputs\n        JSONB raw_request\n    }\n\n    RESPONSES {\n        TEXT id PK, FK\n        TEXT model_name\n        TEXT model_version\n        TIMESTAMPTZ timestamp\n        JSONB parameters\n        JSONB outputs\n        JSONB raw_response\n    }\n\n    ERRORS {\n        TEXT id PK, FK\n        TEXT error\n        JSONB response\n    }\n\n    METADATA {\n        TEXT id PK, FK\n        JSONB metadata\n    }\n\n    REQUESTS ||--|| RESPONSES : \"has\"\n    REQUESTS ||--|| ERRORS : \"has\"\n    REQUESTS ||--|| METADATA : \"has\"</code></pre> <p>Choose a database or storage solution that supports the required data structure and provides the necessary performance and scalability characteristics for the AI system's workload. Additionally, consider the data retention requirements of Art. 19 and the need for data protection and privacy (e.g., interactions with GDPR) when selecting a log storage solution.</p>"},{"location":"engineering-practice/inference-log/#application-middleware","title":"Application Middleware","text":"<p>In order to ensure that all inference requests are automatically logged, the inference log should be implemented as a middleware component in the AI system's application code.</p> <p>The middleware should intercept all incoming inference requests, log the relevant data, and then pass the request on to the model (inference server) for processing.</p> <p>When using the FastAPI, the inference log can be injected as a dependency into the application's route handlers or other dependencies. By further encapsulating the interface to the model itself in another dependency, the inference log can be easily integrated into the application's request handling pipeline.</p> <pre><code>flowchart TD\n    A[Route Handler] --&gt; B[Model Inference Dependency] --&gt; C[Inference Log Dependency]</code></pre>"},{"location":"engineering-practice/inference-log/#key-technologies","title":"Key Technologies","text":"<ul> <li>Any database or storage solution that supports the required data structure<ul> <li>The showcase implementation uses PostgreSQL</li> <li>Other choice include ElasticSearch, MongoDB, or SQLite</li> </ul> </li> <li>Open Inference Protocol specification, as a standardized data structure for the input and output data</li> <li>FastAPI for building the AI system's application code</li> </ul>"},{"location":"engineering-practice/model-monitoring/","title":"Model monitoring","text":"<p>Work in Progress</p> <p>The information on this page is incomplete and meant to illustrate the structure of these pages</p>"},{"location":"engineering-practice/model-monitoring/#model-performance-monitoring","title":"Model Performance Monitoring","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <p>Implementing a model performance solution will help you in achieving compliance with the following regulations:</p> <ul> <li>Art. 12 (Record-Keeping)<ul> <li>Art. 12(1) (Documentation of the AI system)</li> </ul> </li> <li>Art. 26(5) (Monitoring of the AI system's operation by the deployer)</li> <li> <p>Art. 72(2) (Post-market Monitoring), since a model monitoring solution allows for the continuous monitoring of the AI system's performance and compliance with legal requirements.</p> </li> <li> <p>TODO: Art. 15(4), robustness through continuous monitoring of operations and performance</p> </li> </ul>"},{"location":"engineering-practice/model-monitoring/#motivation","title":"Motivation","text":"<p>TODO: Explain why model performance monitoring is important.</p>"},{"location":"engineering-practice/model-monitoring/#implementation-notes","title":"Implementation Notes","text":""},{"location":"engineering-practice/model-monitoring/#model-performance","title":"Model Performance","text":"<p>Estimating the performance of a model is a crucial part of the machine learning lifecycle.</p> <p>For supervised learning application, the availability of labeled data poses a significant challenge. Algorithms that can estimate a model's performance in the absence of ground truth data can help in these scenarios.</p> <p>The NannyML library implements two approaches to model performance estimation:</p> <ul> <li>Confidence-based performance estimation (CBPE) for classification tasks</li> <li>Direct Loss Estimation (DLE) for regression tasks</li> </ul>"},{"location":"engineering-practice/model-monitoring/#data-quality","title":"Data Quality","text":"<p>Changes in the distribution of input or ground thrust data can lead to a decrease in model performance. Data drift detection aims to identify these changes after model deployment and is crucial to maintaining the model's performance.</p>"},{"location":"engineering-practice/model-monitoring/#reporting","title":"Reporting","text":"<p>NannyML performance estimators operate on chunks of model predictions, which can be used to generate reports on the model's performance.</p> <p>It is therefore straightforward to generate periodic reports on the model's performance. These reports can be used to monitor the model's performance over time and identify potential issues (see the next section).</p>"},{"location":"engineering-practice/model-monitoring/#alerting","title":"Alerting","text":"<p>When a model's performance drops below a certain threshold or data drift occurs, it is essential to alert the responsible team. NannyML allows threshold-based alerting for both performance and drift metrics, which can serve as the decision basis for the alerting system.</p> <p>The concrete implementation of the alerting system depends on the organization's requirements and the available infrastructure. Examples include sending an email or Slack message, creating a ticket in a ticketing system, or invoking a webhook.</p>"},{"location":"engineering-practice/model-monitoring/#key-technologies","title":"Key Technologies","text":"<ul> <li>NannyML, used in the showcase implementation</li> <li>Alternatives for model and data quality monitoring:<ul> <li>Evidently, for model performance and data quality monitoring</li> <li>Alibi Detect, for outlier and drift detection</li> </ul> </li> <li>Prometheus Alertmanager, as an alerting middleware</li> </ul>"},{"location":"engineering-practice/model-registry/","title":"Model registry","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <p>TODO: Add article references to provisions addressed by a model registry</p>"},{"location":"engineering-practice/model-registry/#motivation","title":"Motivation","text":"<p>After training, the resulting models are persisted to a preconfigured, often remote storage location, to make the raw models available for (internal) use. Frequently, multiple versions of a given model are trained to assess the influence of certain parameters on the quality of the model outputs. This, together with the requirement for reproducibility in many AI projects, necessitates a solution for managing multiple versions of models side-by-side, that is also highly available, reliable, and can be queried efficiently for any specific version.</p>"},{"location":"engineering-practice/model-registry/#implementation-notes","title":"Implementation notes","text":"<p>Capabilities of a model registry can vary quite widely depending on how much of the stated responsibilities are handled by other tools in the practitioner's AI tool stack of choice.</p> <p>In the simplest case, a model registry can just be a local or remote storage bucket, that keeps track of model metadata and versions by indexing them and storing the information in a database or similar systems. If more responsibilities for the AI model lifecycle need to be handled, it is possible to use a tool that combines the storage of artifacts with their deployment, e.g. as a web application or a local assistant type of model.</p> <p>Importantly, some model servers provide endpoints to query the underlying model registry for information on specific models and versions. A noteworthy standardization of this is the Open Inference Protocol, which specifies an API that model servers can support to allow the user of an AI model to query versions, health, and general metadata of models.</p>"},{"location":"engineering-practice/model-registry/#key-technologies","title":"Key Technologies","text":"<p>Since the task of versioning and storing models is tightly coupled to the training and tracking process itself, many experiment trackers (refer to the documentation on them here) come with a builtin model registry. All of the experiment trackers listed in the mentioned documentation provide support for a model registry as well.</p> <p>An example of a model server implementing the Open Inference Protocol is MLServer by Seldon, which offers supported for multi-model serving, batched requests, and parallel inference.</p>"},{"location":"engineering-practice/model-serving/","title":"Model Serving","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <p>TODO: Incomplete</p> <ul> <li>Art. 15 (Accuracy, Robustness and Cybersecurity)</li> </ul>"},{"location":"engineering-practice/model-serving/#motivation","title":"Motivation","text":"<p>Model serving is the process of deploying machine learning models to production environments, where they can be accessed and used by applications or users.</p> <p>Not every AI system involves a real-time inference component, for example batch processing systems or offline analytics. However, for those that do, model serving is a critical part of the machine learning lifecycle.</p> <p>Model serving needs to be designed to ensure that the deployed models are accurate, robust, and secure.</p>"},{"location":"engineering-practice/model-serving/#implementation-notes","title":"Implementation Notes","text":"<p>Containerization is a common practice for model serving, as it allows for the deployment of models in isolated environments, ensuring consistency and reproducibility. It also enables the use of different versions of models and dependencies without conflicts.</p> <p>The models to be deployed should be obtained from a model registry, in order to preserve the traceability and reproducibility of the models. A model serving should be able to expose metadata about the model and its provenance, in order to associate this information with every prediction (see the page on inference logs).</p>"},{"location":"engineering-practice/model-serving/#inference-api-design","title":"Inference API Design","text":"<p>Real-time inference APIs provide the interface for applications to interact with the deployed models.</p> <p>Designing the API for an interface is a trade-off between flexibility and usability.</p> <p>While a bespoke API can be designed for a single model (which might be tied to a specific input data schema), a more generic API can be designed to support multiple models and input data schemas.</p> <p>One such generic API is specified as the Open Inference Protocol, which is supported by several model serving frameworks. It provides API endpoints and type definitions for inference requests (with a flexible data schema), model metadata, and model management.</p>"},{"location":"engineering-practice/model-serving/#key-technologies","title":"Key Technologies","text":"<ul> <li>MLServer</li> <li>BentoML</li> <li>Seldon Core</li> <li>KServe</li> </ul>"},{"location":"engineering-practice/operational-monitoring/","title":"Operational Monitoring","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <p>Implementing an operational metrics solution will help you in achieving compliance with the following requirements of the AI Act:</p> <p>TODO: Incomplete</p> <ul> <li>Art. 15 (Accuracy, Robustness and Cybersecurity), in particular:<ul> <li>Art. 15(4) Robustness, monitoring and alerting can help detect and mitigate potential robustness and availability issues</li> <li>Art. 15(5) Cybersecurity, since monitoring is a crucial part of threat detection</li> </ul> </li> <li>Art. 26 (Obligations of Deployers of High-Risk AI Systems), in particular:<ul> <li>Art. 26(5) (Monitoring of the AI system's operation by the deployer)</li> <li>Art. 26(6) (Keeping of system logs by the deployer)</li> </ul> </li> </ul>"},{"location":"engineering-practice/operational-monitoring/#motivation","title":"Motivation","text":"<p>Besides monitoring the performance of your machine learning models, it is also important to monitor the performance of the underlying technical infrastructure and services. This includes monitoring the health of the servers, databases, and other components that support your machine learning applications.</p> <p>These operational metrics can give an indication of the overall health of the system and can help you identify potential issues before they become critical. This aligns with the AI Act obligations towards the robustness and cybersecurity of high-risk AI systems.</p>"},{"location":"engineering-practice/operational-monitoring/#implementation-notes","title":"Implementation Notes","text":"<p>Relevant metrics to monitor include:</p> <ul> <li>Resource usage: CPU, GPU, memory, disk, network</li> <li>Service availability: uptime, error rates</li> <li>Latency: request/response times</li> <li>Throughput: requests per second</li> <li>Custom metrics: application-specific metrics (e.g., number of processed records, model inference times)</li> </ul> <p>While it is a crucial part of an operational monitoring solution, this page does not cover the topic of alerting. The following activities can provide a starting point to implement an alerting system:</p> <ul> <li>Determining the thresholds for the metrics</li> <li>Defining the escalation process for alerts</li> <li>Setting up the alerting channels (e.g., email, Slack, PagerDuty)</li> <li>Deploying and setting up the alerting system</li> </ul>"},{"location":"engineering-practice/operational-monitoring/#key-technologies","title":"Key Technologies","text":""},{"location":"engineering-practice/operational-monitoring/#metrics-collection-and-visualization","title":"Metrics Collection and Visualization","text":"<ul> <li>Prometheus, an time-series database for event and metrics collection, storage, monitoring, and alerting<ul> <li>Many tools and frameworks can expose their operational metrics in the Prometheus format</li> </ul> </li> <li>Grafana, an open-source analytics solution for visualization of metrics</li> <li>The ELK (Elasticsearch, Logstash, Kibana) stack, in particular:<ul> <li>Elasticsearch, a distributed search and analytics engine</li> <li>Kibana, a data visualization and exploration tool for Elasticsearch</li> </ul> </li> </ul>"},{"location":"engineering-practice/operational-monitoring/#alerting","title":"Alerting","text":"<ul> <li>Prometheus Alertmanager</li> <li>Grafana Alerting</li> </ul>"},{"location":"engineering-practice/orchestration/","title":"Orchestration","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <ul> <li>Art. 10(2)(c): Consistent and versioned data preprocessing operations</li> <li>Art. 11(1) in conjunction with Annex IV (Technical Documentation), in particular:<ul> <li>Annex IV(2)(c): Description of the system and overall processing architecture</li> </ul> </li> </ul>"},{"location":"engineering-practice/orchestration/#motivation","title":"Motivation","text":"<p>The orchestration of machine learning workflows is a critical aspect of the machine learning lifecycle.</p> <p>In essence, it concerns the automated execution of tasks such as data preprocessing, model training, evaluation, and deployment.</p> <p>A workflow orchestrator takes care to execute these tasks in the correct order, manage dependencies between tasks, and handle failures or retries as needed. It keeps a permanent record of the execution history, which is essential for reproducibility, transparency, and auditability.</p>"},{"location":"engineering-practice/orchestration/#implementation-notes","title":"Implementation Notes","text":"<p>TODO</p>"},{"location":"engineering-practice/orchestration/#key-technologies","title":"Key Technologies","text":""},{"location":"engineering-practice/orchestration/#workflow-orchestrators","title":"Workflow Orchestrators","text":"<ul> <li>Dagster</li> <li>Apache Airflow</li> <li>Prefect</li> <li>Flyte</li> </ul>"},{"location":"engineering-practice/orchestration/#mlops-platforms","title":"MLOps Platforms","text":"<ul> <li>Kubeflow</li> <li>Metaflow</li> <li>ZenML</li> <li>Kedro</li> </ul>"},{"location":"engineering-practice/orchestration/#platform-software-as-a-service-offerings","title":"Platform-/Software-as-a-Service Offerings","text":"<ul> <li>AWS Step Functions</li> <li>Google Cloud Workflows</li> <li>Databricks Workflows</li> </ul>"},{"location":"engineering-practice/reference-architecture/","title":"Engineering Practice","text":""},{"location":"engineering-practice/reference-architecture/#further-resources","title":"Further Resources","text":"<p>The following pages provide more detailed information on the components of the reference architecture, grouped by the phase of the machine learning lifecycle.</p>"},{"location":"engineering-practice/reference-architecture/#training-phase","title":"Training Phase","text":"<ul> <li>Experiment Tracking</li> <li>Model Registry</li> <li>Data Governance<ul> <li>Bias Mitigation</li> <li>Data Versioning</li> <li>Data Quality</li> </ul> </li> <li>Measuring Accuracy</li> <li>Workflow Orchestration</li> </ul>"},{"location":"engineering-practice/reference-architecture/#deployment-phase","title":"Deployment Phase","text":"<ul> <li>Model Serving</li> <li>Inference Log</li> <li>Model Explainability</li> <li>Model Performance Monitoring</li> <li>Operational Monitoring</li> <li>Containerization</li> </ul>"},{"location":"engineering-practice/data-governance/","title":"Data Governance","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <ul> <li>Article 10</li> </ul> <p></p> <p>In general, data governance encompasses all methodologies for managing data throughout its entire lifecycle. With respect to the AI Act, Article 10 mandates that datasets used in developing high-risk AI systems must be of high quality, relevant, representative, free from bias, and appropriately documented to ensure fairness, accuracy, and reliability. While the article outlines certain required practices, it lacks a comprehensive definition of data governance and data management.</p> <p>To enhance implementation clarity, we distinguish between components suitable for automation (engineering practices, described in this section) and those centered on process and documentation, see Technical Documentation. This is not a simple mapping of paragraphs to either or, moreover each paragraph of Art. 10 can include both types of tasks.</p> <p>Specific to bias. Refer to the page on bias mitigation for more information.</p> <ol> <li>For the development of high-risk AI systems not using techniques involving the training of AI models, paragraphs 2 to 5 apply only to the testing data sets.</li> </ol> <p>This paragraph addresses high-risk AI systems that are developed without using techniques involving the training of AI models. These systems might rely on alternative approaches, such as rule-based systems, hard-coded algorithms, or pre-existing models that do not require additional training or updates to their parameters.</p> <ul> <li>Data Versioning</li> <li>Data Quality</li> </ul>"},{"location":"engineering-practice/data-governance/bias-mitigation/","title":"Bias mitigation","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <ul> <li>Art. 10 (Data and Data Governance), in particular:<ul> <li>Art. 10(2)(g): Appropriate measures for bias detection, prevention, and mitigation</li> <li>Art. 10(3): Assert appropriate statistical properties regarding natural persons related to the use of the high-risk AI system</li> <li>Art. 10(5): Use of special categories of personal data in bias detection and correction</li> </ul> </li> </ul>"},{"location":"engineering-practice/data-governance/bias-mitigation/#motivation","title":"Motivation","text":"<p>Biases are commonly considered one of the most detrimental effects of artificial intelligence (AI) use.</p> <p>Art. 10 mandates the examination, detection, prevention, and mitigation of biases present in the data that could result in a harmful impact to health, safety, or fundamental rights.</p> <p>As such, data governance activities should include practices to cover these requirements and map them to activities in the machine learning lifecycle.</p>"},{"location":"engineering-practice/data-governance/bias-mitigation/#implementation-notes","title":"Implementation Notes","text":""},{"location":"engineering-practice/data-governance/bias-mitigation/#bias-and-fairness-analysis-techniques","title":"Bias and Fairness Analysis Techniques","text":"<ul> <li>Conduct Exploratory Data Analysis (EDA): Analyze the dataset for imbalances or patterns that may suggest bias, such as over-representation or under-representation of certain groups.</li> <li>Fairness Metrics: Calculate fairness metrics such as demographic parity, equalized odds or disparate impact to quantify bias in datasets and model outputs.</li> <li> <p>See fairlearn documentation for an introduction to commonly used metrics</p> </li> <li> <p>Diversity Analysis: Evaluate the dataset's demographic diversity, ensuring it represents all relevant populations appropriately.</p> <ul> <li>Group Representation:     Checks whether groups are proportionally represented in the dataset (calculate as fraction of the total dataset size)</li> <li>Overall Accuracy Equality:     Ensures that accuracy rates are equal across groups.</li> </ul> </li> </ul>"},{"location":"engineering-practice/data-governance/bias-mitigation/#mitigation-techniques","title":"Mitigation Techniques","text":"<p>Bias mitigation techniques broadly fall into three categories, based on their applicability during the machine learning lifecycle:</p> <pre><code>flowchart LR\n    assessment[Bias Assessment]\n    audit[Auditing]\n\n    subgraph mitigation[Bias Mitigation]\n        direction TB\n\n        preproc[Preprocessing Techniques]\n        inproc[Inprocessing Techniques / Model Training]\n        postproc[Postprocessing Techniques]\n\n        preproc --&gt; inproc\n        inproc --&gt; postproc\n    end\n\n    assessment --&gt; mitigation\n    mitigation --&gt; audit</code></pre>"},{"location":"engineering-practice/data-governance/bias-mitigation/#preprocessing-techniques","title":"Preprocessing Techniques","text":"<ul> <li>Resampling Techniques: Use oversampling or undersampling to balance the representation of different demographic groups.</li> <li>Synthetic Data Generation: Generate synthetic examples for under-represented groups to ensure better balance in the dataset.</li> <li>Reweighing: Adjust the weights of data instances to ensure fair representation across groups.</li> </ul>"},{"location":"engineering-practice/data-governance/bias-mitigation/#inprocessing-techniques-model-training","title":"Inprocessing Techniques / Model Training","text":"<ul> <li>Bias-Corrected Features: Transform features to reduce correlations with sensitive attributes (e.g., gender, race).</li> <li>Fair Representations: Use fairness-aware models that explicitly optimize for fairness metrics alongside predictive accuracy.</li> </ul>"},{"location":"engineering-practice/data-governance/bias-mitigation/#post-processing-techniques","title":"Post-processing Techniques","text":"<ul> <li>Outcome Adjustments: Adjust decision thresholds or outputs to ensure equitable outcomes across demographic groups.<ul> <li>Equalized Odds Postprocessing: Modifies predictions to satisfy equalized odds constraints.</li> </ul> </li> <li>Bias Mitigation Strategies: Apply fairness postprocessing methods, such as calibration by group or equalized odds adjustments.</li> </ul>"},{"location":"engineering-practice/data-governance/bias-mitigation/#auditing","title":"Auditing","text":"<p>Regularly audit data to ensure it is free from systemic errors or biases.</p> <ul> <li>Segmentation Analysis: Partition the dataset based on sensitive attributes and assess performance metrics for each segment to detect disparities.</li> <li>Subgroup Fairness Checks: Compare outcomes for different demographic subgroups to identify discrepancies.</li> <li>Drift Detection: Use tools to detect data or model drift that may reintroduce bias over time.</li> </ul>"},{"location":"engineering-practice/data-governance/bias-mitigation/#privacy-concerns","title":"Privacy Concerns","text":"<p>TODO: Discuss requirements of Art. 10(5).</p>"},{"location":"engineering-practice/data-governance/bias-mitigation/#key-technologies","title":"Key Technologies","text":"<ul> <li>AI Fairness 360 (<code>aif360</code>) by IBM</li> <li>Fairlearn by Microsoft</li> <li>What-if Tool by Google</li> <li><code>imblearn</code> Python package for scikit-learn, provides tools when dealing with classification with imbalanced classes.</li> </ul>"},{"location":"engineering-practice/data-governance/bias-mitigation/#further-reading","title":"Further Reading","text":"<ul> <li>European Parliamentary Research Service (2022) - Auditing the quality of datasets used in algorithmic decision-making systems, Study</li> <li>Caton, Haas (2024) - Fairness in Machine Learning: A Survey</li> <li>Barocas, Hardt, Narayanan (2023) - Fairness and Machine Learning: Limitations and Opportunities</li> </ul>"},{"location":"engineering-practice/data-governance/data-quality/","title":"Data quality","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <ul> <li>Art. 10 (Data and Data Governance), in particular:<ul> <li>Art. 10(2)(c)</li> <li>Art. 10(3)</li> </ul> </li> </ul>","tags":["phase::scoping","phase::data engineering"]},{"location":"engineering-practice/data-governance/data-quality/#motivation","title":"Motivation","text":"<p>Art. 10(3) of the AI Act demands a certain quality of data used for training and evaluating models, in particular these data sets should be:</p> <ul> <li>relevant,</li> <li>sufficiently representative</li> <li>complete, and</li> <li>free of errors.</li> </ul> <p>To achieve those qualities, there are different techniques available at different steps in the system lifecycle.</p>","tags":["phase::scoping","phase::data engineering"]},{"location":"engineering-practice/data-governance/data-quality/#implementation-notes","title":"Implementation Notes","text":"<p>Note that the techniques discussed in the section focus on technical approaches for ensuring data quality. They need to be accompanied by organizational and governance measures to become fully effective.</p>","tags":["phase::scoping","phase::data engineering"]},{"location":"engineering-practice/data-governance/data-quality/#data-preprocessing","title":"Data Preprocessing","text":"<ul> <li>Detect and handle missing or incomplete data<ul> <li>Conduct data analysis to identify missing fields.</li> <li>Use statistical methods to assess if missing data skews results.</li> <li>Implement appropriate handling (e.g., interpolation, mean/mode imputation).</li> </ul> </li> <li>Perform data consistency checks<ul> <li>Enforce data schema for tabular data.</li> <li>Identify and remove duplicate records.</li> <li>Ensure data formats are consistent (e.g., all dates in correct format).</li> <li>Check for missing values and determine handling strategies (imputation or removal).</li> </ul> </li> <li>Keep preprocessing consistent, versioned and reproducible<ul> <li>Avoid manual processing steps, rely on data pipelines in a workflow orchestrator instead</li> </ul> </li> </ul>","tags":["phase::scoping","phase::data engineering"]},{"location":"engineering-practice/data-governance/data-quality/#data-quality-validation","title":"Data Quality Validation","text":"<ul> <li>Validate data against ground truth<ul> <li>Cross-check a sample of the dataset against verified real-world sources or domain experts.</li> </ul> </li> <li>Ensure data accuracy through automated validation<ul> <li>Logical inconsistencies (e.g., negative age values).</li> <li>Outliers and anomalies using statistical methods (e.g., z-score, IQR analysis).</li> </ul> </li> <li>Produce automated data quality reports for human review and inclusion in technical documentation.</li> <li>Monitor for data drift over time<ul> <li>Set up periodic validation checks to see if the data distribution changes over time.</li> <li>Retrain models if significant drift is detected.</li> </ul> </li> </ul>","tags":["phase::scoping","phase::data engineering"]},{"location":"engineering-practice/data-governance/data-quality/#key-technologies","title":"Key Technologies","text":"<ul> <li>Pandas<ul> <li>Other dataframe libraries with similar features exist, e.g., Polars, Spark <code>DataFrame</code>s</li> </ul> </li> <li>Pandera, for data quality validation</li> <li>Great Expectations / GX Core, for data quality validation</li> </ul>","tags":["phase::scoping","phase::data engineering"]},{"location":"engineering-practice/data-governance/data-versioning/","title":"Data versioning","text":"<p>Compliance Info</p> <p>TODO: Explain the \"Compliance Info\" box.</p> <p>TODO: Add article references to provisions addressed by data versioning</p>","tags":["phase::data engineering","phase::modeling"]},{"location":"engineering-practice/data-governance/data-versioning/#motivation","title":"Motivation","text":"<p>In fast-moving, complex environments such as the subject of AI, it is vital to keep track of where data and information originated, not only to be transparent, or to easily share work with colleagues, but to be able to identify the root cause in the event of a problem with your system.</p> <p>In the software engineering world, distributed version control systems (VCS) have seen wide adoption because they address all of these concerns. Through a set of basic abstractions, they provide bookkeeping powers by means of unique and immutable references, distributed storage for work sharing, and a graph-based history building for detailed information keeping. Consequently, data version control or data versioning systems (DVCS) can be thought of as the similar approach to all data artifacts produced by your machine learning systems.</p>","tags":["phase::data engineering","phase::modeling"]},{"location":"engineering-practice/data-governance/data-versioning/#implementation-notes","title":"Implementation notes","text":"<p>To add data versioning to a machine learning project, it is important to figure out a few steps:</p> <ol> <li>Collaborative development.</li> </ol> <p>A suitable data version control system needs to accommodate multiple engineers working simultaneously on different versions of the data, and ensure that changes made by one engineer do not invalidate the work of another. This can be done for example by using a branch workflow, where each person has their own siloed copy of the data, and can make changes to it without changing the canonical version (the \"main\" branch in this model).</p> <pre><code>%%{ init: {'theme': 'base'} }%%\n---\ntitle: Branching in a (data) version control system\n---\ngitGraph\n   commit\n   commit\n   branch develop\n   checkout develop\n   commit\n   commit\n   checkout main\n   merge develop\n   commit\n   commit</code></pre> <p>Figure 1: A branching data version control approach, with commits (immutable snapshots of the data) shown as dots.</p> <ol> <li>Distributed storage.</li> </ol> <p>A data version control system should be accessible for all developers, and host the data in an off-device location to ensure easy access, fault tolerance, and availability of backups.</p> <ol> <li>Communication with users.</li> </ol> <p>In addition to availability, security, and fault tolerance, it is necessary that the data version control system can be easily interfaced with in AI application code. This usually means that a selection of API clients or SDKs is available for a variety of programming languages, which allows developers to efficiently interface with the DVCS.</p>","tags":["phase::data engineering","phase::modeling"]},{"location":"engineering-practice/data-governance/data-versioning/#interoperability","title":"Interoperability","text":"<ul> <li>Where applicable, ensure that datasets and documentation are interoperable with standard regulatory frameworks and can be audited by authorities.<ul> <li>Standardize formats (e.g., CSV or Parquet files for tabular data)</li> <li>Centralize data storage (e.g., in a data lake or data warehouse) such that access to data sets can be given at any time, and it is not depending on data sets on local machines of ML Engineers and Data Scientists.</li> </ul> </li> </ul>","tags":["phase::data engineering","phase::modeling"]},{"location":"engineering-practice/data-governance/data-versioning/#key-technologies","title":"Key technologies","text":"<ol> <li>lakeFS</li> </ol> <p>lakeFS models a lot of its version control abstractions like the git VCS tool. It builds a linear history on commits, separates different avenues of work with branches, allows annotating data with tags, and also has supported for a merge workflow. It can be set up on either local storage or a cloud storage bucket of one of the larger providers (AWS, GCP, Azure).</p> <ol> <li>DVC</li> </ol> <p>DVC is a pure-Python command-line interface (CLI) for versioning data and model artifacts resulting from ML pipelines. In addition, it has functionality for local experiment tracking, and versioning data pipelines in git together with project source code.</p> <ol> <li>git-lfs</li> </ol> <p>Git Large File Storage (LFS) stores large data assets in remote locations, and replaces the data with text pointers in git, so that data and its usage are decoupled on the storage level. It integrates with the git command-line interface, augmenting developers' existing git workflows intuitively, and reducing the learning curve that an addition of a standalone tool would bring.</p> <ul> <li> <p>Establish a data management system</p> <ul> <li>Centralize data storage (e.g., in a Data Lake or Data Warehouse)<ul> <li>Implement access control and restrict access to data to authorized personal and systems only</li> <li>Avoid storing data on developer machines. Provide discovery tooling. Provide centralized compute with high-speed data access.</li> </ul> </li> </ul> </li> <li> <p>Data versioning:</p> <ul> <li>Automation:<ul> <li>Version datasets on data processing pipelines</li> <li>Generate logs on each update</li> <li>Execute pipelines on data changes</li> </ul> </li> <li>Depending on the use case and data set properties:<ul> <li>Store complete versions (suitable for small data sets only)</li> <li>Store increments (new image objects, new partitions in time series, etc)</li> <li>Store differences (deltas) between data set versions</li> </ul> </li> <li>Rule of thumb: You should version data whenever changes to the dataset occur that could impact its use, reproducibility, or compatibility with downstream systems.<ul> <li>Initial data set creation</li> <li>Data updates (new data, corrections, expansions)</li> <li>Data processing: after applying preprocessing steps (evaluate based on compute vs. storage requirements)</li> <li>Model training: maintain a version of the data for each trained model</li> <li>Performance: after subsampling or aggregating</li> <li>Experiments: when experimenting with different versions, including different features, etc.</li> <li>Collaboration: track contributions per team/person</li> <li>Decommissioning: store the last version of the data</li> <li>Scheduled: hourly, daily, weekly, etc.</li> </ul> </li> <li>Tools: LakeFS, DVC, Delta Lake, Git LFS</li> </ul> </li> </ul>","tags":["phase::data engineering","phase::modeling"]},{"location":"showcase/","title":"Showcase","text":"<p>Placeholder for all information</p> <p>Reading: - Retiring Adult: New Datasets for Fair Machine Learning</p>"}]}