{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A project demonstrating the implementation of a trustworthy AI system.</p>"},{"location":"explanations/accuracy/","title":"Accuracy","text":"<p>This article requires that high-risk AI systems be designed to achieve appropriate levels of accuracy, robustness, and cybersecurity. Systems should perform consistently throughout their lifecycle, be resilient to errors and faults, and have measures in place to mitigate risks associated with cybersecurity threats. The accuracy of AI systems should be declared in their instructions, and systems should be designed to minimize risks of producing biased outputs.</p>"},{"location":"explanations/accuracy/#continuous-testing-and-benchmarking-of-models","title":"Continuous testing and benchmarking of models","text":"<ul> <li>Regularly test datasets to validate their quality, accuracy, and relevance to the AI system's intended use.<ul> <li>Data quality:<ul> <li>Missing Values Analysis: Identify and handle missing or incomplete data using imputation, removal, or domain-specific methods.</li> <li>Outlier Detection: Use statistical or machine learning techniques to detect and address outliers (e.g., Z-scores).</li> <li>Duplicate Records: Regularly check for and remove duplicate entries to maintain dataset integrity.</li> <li>Noise Reduction: Identify noisy or erroneous data points using heuristics, domain knowledge, or automated tools.</li> </ul> </li> <li>Relevance:<ul> <li>Feature Relevance Analysis: Perform feature selection or importance ranking to ensure all features contribute meaningfully to the AI model. (SHAP)</li> <li>Temporal Relevance: Test datasets periodically to ensure they remain current and relevant (e.g., avoid outdated information).</li> </ul> </li> <li>Drift detection:<ul> <li>Concept Drift: Regularly test for shifts in the relationship between input features and target labels over time.</li> <li>Data Distribution Drift: Monitor changes in feature distributions compared to baseline distributions<ul> <li>Kolmogorov-Smirnov test</li> <li>Earth Mover\u2019s Distance</li> </ul> </li> </ul> </li> </ul> </li> <li>Metrics:<ul> <li>Regression:<ul> <li>Mean Absolute Error (MAE)</li> <li>Median Absolute Error (MedAE)</li> <li>Mean Squared Error (MSE)</li> <li>Root Mean Squared Error (RMSE)</li> <li>R-Squared</li> </ul> </li> <li>Classification:<ul> <li>Accuracy (Balanced accuracy)</li> <li>Logarithmic Loss</li> <li>Precision</li> <li>Recall</li> <li>F1-score</li> <li>Area Under the ROC Curve (AUC-ROC)</li> </ul> </li> </ul> </li> <li>Tools:<ul> <li>Great Expectations, Pandas, Evidently AI, NannyML</li> </ul> </li> <li>Ensure that datasets are updated as necessary to reflect current conditions or contexts.</li> </ul>"},{"location":"explanations/accuracy/#reading","title":"Reading","text":"<p>From the book Trustworthy Machine Learning * Chapter 5.1</p>"},{"location":"explanations/bias/","title":"Mitigate Bias","text":"<ul> <li>Implement measures to identify, monitor, and mitigate potential biases in datasets that could lead to discriminatory outcomes.</li> <li>Conduct Exploratory Data Analysis (EDA): Analyze the dataset for imbalances or patterns that may suggest bias, such as over-representation or under-representation of certain groups.</li> <li> <p>Fairness Metrics: Use fairness metrics like demographic parity, equalized odds, and disparate impact to quantify bias in datasets and model outputs.</p> <ul> <li>Demographic Parity (Statistical Parity):   Ensures that all groups have equal probabilities of receiving a positive outcome.</li> </ul> <p>$P(Outcome=Positive\u2223Group=A)=P(Outcome=Positive\u2223Group=B)$ - Equalized Odds:   Requires equal true positive rates (TPRs) and false positive rates (FPRs) across groups.</p> <p>$P(Predicted Positive\u2223Actual Positive,Group=A)=P(Predicted Positive\u2223Actual Positive,Group=B)$ - Disparate Impact:   Compares the ratio of favorable outcomes for different groups. Often used to detect discrimination.</p> <p>$Disparate Impact=P(Positive Outcome\u2223Group=B) / P(Positive Outcome\u2223Group=A)$</p> </li> <li> <p>Diversity Analysis: Evaluate the dataset's demographic diversity, ensuring it represents all relevant populations appropriately.</p> </li> <li>Group Representation:     Checks whether groups are proportionally represented in the dataset (calculate as fraction of the total dataset size)</li> <li> <p>Overall Accuracy Equality:     Ensures that accuracy rates are equal across groups.</p> </li> <li> <p>Open-Source Tools:</p> </li> <li>IBM AI Fairness 360</li> <li>Microsoft's Fairlearn</li> <li> <p>Google's What-If Tool</p> </li> <li> <p>Resampling Techniques: Use oversampling (e.g., SMOTE) or undersampling to balance the representation of different demographic groups.</p> </li> <li> <p><code>imblearn</code> package</p> </li> <li> <p>Synthetic Data Generation: Generate synthetic examples for under-represented groups to ensure better balance in the dataset.</p> </li> <li> <p>Reweighting: Adjust the weights of data instances to ensure fair representation across groups.</p> </li> <li> <p>Bias-Corrected Features: Transform features to reduce correlations with sensitive attributes (e.g., gender, race).</p> </li> <li> <p>Fair Representations: Use fairness-aware models that explicitly optimize for fairness metrics alongside predictive accuracy.   -Learning Fair Representations (LFR): Produces a transformed dataset with minimal bias.</p> </li> <li> <p>Outcome Adjustments: Adjust decision thresholds or outputs to ensure equitable outcomes across demographic groups.</p> </li> <li> <p>Equalized Odds Postprocessing: Modifies predictions to satisfy equalized odds constraints.</p> </li> <li> <p>Bias Mitigation Strategies: Apply fairness postprocessing methods, such as calibration by group or equalized odds adjustments.</p> </li> <li> <p>Regularly audit data to ensure it is free from systemic errors or biases.</p> </li> <li>Segmentation Analysis: Partition the dataset based on sensitive attributes and assess performance metrics for each segment to detect disparities.</li> <li>Subgroup Fairness Checks: Compare outcomes for different demographic subgroups to identify discrepancies.</li> <li>Drift Detection: Use tools to detect data or model drift that may reintroduce bias over time.</li> </ul>"},{"location":"explanations/bias/#freestyle","title":"Freestyle","text":"<ul> <li>interpretation of AI Act regarding bias (referencing Annexes)</li> <li>definition we use in the project and why</li> <li>identifying possible biases </li> <li>methods to detect bias</li> <li> <p>what we choose for this use-case and why</p> </li> <li> <p>AI Act Article X</p> </li> <li>Interpretation of Activity Y in Article X</li> <li>Definition of Activity Y in this project</li> <li>How is Activity Y relevant for this project</li> <li>Methods for Activity Y</li> <li>Selected methods and tools for Activity Y</li> <li>Findings</li> </ul>"},{"location":"explanations/bias/#reading","title":"Reading","text":"<p>From the book Trustworthy Machine Learning * Chapter 2.8 - 2.9 (and previous chapter as mentioned in the text) </p>"},{"location":"explanations/cybersecurity/","title":"Cybersecurity","text":""},{"location":"explanations/cybersecurity/#data-protection","title":"Data Protection","text":"<ul> <li>Encryption: Encrypt data at rest and in transit to prevent unauthorized access.</li> <li>Anonymization and Pseudonymization: Remove or mask sensitive data used in the AI system.</li> </ul>"},{"location":"explanations/cybersecurity/#authentication-and-access-control","title":"Authentication and Access Control","text":"<ul> <li>Role-Based Access Control (RBAC): Restrict system access based on user roles.</li> <li>Multi-Factor Authentication (MFA): Implement MFA for system access to enhance security.</li> <li>Audit Logging: Record and monitor access to sensitive system components and data.</li> </ul>"},{"location":"explanations/cybersecurity/#secure-deployment","title":"Secure Deployment","text":"<ul> <li>Containerization: Deploy AI systems in secure containers and isolate them from external systems.</li> <li>Runtime Monitoring: Use runtime security tools to monitor and block malicious activities during operation.</li> </ul>"},{"location":"explanations/data-governance/","title":"Data Governance","text":"<ul> <li> <p>Establish a data management system</p> <ul> <li>Centralize data storage (e.g., in a Data Lake or Data Warehouse)<ul> <li>Implement access control and restrict access to data to authorized personal and systems only</li> <li>Avoid storing data on developer machines. Provide discovery tooling. Provide centralized compute with high-speed data access.</li> </ul> </li> <li>Establish a organization-wide glossary index<ul> <li>Define terms (including definition/description) and use those to annotate data set and fields in the data sets. Single source of truth for interpreting data sets and fields.</li> </ul> </li> </ul> </li> <li> <p>Document data sources (in a Data Catalog or in a Data Card):</p> <ul> <li>Data selection process</li> <li>Quality improvement measures</li> <li>Data owners</li> <li>Description</li> <li>Classification (applicable glossary terms)</li> <li>Fields (name, type, description, classification) for tabular data</li> <li>Data properties (e.g., format, resolution) for non-tabular data</li> </ul> </li> <li> <p>Establish data lineage (i.e., define upstream or downstream data sets) on data set level or on column level for tabular data.</p> <ul> <li>Define lineage in a data catalog, or have it automatically represented in a workflow/data orchestrator<ul> <li>Keep records of the data lifecycle, including the sources of data, selection criteria, and preprocessing steps (all steps to model training).</li> </ul> </li> </ul> </li> <li> <p>Data versioning:</p> <ul> <li>Automation:\ud83c\udf4f<ul> <li>Version datasets on data processing pipelines</li> <li>Generate logs on each update</li> <li>Execute pipelines on data changes</li> </ul> </li> <li>Depending on the use case and data set properties:<ul> <li>Store complete versions (suitable for small data sets only)</li> <li>Store increments (new image objects, new partitions in time series, etc)</li> <li>Store differences (deltas) between data set versions</li> </ul> </li> <li>Rule of thumb: You should version data whenever changes to the dataset occur that could impact its use, reproducibility, or compatibility with downstream systems.<ul> <li>Initial data set creation</li> <li>Data updates (new data, corrections, expansions)</li> <li>Data processing: after applying preprocessing steps (evaluate based on compute vs. storage requirements)</li> <li>Model training: maintain a version of the data for each trained model</li> <li>Performance: after subsampling or aggregating</li> <li>Experiments: when experimenting with different versions, including different features, etc.</li> <li>Collaboration: track contributions per team/person</li> <li>Decommissioning: store the last version of the data</li> <li>Scheduled: hourly, daily, weekly, etc.</li> </ul> </li> <li>Tools: LakeFS, DVC, Delta Lake, Git LFS\ud83c\udf4f</li> <li>\ud83c\udf4fWhat would the user have to think about when choosing between them ( thinking about the methodlodies that define and separate them)</li> </ul> </li> </ul>"},{"location":"explanations/explainability/","title":"Reading","text":"<ul> <li>As a primer, transferlab overview     and training</li> <li>From the book Trustworthy Machine Learning, Chapter 4</li> </ul>"},{"location":"explanations/fairness/","title":"Reading","text":"<ul> <li>Fairness in Machine Learning: A Survey</li> </ul>"},{"location":"explanations/interoperability/","title":"Interoperability","text":"<ul> <li>Where applicable, ensure that datasets and documentation are interoperable with standard regulatory frameworks and can be audited by authorities.<ul> <li>Standardize formats (e.g., CSV or Parquet files for tabular data)</li> <li>Centralize data storage (e.g., in a data lake or data warehouse) such that access to data sets can be given at any time, and it is not depending on data sets on local machines of ML Engineers and Data Scientists.</li> </ul> </li> </ul>"},{"location":"explanations/privacy-and-data-security/","title":"Privacy and data security","text":"<ul> <li> <p>Use anonymization or pseudonymization techniques where personal data is involved</p> <ul> <li>Masking: Replace sensitive information with placeholder characters or symbols.</li> <li>Generalization: Replace specific data values with more general ones to reduce specificity.</li> <li>Suppression: Remove sensitive data fields entirely or omit certain records to ensure privacy.</li> <li>Pseudonymization: Replace identifiable information with pseudonyms or tokens.</li> <li>Noise: Add random noise to numerical data to obscure exact values while preserving overall patterns.</li> <li>Differential Privacy: Introduce carefully calibrated noise to data or query results to ensure privacy guarantees (Laplace)</li> </ul> </li> <li> <p>Protect datasets against unauthorized access, breaches, and other security risks.</p> </li> </ul>"},{"location":"explanations/privacy-and-data-security/#reading","title":"Reading","text":"<ul> <li>When Machine Learning Meets Privacy: A Survey and   Outlook</li> </ul>"},{"location":"explanations/robustness/","title":"Robustness","text":""},{"location":"explanations/robustness/#robust-model-design","title":"Robust Model Design","text":"<ul> <li>Regularization Techniques: Apply L1/L2 regularization or dropout to prevent overfitting and improve generalization.</li> <li>Ensemble Learning: Combine predictions from multiple models to improve robustness against errors in individual models. Know when certain models in the ensemble are not suitable.</li> <li>Adversarial Training: Train the model on adversarial examples to enhance its resilience to malicious inputs.</li> </ul>"},{"location":"explanations/robustness/#testing","title":"Testing","text":"<ul> <li>Boundary Testing: Test the AI system with inputs near decision boundaries to evaluate its behavior under edge cases.</li> <li>Scenario-Based Testing: Simulate real-world scenarios, including extreme conditions, to assess system stability.</li> </ul>"},{"location":"explanations/robustness/#monitoring-and-feedback-loops","title":"Monitoring and Feedback Loops","text":"<ul> <li>Performance Monitoring: Continuously monitor the AI system\u2019s performance in production to detect deviations or degradation over time.</li> <li>Feedback Mechanisms: Implement systems that allow end-users to report issues or anomalies.</li> </ul>"},{"location":"explanations/robustness/#model-updating-and-retraining","title":"Model Updating and Retraining","text":"<ul> <li>Continuous Learning: Periodically retrain the model with updated data to adapt to changes in the operating environment.</li> <li>Version Control: Maintain and document model versions, including performance evaluations for each update.</li> </ul>"},{"location":"explanations/robustness/#reading","title":"Reading","text":"<p>From the book Trustworthy Machine Learning * Chapter 2.10 - 2.15</p>"},{"location":"explanations/technical-documentation/","title":"Technical Documentation","text":"<ul> <li>Article 11</li> </ul> <p>Providers of high-risk AI systems are required to prepare detailed technical documentation before placing the system on the market. This documentation must be kept up to date and should demonstrate the system's compliance with the AI Act's requirements. It should include a general description of the AI system, its intended purpose, design specifications, and information on performance evaluation. Small and medium-sized enterprises (SMEs) may provide this information in a simplified form, as specified by the EU. </p>"},{"location":"explanations/technical-documentation/#model-cards","title":"Model cards","text":"<p>A good model card is a structured document that provides clear, concise, and comprehensive information about a machine learning model:</p> <ul> <li>Model Overview:<ul> <li>Model Name and Version</li> <li>Model Description: A brief overview of the model's purpose, capabilities, and intended use.</li> <li>Contact Information: Details of the organization, department, team or individual responsible for the model, including contact information.</li> </ul> </li> <li>Intended Use:<ul> <li>Primary Use Cases: Description of the specific tasks the model is designed for.</li> <li>Out-of-Scope Use Cases: Explicitly state where the model should not be used</li> </ul> </li> <li>Dataset Information:<ul> <li>Training Data: Details of the dataset used to train the model, including its source, size, and key characteristics.</li> <li>Validation and Test Data: Information about datasets used for validation and testing (similar to training data).</li> <li>Preprocessing: Description of any preprocessing steps applied to the data.<ul> <li>normalization, encoding, handling missing values, feature engineering, etc.</li> </ul> </li> </ul> </li> <li>Performance Metrics:<ul> <li>Overall Performance: See metrics for Accuracy</li> <li>Subgroup Performance: Performance metrics broken down by demographic or contextual subgroups (e.g., age, gender, race, etc.).</li> <li>Benchmarks: Comparison against other models (or older versions of the model)</li> </ul> </li> <li>Fairness and Bias Analysis:<ul> <li>Evaluation: Results of bias testing across demographic groups, including fairness metrics used</li> <li>Mitigation: Actions taken to address identified biases in the model or training data.</li> </ul> </li> <li> <p>Limitations:</p> <ul> <li>Known Limitations: Description of scenarios where the model may not perform well or could produce unreliable results.</li> <li>Uncertainties: Aspects of the model's behavior that are not well understood or tested.</li> </ul> </li> <li> <p>Ethical Considerations:</p> <ul> <li>Potential Harms: Risks or harms that may arise from misuse or unintended use of the model.</li> <li>Privacy Concerns: Details on how the model handles sensitive data, compliance with GDPR.</li> </ul> </li> <li>Risk Management:<ul> <li>Risk Assessment: Identification of risks associated with the model and measures taken to mitigate them.</li> <li>Fail-Safes and Controls: Mechanisms for monitoring and managing model outputs, including fallback procedures.</li> </ul> </li> <li>Technical Specifications:<ul> <li>Model Architecture: A description of the underlying algorithm or architecture.</li> <li>Input and Output: Details of the expected input formats and output types</li> <li>Dependencies: Required software, libraries, or hardware for using the model.</li> </ul> </li> <li>Transparency and Explainability:<ul> <li>Explainability Techniques: Methods used to make the model's decision-making interpretable (e.g., SHAP, LIME).</li> <li>Interpretation Guidelines: Instructions for understanding and using model outputs responsibly.</li> </ul> </li> <li>Maintenance and Updates:\ud83c\udf4f<ul> <li>Update Schedule: Information about planned updates or retraining of the model.</li> <li>Changelog: A log of changes made to the model, datasets, or documentation over time.</li> <li>Use case dependent: what are example use cases? Can you group tasks?</li> </ul> </li> <li>Compliance Information:<ul> <li>Regulatory Compliance: Statement of compliance with relevant regulations (e.g., the EU AI Act, GDPR)</li> <li>Standards and Certifications: Details of standards followed (e.g., Code of Practice)</li> </ul> </li> <li>Usage Guidelines:<ul> <li>Installation and Deployment: Steps for deploying and using the model in various environments.</li> <li>Monitoring and Evaluation: Recommendations for ongoing performance monitoring and evaluation.</li> <li>Decommissioning: Guidance for safely retiring the model when it's no longer in use. State when the model has to be decommissioned.</li> </ul> </li> <li>Licensing:<ul> <li>Usage License: The terms under which the model can be used, modified, or distributed.</li> <li>Third-Party Content: Attribution and licensing for any third-party datasets, libraries, or tools used.</li> </ul> </li> </ul>"},{"location":"explanations/technical-documentation/#annex-iv-technical-documentation-details","title":"Annex IV: Technical Documentation Details","text":"<p>Annex IV provides a comprehensive list of elements that must be included in the technical documentation referred to in Article 11. This includes detailed descriptions of the AI system's design specifications, algorithms, training data sets, risk management systems, validation and testing procedures, performance metrics, and cybersecurity measures. The annex ensures that all relevant information is available to assess the system's compliance with the AI Act.</p>"},{"location":"explanations/traceability/","title":"Ensure traceability and documentation","text":"<ul> <li>Keep records of the data lifecycle, including the sources of data, selection criteria, and preprocessing steps.</li> </ul>"},{"location":"explanations/transparency/","title":"Transparency and Provision of Information","text":"<p>To get a more hands-on reading of Article 13, we shall assume, that there is a fix use-case to solve and the input data have a specified format (does this makes sense?).</p> <p>The first thing to notice is, that information must be provided to the user even on an event level. This means, that there must be an identifier for each event (inference call, failure, ...), which can be used to trace back the collected information. This is tightly connected to Article 12, which handles the requirements regarding record-keeping. We can elaborate on this by showing a simple event-based architecture using databases to persist the information and make them available to the user via an API.</p> <p>Let's consider which kind of information must be provided to the user (in the context of the AI act this is the deployer). The requirements described in Article 13 of the EU AI Act can be broadly seperated into two categories. The first category is static information about the system, which is not input dependent. The second category is the information about the output for a specific input (this is a point of discussion!).</p>"},{"location":"explanations/transparency/#static-information-about-the-system","title":"Static information about the system","text":"<p>This includes all information about the system, which is not input dependent. There seem to be quite an overlap with Hugging Face model cards.</p> <p>The static information can be further subdivided into those which are model-independent and those which are model-dependent.</p>"},{"location":"explanations/transparency/#model-independent-information","title":"Model-independent information","text":"<p>All information, which is not specific for a single model or may be stable over a long time:</p> <ul> <li> <p>identity and contact details of provider; Who is responsible for the system?</p> <ul> <li>13.3 a</li> <li>This does not need much of an explanation, but we can include it in our showcase implementation as well.</li> </ul> </li> <li> <p>intended purpose of a system; Which problem will be solved?</p> <ul> <li>13.3 b(i)</li> <li>This needs clarification about how detailed the description of the system   has to be and which language (in the sense of using machine learning terminology) should be used. We can provide an example for the simple case of classification and regression with the census data.</li> <li>A still vague definition is given in article 3.12.</li> </ul> </li> <li> <p>input data scheme; How to get an output of the model?</p> <ul> <li>13.2, 13.3 b(vi)</li> <li>Although this is not mentioned explicitly, it belongs to the general instructions of use; it has to be clear and accessible to the user, how the input schema has to look like to run inference of the model.</li> <li>Again, this information can be made accessible via an API, which provides the schema of the input data, and we can easily showcase this.</li> </ul> </li> </ul>"},{"location":"explanations/transparency/#model-dependent-information","title":"Model-dependent information","text":"<ul> <li> <p>description of the model; What kind of model is used?</p> <ul> <li>13.3 b(iv)</li> <li>this includes the description of the model type/architecture, hyperparameters and preprocessing. Here the task will be to show how to make all this information amenable for logging, e.g. automatically create a string representation.</li> </ul> </li> <li> <p>evaluation metrics for the model to show the performance to expect</p> <ul> <li>13.3 b(ii)</li> <li>I think which metrics (in the mathematical sense) to use for which ml problem   is pretty standard, e.g. accuracy, precision, recall, F1-score for a binary classification; Maybe it is best to not elaborate on this too much but reference a   good source for this?</li> <li>More important than the metrics themselves is the question of how the evaluation   was done, e.g. how the data was split or if cross-validation was used. This   is also tightly connected to the information about the training data. There   should be transparency about how to reproduce the evaluation result.</li> </ul> </li> <li> <p>statistics about restricted performance, e.g. subgroup performance;</p> <ul> <li>13.3 b(v)</li> <li>This is a point of discussion. We should explain in which scenario an isolated   view on a specific subgroup (may it be a group having a specific feature or   in a binary problem looking only at the positive labeled group, i.e. precision and recall) makes sense.</li> <li>We should make the connection to Article 11 and Annex IV</li> </ul> </li> <li> <p>information about the training data;</p> <ul> <li>13.3 b(vi)</li> <li>this information might be about the data collection process, any data transformation, basically the information collected from the data governance view point. Here it might be more challenging to find a good format for providing it   to the user, but maybe it also makes sense to make it available via an API?</li> <li>We should make the connection to Article 11 and Annex IV</li> </ul> </li> </ul>"},{"location":"explanations/transparency/#information-about-the-output-for-a-specific-input","title":"Information about the output for a specific input","text":"<p>In contrast to the information discussed so far, this includes all information which is specific to a single input. * logs for a specific inference call;     * 13.3 f     * based on an event-based architecture, we can provide a simple API to access the logs for a specific inference call and an instruction how to use the endpoints should be available.     * again, this is tightly connected to article 12 and the record-keeping requirements. * additional information, which allow for interpretation of the output.     * 13.3 b(vii), 13.3 d (connection to human oversight)     * the AI act is extremely vague in regard to this point. It uses phrase like       \"where applicable\" and \"enable deployers to interpret the output\".     * We should elaborate on how to make this more concrete. One way could be to speak       about the terms interpretability and explainability how they are used in the technical community, i.e. a discussion about intrinsically interpretable models and post-hoc interpretability methods for black-box models.</p>"},{"location":"explanations/transparency/#open-questions","title":"Open questions","text":"<ul> <li>since explainability (XAI) and interpretability are not mentioned explicitly in article 13, but might be helpful to enable the user to handle the output (\"interpret/ explain the output\") in a better way, should we include it in our explanations and showcase?<ul> <li>I think we can keep the explanation part short and reference to a good source for this.</li> <li>It should be included into the showcase, because the costs to actually implement it are not high and the information gain for the user is high.</li> <li>it is not clear how to deal with the terms \"where applicable\" and \"when appropriate\".</li> </ul> </li> <li>what part of the information collected for data governance should be made available to the user and how?<ul> <li>Can Hugging Face model cards be used as a template for this?</li> </ul> </li> <li>the term \"reasonably foreseeable misuse\" is used in article 13.b (iii) and defined in article 3.13.<ul> <li>In short, any possible not intended use due to the fact that humans are humans. E.g. they are lacking skills to use the system appropriately, I think of the typical it-support scenario or the user has evil intentions.</li> <li>Still, it is unclear how to include it in our showcase. Besides the explainability part, this will is the biggest open question.</li> </ul> </li> <li>Art. 13(3), (b)(iv): Get a sense for \"technical characteristics and capabilities\" - what's the intention and rationale behind this phrasing?</li> <li>What is the connection between Article 11, Annex IV and Article 13.3 b(v, vi)? Is it instruction for use vs. technical documentation (what is the difference)?</li> <li>The meaning and intention of Article 13.3 c is completely unclear (is this something like a \"diff\")</li> <li>What is the difference between the terms \"misuse\", \"abuse\" and \"attack\" in the context of the AI act?  </li> </ul>"},{"location":"explanations/transparency/#potential-tasks","title":"Potential tasks","text":"<ul> <li>Set up an event logging system and document its architecture in line with Art. 13(3), (f) and Art. 12</li> <li>starting from our use-case, think about potential misuses.<ul> <li>e.g. use the model for a different age span than in the training,   so the model will still give an output, but it is not semantically meaning full.</li> </ul> </li> <li>set up an automatic way to build the (provenance) information about the model, preprocessing and training data.</li> <li>the goal should be to create transparency and traceability about the complete processing pipeline</li> <li>e.g. sklearn pipeline steps documented included parameterization</li> <li>implement a basic API to provide the static information about the system.</li> <li>implement the usage of intrinsically explainable models and/or usage of post-hoc methods, have a look at the interpret package.</li> </ul> <p>Related Norms: * ISO/IEC FDIS 12792 (Transparency taxonomy of AI systems) * ISO/IEC TS 4213 * ISO/IEC AWI 4213 * ISO/IEC DTS 6254</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Describe what this project is all about</p>"},{"location":"getting-started/ai-act-in-a-nutshell/","title":"EU AI Act in a nutshell","text":"<p>The goal is to have an entry point into this project. Think of this as some kind of abstract or syllabus of the content or a tl;dr of the selected parts of the AI Act, which influence the implementation directly.</p> <p>Transparency and Provision of Information</p>"}]}