{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#_1","title":"Home","text":"<p>Welcome to the trustworthy-AI pipeline (twai) project!</p> <p>The introduction of the EU AI Act adds a new dimension of requirements to the implementation of high-risk artificial intelligence systems. At first glance, the legal text may not clearly outline how these compliance requirements affect the practical aspects of AI system development.</p> <p>This project aims to bridge that gap by offering valuable insights and practical examples to help accelerate the integration of compliance requirements into real-world projects. It is designed as an entry point for professionals approaching the AI Act from a technical perspective. A key message we want to convey is that adhering to software engineering best practices already provides a strong foundation for achieving compliance.</p> <p>To support this goal, the project will present an exemplary system architecture for a trustworthy AI system, offer concrete implementations of essential software components, and establish clear connections between engineering best practices and the specific requirements for high-risk systems under the AI Act.</p> <p>This is a living document and will be continuously updated as more legal and technical information becomes available. Your feedback and discussions are highly encouraged, as they will help us refine the content and ensure its relevance to the community.</p> <ul> <li> <p> Reader's Guide</p> <p>Find out how to navigate the resources on this website</p> </li> <li> <p> Showcase</p> <p>A worked end-to-end ML pipeline for a (hypothetical) high-risk AI system</p> </li> <li> <p> Engineering Practice</p> <p>Engineering techniques for trustworthy AI systems</p> </li> <li> <p> AI Act Conformity</p> <p>Requirements originating from the AI Act and connections to engineering practice</p> </li> </ul>"},{"location":"readers-guide/","title":"Reader's Guide","text":"<p>Legal Disclaimer</p> <p>The information provided on this website is for informational purposes only and does not constitute legal advice. The tools, practices, and mappings presented here reflect our interpretation of the EU AI Act and are intended to support understanding and implementation of trustworthy AI principles. Following this guidance does not guarantee compliance with the EU AI Act or any other legal or regulatory framework.</p> <p>We are not affiliated with, nor do we endorse, any of the tools listed on this website.</p>"},{"location":"readers-guide/#who-is-this-website-for","title":"Who is this website for?","text":"<p>This project is intended for AI practitioners, data scientists, and engineers who are interested in implementing trustworthy AI systems under the European AI Act.</p> <p>The information is applicable to both low- and high-risk AI systems, since it establishes a sound engineering foundation for developing trustworthy AI systems in general.</p> <p>Most content is written from the perspective of providers of AI systems under the AI Act. Deployers of such AI systems may also find the information useful, as it provides a solid foundation for understanding the requirements and best practices for compliance.</p>"},{"location":"readers-guide/#syllabus","title":"Syllabus","text":"<p>This project is structured around three interconnected areas, each addressing a key aspect of AI system development in the context of EU AI Act compliance. The content is modular and designed for flexible navigation\u2014readers are encouraged to explore the connections between legal requirements, engineering practices, and implementation choices.</p> <p>A central aim of this work is to bridge the gap between regulatory obligations and technical execution. By making the relationships between legal texts, software engineering practice, and actual implementations explicit, we hope to support teams in building AI systems that are not only effective, but also aligned with compliance goals from the ground up.</p> <ul> <li>Showcase: Presents a practical use case that runs throughout the project. It illustrates the application of compliance concepts in context and includes a risk classification based on the EU AI Act.</li> <li>Engineering Practice: Provides a set of software engineering best practices that form the technical foundation for compliance. Each practice is mapped to relevant AI Act provisions to guide implementation.</li> <li>AI Act Conformity: Breaks down the legal requirements of the EU AI Act and connects them to concrete engineering actions, helping translate regulatory language into actionable guidance.</li> </ul> <p>Depending on your background, different entry points may make more sense. If you're from a technical or engineering background, you might start with Engineering Practice. If you're approaching from a legal, regulatory, or policy perspective, AI Act Conformity may be the best starting point. If you prefer to see things in context first, the Showcase offers a concrete example that ties the other sections together.</p>"},{"location":"readers-guide/#recommended-knowledge","title":"Recommended Knowledge","text":"<ul> <li>A basic understanding of machine learning and AI concepts</li> <li>An understanding of the terminology in Art. 3 of the AI Act</li> <li>Familiarity with Python programming, if you want to follow along with the code examples</li> <li>Software engineering best practices for ML:<ul> <li>See the Beyond Jupyter series for an introduction</li> </ul> </li> </ul>"},{"location":"readers-guide/#how-to-navigate-this-website","title":"How to navigate this website?","text":"<p>A key goal of this project is making connections between AI Act requirements and engineering best practices explicit. Since these relationships are often many-to-many rather than one-to-one, the website uses multiple navigation tools to help you explore these connections:</p> <ul> <li> <p>Overview Tables: Both the Engineering practice and AI Act Conformity sections contain summary tables showing the relationships between practices and requirements.</p> </li> <li> <p>Information Boxes: Each page in these sections includes specialized information boxes:</p> <ul> <li>Pages in Engineering practice include Compliance Information Boxes linking to relevant AI Act requirements</li> <li>Pages in AI Act Conformity include Engineering Information Boxes linking to relevant engineering practices</li> </ul> </li> </ul> <p>These tools help you navigate the complexity of implementing AI Act requirements through sound engineering practices, allowing you to move seamlessly between legal requirements and their technical implementations.</p>"},{"location":"readers-guide/#compliance-information-boxes","title":"Compliance Information Boxes","text":"<p>The pages on engineering practices include information boxes that reference applicable parts of the AI Act. For an example, see the box at the bottom of this section. These references are intended to help you understand how the engineering practices relate to the legal requirements of the AI Act.</p> <p>Please note that these references do not imply that following a given engineering practice will guarantee compliance with the referenced parts of the AI Act.</p> <p>Compliance Info</p> <p>This is an example for the Compliance Info box.</p> <ul> <li>Art. 14 (Human Oversight), in particular:<ul> <li>Art. 14(4)(e), continuous monitoring the operation of the systems helps     to detect conditions requiring potential intervention</li> </ul> </li> </ul>"},{"location":"readers-guide/#engineering-information-boxes","title":"Engineering Information Boxes","text":"<p>Given the requirements of the AI Act, it is interesting to link certain requirements to the corresponding engineering practices. These connections are represented by information boxes, like the one at the bottom of this section. You can think of them as the reverse direction of the connections collected in Compliance Information Boxes.</p> <p>Following these practices does not guarantee compliance, as there may be parts of a requirement that are not amenable to automation.</p> <p>Engineering Info</p> <p>This is an example for the Compliance Info box.</p> <ul> <li>Operational Monitoring:<ul> <li>Art. 14(4)(e), continuous monitoring the operation of the systems helps     to detect conditions requiring potential intervention</li> </ul> </li> </ul>"},{"location":"readers-guide/#page-backlinks","title":"Page Backlinks","text":"<p>You might notice that some pages contain hyperlinks to other pages marked with an \u2199 arrow at the top of the page:</p> <p>These links point to other pages on this website that contain links to the current page. They help you easily discover and navigate related content, even if it is not mentioned in the info box of the page.</p>"},{"location":"readers-guide/#what-is-out-of-scope","title":"What is out of scope?","text":""},{"location":"readers-guide/#assessing-your-use-cases-risk-category","title":"Assessing your use case's risk category","text":"<p>If you are unsure how the AI Act applies to your use case, you should first try to determine the appropriate risk classification for your use case.</p> <p>The following resources can help you with that:</p> <ul> <li>The appliedAI Institute Risk Classification Database, a comprehensive list of practical examples of high-risk and non high-risk use cases on AI systems under the EU AI Act</li> <li>The EU Act Compliance Checker, an interactive tool that helps you assess the risk category of your AI system and applicable requirements from the AI Act</li> </ul>"},{"location":"readers-guide/#systems-covered-under-european-union-harmonisation-legislation","title":"Systems covered under European Union harmonisation legislation","text":"<p>If your system is covered under European Union harmonisation legislation (see Annex I), both the AI Act and the rules in those laws apply. We do not cover these additional requirements in this showcase.</p>"},{"location":"readers-guide/#general-purpose-ai-models","title":"General-purpose AI models","text":"<p>The AI Act also creates a special category of AI systems, called general purpose AI systems, where a general purpose AI model is integrated into a larger software product. These are a special category of AI systems, for which the rules for high-risk systems continue to apply. If you are building a general-purpose AI system, you will still find the engineering resources useful, since they mirror good software engineering practices.</p> <p>The AI Act also creates separate rules for general purpose AI models (e.g. foundation models like LLMs) and AI systems. However, this resource is specifically tailored to the requirements for high-risk AI systems.</p> <p>The rules for general purpose AI models are contained in Chapter V of the AI Act. For guidance on how to comply with Chapter V of the AI Act, you might find the following resources helpful:</p> <ul> <li>The European AI Office, which oversees the implementation of the AI Act, while ensuring compliance, fostering innovation, and coordinating AI governance across EU Member States.</li> <li>The General-Purpose AI Code of Practice, and an explorer for the draft Code of Practice.</li> </ul>"},{"location":"readers-guide/#relationship-to-privacy-and-data-protection","title":"Relationship to Privacy and Data Protection","text":"<p>The project does not address the relationship between the AI Act and the General Data Protection Regulation (GDPR), or the question of privacy in Machine Learning more broadly.</p> <p>The AI Act and the GDPR are separate legal frameworks, and while they may overlap in some areas, they have different objectives and requirements. Although some of the engineering practices on this website also help you comply with the GDPR, we do not explicitly consider these privacy and data protection aspects.</p>"},{"location":"readers-guide/#on-the-iterative-nature-of-this-document","title":"On the iterative nature of this document","text":"<p>As the legislation and standardization around artificial intelligence in the European Union still evolve, this showcase is subject to changes based on different types of developments. These can include, but are not limited to:</p> <ol> <li>Community feedback, like corrections and additions of missing content,</li> <li>Changes and updates to standards on artificial intelligence, such as through CEN-CENELEC (see below),</li> <li>Changes to the technology stack used in the example high-risk AI system.</li> </ol> <p>Providers of high-risk AI systems specifically may also consider the following note on the future availability of EU harmonized standards.</p> <p>Note</p> <p>Providers of high-risk AI systems must build their systems according to the rules in Chapter III, Section 2 of the AI Act as discussed in the AI Act Conformity section of this website.</p> <p>To benefit from a presumption of conformity with these rules, providers must apply harmonized standards in their design and implementation process. These standards are due to be published by CEN-CENELEC between late 2025 and early 2026.</p> <p>Please refer to the CEN-CENELEC JTC 21 website for up-to-date information.</p>"},{"location":"conformity/","title":"Cross-Reference: AI Act Articles to Engineering Practices and System Components","text":"Article Relevant Engineering Practices Art. 9:Risk Management System Art. 10:Data and Data Governance Data Quality Data Versioning Bias Mitigation Art. 11:Technical Documentation Model Registry Experiment Tracking Explainability Art. 12:Record-Keeping Inference Log Data Versioning Experiment Tracking Orchestration Art. 13:Transparency and Provision of Information to the Deployers Explainability Experiment Tracking Model Registry Operational Monitoring Art. 14:Human Oversight Experiment Tracking Explainability Model Monitoring Operational Monitoring Art. 15:Accuracy, Robustness and Cybersecurity Experiment Tracking Containerization Bias Mitigation Model Monitoring Operational Monitoring Model Serving"},{"location":"conformity/accuracy-robustness-cybersecurity/","title":"Accuracy, Robustness and Cybersecurity","text":"<p>This article requires that high-risk AI systems be designed to achieve appropriate levels of accuracy, robustness, and cybersecurity. However, these terms are not clearly defined within the context of the AI Act. It makes sense to get an understanding, which dimensions of the AI systems shall be addressed.</p> <p>Engineering Info</p> <p>Below we map specific engineering practices and system components from our showcase implementation to demonstrate practical automation possibilities that support EU AI Act compliance.</p> <ul> <li>Bias Mitigation:<ul> <li>Art. 15(4), Bias mitigation increases resilience against bias-related errors and inconsistencies</li> </ul> </li> <li>Containerization:<ul> <li>Art. 15(1), Containers provide a consistent and isolated runtime environment</li> <li>Art. 15(4), Containers allow for redundant, reproducible and reliable application execution</li> </ul> </li> <li>Experiment Tracking:<ul> <li>Art. 15(3), Experiment tracking captures used performance metrics and levels of accuracy</li> </ul> </li> <li>Model Monitoring:<ul> <li>Art. 15(4), Resilience and robustness through continuous monitoring of model performance</li> </ul> </li> <li>Operational Monitoring:<ul> <li>Art. 15(4) Monitoring and alerting can help detect and mitigate potential robustness and availability issues</li> <li>Art. 15(5) Monitoring is a crucial part of threat detection</li> </ul> </li> </ul>"},{"location":"conformity/accuracy-robustness-cybersecurity/#accuracy","title":"Accuracy","text":"<p>Art. 15(2) makes it clear that accuracy in the context of the AI Act does not equal the established metric by the same name, but rather should be understood as a set of quality measures of a system. In the case of a classification the accuracy metric might one but not the only applicable quality measure.</p> <p>Which specific metric is most suitable as quality measure heavily depends on the problem type at hand. A good overview for the case of classification problems can be found in the norm document ISO/IEC TS 4213:2022. The upcoming replacement of this specification broadens the scope to include measures of performance for classification, regression, clustering and recommendation tasks.</p> <p>Annex IV mandates that the technical documentation must include a description of the metrics used to measure the accuracy of the system. In order to achieve a better level of transparency, information about the accuracy of AI systems should also be included in their instructions for use.</p>"},{"location":"conformity/accuracy-robustness-cybersecurity/#robustness","title":"Robustness","text":"<p>Systems should perform consistently throughout their lifecycle, be resilient to errors and faults, and have measures in place to mitigate risks associated with cybersecurity threats.</p> <p>ISO/IEC 25059:2023 (Artificial intelligence concepts and terminology) defines robustness as the \"ability of a system to maintain its level of performance under any circumstances\". This contrasts with the definition of accuracy, which focuses on the performance on a distinct evaluation data set.</p> <p>In practice, this means that there is a trade-off between maximizing the accuracy of a system, as evaluated on a test set, and ensuring its robustness in the real world.</p> <p>A valuable synergy emerges from making AI systems more interpretable and explainable as this can also help to improve their robustness, by providing measures which allow to identify and mitigate potential issues with the system's performance. Model monitoring techniques like anomaly detection can also be used to identify potential issues with the system's performance.</p> <p>See chapter 10 of the ISO/IEC TS 25058:2024 (Guidance for quality evaluation of artificial intelligence (AI) systems) for guidance on how to evaluate the robustness of AI systems. For neural networks in particular, also refer to the ISO/IEC TR 24029 (Assessment of the robustness of neural networks) series for concrete assessment methods.</p>"},{"location":"conformity/accuracy-robustness-cybersecurity/#prevention-of-bias-feedback-loops","title":"Prevention of Bias Feedback Loops","text":"<p>High-risk AI systems that continually learn after being put in production should be designed to minimize risks of producing biased outputs based on feedback loops. </p> <p>To make this more concrete, consider our use-case example of an assistant system for hiring processes. If the system (without proper mitigation measures) is biased against applicants from a certain demographic, it may negatively affect the hiring chances for the applicants. If the output of the HR system is subsequently used to retrain the AI system, it may lead to a feedback loop that further amplifies the pre-existing bias.</p> <p>See the page on bias mitigation to understand how to address bias concerns when building an AI system.</p>"},{"location":"conformity/accuracy-robustness-cybersecurity/#cybersecurity","title":"Cybersecurity","text":"<p>Besides the classical issues of cybersecurity, such as confidentiality, integrity, and availability, the AI Act also requires that high-risk AI systems be designed to be resilient attacks specific to AI systems:</p> <ul> <li>Data poisoning</li> <li>Model poisoning</li> <li>Adversarial attacks / model evasion</li> <li>Confidentiality attacks, like model inversion or model theft</li> </ul> <p>In practice, this means that the cybersecurity measures for high-risk AI systems should encompass both traditional cybersecurity measures and the tactics, techniques, and procedures that are unique to AI systems. Framework like MITRE ATLAS can help to identify and mitigate these risks.</p> <p>It is worth noting that the cybersecurity provisions of the AI Act interact with other legal frameworks, such as the EU Cyber Resilience Act (see below), which also impose requirements on the cybersecurity of software and hardware products.</p>"},{"location":"conformity/accuracy-robustness-cybersecurity/#resources","title":"Resources","text":"<p>Note</p> <p>According to Art. 15(2), we can expect concrete guidance for the implementation of these requirements in the form of upcoming benchmarks and measurement methodologies.</p> <ul> <li>ISO/IEC TS 4213:2022:<ul> <li>provides a broad overview of statistical metrics suitable for various forms of classification problems.</li> <li>The upcoming replacement, currently in its draft stage as ISO/IEC AWI 4213 broadens the scope to include measures of performance for classification, regression, clustering and recommendation tasks</li> </ul> </li> <li>ISO/IEC TS 25058:2024 (Guidance for quality evaluation of artificial intelligence (AI) systems)</li> <li>ISO/IEC TR 24029 (Artificial Intelligence (AI) \u2014 Assessment of the robustness of neural networks)</li> <li>Hamon, et al. (2020) - Robustness and Explainability of Artificial Intelligence, EUR 30040 EN, Publications Office of the European Union</li> <li>OWASP Machine Learning Security Top Ten</li> <li>MITRE ATLAS (Adversarial Threat Landscape for Artificial-Intelligence Systems)</li> <li>Annex I of the EU Cyber Resilience Act list essential requirements for the cybersecurity of products with digital elements.</li> </ul>"},{"location":"conformity/data-governance/","title":"Data and Data Governance","text":"<p>Engineering Info</p> <p>Below we map specific engineering practices and system components from our showcase implementation to demonstrate practical automation possibilities that support EU AI Act compliance.</p> <ul> <li>Bias Mitigation:<ul> <li>Art. 10(2)(g): Appropriate measures for bias detection, prevention, and mitigation</li> <li>Art. 10(3): Assert appropriate statistical properties regarding natural persons related to the use of the high-risk AI system</li> <li>Art. 10(5): Use of special categories of personal data in bias detection and correction</li> </ul> </li> <li>Data Versioning:<ul> <li>Art. 10(2)(b): Tracking the origin of training and test data</li> <li>Art. 10(2)(e): Assessment of availability, quantity and suitability of the data sets</li> </ul> </li> <li>Data Quality:<ul> <li>Art. 10(2)(c): Tracking data preparation steps like labelling, cleaning, imputation, enrichment;</li> <li>Art. 10(3): Ensuring complete, error-free, and sufficiently representative (for the respective application) training and testing data sets</li> </ul> </li> </ul>"},{"location":"conformity/data-governance/#non-technical-requirements","title":"Non-technical requirements","text":"<p>Data governance measures serve two purposes in general in a high-risk AI use case: Transparency and accountability as well as quality assurance. The former is important for ensuring that no systematic biases in the data influence the behavior of the AI system, and to ensure reproducibility to allow ML engineers to identify issues in the training and evaluation process. The latter, meanwhile, is important to set up a rigorous performance evaluation (\"benchmarking\") process to ensure that an AI system performs as expected, and meets quality and safety standards.</p> <p>Datasheets can be used to document training methodologies, metadata and relevant features and characteristics of the used datasets, as well as other non-technical aspects around the system such as the data provenance, how labels were obtained, and cleaning methods used.</p> <p>In practice, an organizational focus on thorough data management pays dividends in other areas of the machine learning lifecycle as well. Advantages of this include better awareness on different forms and versions of data, helping in crafting reproducible machine learning experiments, and higher availability of data assets to different teams, resulting in more efficient parallel workflows.</p> <p>A point to remember is that since there are different aspects of data governance, this usually results in the adoption of one or more software tools to manage data. In many cases, this means additional complexity in managing deployments, cloud operations, and/or higher staffing costs when employing personnel dedicated to managing data infrastructure. It is therefore important to weigh the benefits of the chosen solution against its estimated complexity and costs, especially for smaller companies and teams.</p>"},{"location":"conformity/data-governance/#relation-to-other-articles","title":"Relation to other Articles","text":"<p>Data governance is linked to the following articles:</p> <ul> <li>Art. 12 (Record Keeping): For transparency on how high-risk AI systems perform in practice,     and logging of events and input data over the course of the system's lifetime.</li> <li>Annex IV (Technical Documentation): To ensure the quality (especially correctness and completeness) of input data and to document engineering practices around the used data.</li> </ul>"},{"location":"conformity/human-oversight/","title":"Human Oversight","text":"<p>Article 14 of the EU AI Act establishes requirements for human oversight of high-risk AI systems. It essentially imposes three types of obligations.</p> <p>First, that the system is observable/monitorable by the human overseer. In other words, they must be able to monitor its operation to detect and address anomalies, dysfunctions and unexpected performance. For example through warnings in a dashboard. Almost all engineering practices we present are connected to this type.</p> <p>Second, that the system is designed/provided such that the overseer is informed. In other words, they should not over-rely on the AI system and should be able to understand its outputs. This could be either through features built into the system, shared through instructions of use, or through corporate up-skilling.</p> <p>Finally, that the system is controllable by the overseer. In other words, they should be able to disregard, override or reverse the outputs and intervene or interrupt the system. For example by manually editing the output of a system.</p> <p>When implementing human oversight requirements, it's crucial to understand that automation opportunities are primarily limited to the collection of metrics and supporting information. The substantive aspects of oversight\u2014interpretation, decision-making, and intervention\u2014must remain human-driven.</p> <p>Engineering Info</p> <p>Below we map specific engineering practices and system components from our showcase implementation to demonstrate practical automation possibilities that support EU AI Act compliance.</p> <p>Related engineering practice focus on implementing comprehensive oversight measures that enable human supervision of AI systems. These measures must be integrated throughout the system's lifecycle\u2014from design to deployment and operation\u2014ensuring humans can make informed interventions.</p> <ul> <li>Experiment Tracking:<ul> <li>Art. 14(4)(a), understand the limitation of the underlying model by     interpreting performance on reference data</li> </ul> </li> <li>Explainability:<ul> <li>Art. 14(3)(a), automated XAI features can be built directly into     the system to generate explanations</li> <li>Art. 14(4)(c), explainability approaches provide the necessary information     to enable human interpretation of system outputs</li> <li>Art. 14(4)(d), automated explanations provide the basis for humans     to decide whether to use or disregard outputs</li> </ul> </li> <li>Model Monitoring:<ul> <li>Art. 14(4)(a), automated tracking of drift and performance degradation     helps to understand the capacities of the system during its lifetime</li> <li>Art. 14(4)(e), observing degradation overtime enables to intervene and     initialize a retraining, for example</li> </ul> </li> <li>Operational Monitoring:<ul> <li>Art. 14(4)(e), continuous monitoring the operation of the systems helps     to detect conditions requiring potential intervention</li> </ul> </li> </ul>"},{"location":"conformity/human-oversight/#non-technical-requirements","title":"Non-technical Requirements","text":"<p>The text of Art. 14 itself focuses on the suitability of oversight measures, which should enable human operators. There is a rather implicit requirement, namely that the designated person has to be able to digest, interpret those and launch appropriate actions based on the bespoken oversight measures. While the technical features of oversight (e.g. explainability, monitoring, alerts) provide the means, they must be matched with human readiness to act on them. This is also connected to the requirements stated in Art. 4 (AI literacy).</p>"},{"location":"conformity/human-oversight/#relation-to-other-articles","title":"Relation to other Articles","text":"<ul> <li>Art. 4 (AI literacy): Persons involved in designing and operating AI systems are appropriately trained. </li> <li>Art. 13 (Transparency and Provision of Information to Deployers): Provide     the context necessary for humans to interpret system behavior and make informed     decisions about its use.</li> </ul>"},{"location":"conformity/instructions-for-use/","title":"Transparency and Provision of Information","text":"<p>Article 13 mandates that high-risk AI systems must be accompanied by instructions for use, so that their operation is sufficiently transparent to deployers.</p> <p>This document equips deployers with the necessary information to ensure that they can operate the system appropriately and interpret its output.</p> <p>A number of engineering practices can help providers to comply with the requirements of Article 13 by ensuring that the required information is collected in a structured and accessible manner.</p> <p>Engineering Info</p> <ul> <li>Explainability:<ul> <li>Art. 13(1): Explainability techniques contribute to a transparent system operation</li> <li>Art. 13(3)(b)(iv)(vii): Explainability techniques directly provide information relevant to explain the system's output</li> <li>Art. 13(3)(d): Explainability techniques can be used in the human oversight process to interpret the system's behavior</li> </ul> </li> <li>Experiment Tracking:<ul> <li>Art. 13(3)(b)(ii): Experiment tracking captures used performance metrics and levels of accuracy</li> </ul> </li> <li>Model Cards:<ul> <li>Art. 13(3)(b): Model-specific information can be documented in a model card</li> <li>Art. 13(3)(e): Computational/hardware resources needed can be documented in a model card</li> </ul> </li> <li>Model Registry:<ul> <li>Art. 13(3)(b)(iv): Logging the model architecture and hyperparameters makes the system characteristics transparent</li> </ul> </li> <li>Operational Monitoring:<ul> <li>Art. 13(3)(e): Monitoring the operation of the system enables to provide statistics about the system resource usage</li> </ul> </li> <li>Data Versioning:<ul> <li>Art. 13(3)(b)(vi): Data versioning ensures that the data used for training and validation is documented and can be traced back</li> </ul> </li> </ul>"},{"location":"conformity/instructions-for-use/#contents","title":"Contents","text":"<p>The information contained in the instructions for use roughly falls into two categories:</p> <ul> <li>Information about the system in general</li> <li>Information about the system that is specific to the model used</li> </ul> <p>There is significant overlap between the information required for the instructions for use and the information required for the technical documentation of high-risk AI systems. While the kind of information is the same, the intended audiences are different. Article 13 requires the instructions of use to be accessible and comprehensible to the deployer. In other words, providers must give some thought to the technical capabilities and knowledge of the deployer before drafting the instructions.</p> <p>Both documents can benefit from a structured approach to documentation, such as the use of model cards or experiment tracking.</p>"},{"location":"conformity/instructions-for-use/#generic-information","title":"Generic information","text":"<p>All information which is not specific for a single model or may be stable over a long time:</p> <ul> <li>Art. 13(3)(a): the identity and contact details of provider</li> <li>Art. 13(3)(b):<ul> <li>(i): intended purpose of the system</li> <li>(vi): information about the expected input data schema; relevant information about training/validation data sets</li> </ul> </li> <li>Art. 13(3)(e):<ul> <li>Computational/hardware resources needed</li> <li>Expected lifetime of the system</li> <li>Necessary maintenance and care measures (including software updates)</li> </ul> </li> </ul>"},{"location":"conformity/instructions-for-use/#model-specific-information","title":"Model-specific information","text":"<p>Other parts of the information to be included in the instructions for use refer to characteristics of the actual machine learning model employed in the system.</p> <ul> <li>Art. 13(3)(b):<ul> <li>(ii): expected level of accuracy, metrics, robustness, and cybersecurity, used for testing and validation of the system; potential circumstances that may impact these characteristics (Art. 15)</li> <li>(iii): known or forseeable circumstances which may lead to a risk (Art. 9); can depend on the model's type</li> <li>(iv): technical characteristics and capabilities of the system relevant to explain its outputs</li> <li>(v): statistics about the system's performance regarding specific (groups of) persons</li> </ul> </li> <li>Art. 13(3)(d): Human-oversight measures under Art. 14; technical measures that aid the interpretation of system outputs</li> <li>Art. 13(3)(f): Information about record-keeping mechanisms under Art. 12 (collection, storage, and interpretation)</li> </ul>"},{"location":"conformity/record-keeping/","title":"Record-Keeping","text":"<p>Article 12 of the AI Act sets out requirements for record-keeping by providers of high-risk AI systems.</p> <p>Engineering Info</p> <ul> <li>Inference Log, logging inference events including corresponding meta information         allows in particular for:<ul> <li>Art. 12(2)(a): Identifying inputs, which cause an unwanted behavior of the system</li> <li>Art. 12(2)(b): Drift detection as a realization of a post-market monitoring</li> <li>Art. 12(2)(c): Monitoring of the AI system's operation</li> </ul> </li> <li>Experiment Tracking:<ul> <li>Art. 12(2)(a): Backtracking to used training data</li> </ul> </li> <li>Data Versioning:<ul> <li>Art. 12(2)(a): Identifying used datasets (training and reference data) by a unique identifier</li> </ul> </li> <li>Orchestration:<ul> <li>Art. 12(2)(a): Reconstructing a complete model generation process through usage     of a unique run id  </li> </ul> </li> </ul>"},{"location":"conformity/record-keeping/#what-is-record-keeping","title":"What is record-keeping?","text":"<p>Record-keeping is the practice of keeping records of events (logs) of an AI system throughout its lifetime. This includes the automatic collection, storage, and processing of appropriate data that is relevant to the intended purpose of the AI system.</p>"},{"location":"conformity/record-keeping/#what-should-be-recorded","title":"What should be recorded?","text":"<p>The AI Act does not provide specific guidance on what concrete data should be recorded. Rather, the necessary data should be determined based on the results of the risk assessment and the intended purpose of the AI system, so that any deviations from the safe operation of the AI system can be detected.</p>"},{"location":"conformity/record-keeping/#general-requirements","title":"General requirements","text":"<p>Art. 12(2) calls for a compliant record-keeping system to capture data that allows:</p> <ul> <li>Identification of situations that may result in the system presenting a risk within the meaning of Art. 79(1)</li> <li>Identification of situations that may lead to a substantial modification of the AI system</li> <li>Post-market monitoring of the AI system as defined in Art. 72</li> <li>Monitoring of the AI system's operation by the deployer, as defined in Art. 26(5).</li> </ul> <p>The draft standard prEN ISO/IEC 24970 (Artificial intelligence \u2014 AI system logging) will provide concrete guidance on the implementation of the event logging. It is intended to be used with a risk management system, such as required by Art. 9.</p> <p>For special categories of AI systems, such as remote biometrics identification systems, Art. 12(3) mandates additional data to be recorded. However, these systems are out of scope for this website.</p>"},{"location":"conformity/record-keeping/#process-description","title":"Process description","text":"<p>The following diagram illustrates a schematic process for identifying the necessary logs to be kept for a high-risk AI system:</p> <pre><code>flowchart TB\n  S1[**Step 1**: Define the purpose, condition of use, pre-planned changes] --&gt; S2[**Step 2**: Identify relevant situations] --&gt; S3[**Step 3**: Identify validation conditions] --&gt; S4[**Step 4**: Define what to log per risk] --&gt; S5[**Step 5**: Implement &amp; report]\n\nS1 -.-&gt; s1o@{ shape: doc, label: \"AI system scope and pre-planned changes\" }\nS2 -.-&gt; s2o1@{ shape: docs, label: \"Situations potentially leading to risk\" }\nS2 -.-&gt; s2o2@{ shape: docs, label: \"Situations potentially leading to substantial modification\" }\nS3 -.-&gt; s3o@{ shape: docs, label: \"Validation conditions\" }\nS4 -.-&gt; s4o@{ shape: docs, label: \"Log event characteristics\" }\nS5 -.-&gt; s5o1@{ shape: st-rect, label: \"Log implementation\" }\nS5 -.-&gt; s5o2@{ shape: database, label: \"Events logs\" }\ns5o1 -.-&gt; s5o2\n\nclassDef doc fill:#d9e7fe;\n\nclass s1o,s2o1,s2o2,s3o,s4o,s5o1,s5o2 doc;</code></pre> <p>For each of the identified risks and substantial modifications, the following action should be taken (step 4 in the flowchart):</p> <ul> <li>Developer consultation: Check with developers if existing logs can track the events that lead to the risk. If not, define new events that need to be logged.</li> <li>Log importance evaluation: Assess the cost/benefit ratio of logging the new events.</li> <li>Log characteristics: Define the frequency, resolution, format, and other characteristics for each log.</li> </ul> <p>Lastly, implement the logging and reporting to the governance team (step 5):</p> <ul> <li>Comprehensive logs: Capture who, what, when, and outcomes of key events</li> <li>Stakeholder reporting: Set up processes for accessing logs and reporting</li> <li>Access management: Define who can access logs and under what conditions</li> <li>Model logging: Record input/output data, timestamps, expected outputs, and user changes</li> <li>Violation logs: Extend logs to include performance and MLOps KPI violations</li> <li>Detailed event logs: Maintain logs of significant events, including performance metrics and user actions</li> </ul>"},{"location":"conformity/record-keeping/#post-market-monitoring","title":"Post-Market Monitoring","text":"<p>Providers of high-risk AI systems must establish a post-market monitoring system to ensure that the AI system continues to comply with the requirements of the AI Act.</p> <p>Art. 3, point (25) defines a post-market monitoring system as follows:</p> <p>[...] all activities carried out by providers of AI systems to collect and review experience gained from the use of AI systems they place on the market or put into service for the purpose of identifying any need to immediately apply any necessary corrective or preventive actions;</p> <p>A post-market monitoring system for high-risk AI systems must fulfill the following requirements, as set out in Art. 72:</p> <ul> <li>Be established and documented based on a post-market monitoring plan, proportionate to the risk of the AI system</li> <li>Allow for systematic collection, documentation, and analysis of performance data to evaluated the ongoing compliance of the AI system with the requirements for high-risk AI systems</li> <li>If necessary, interactions with other AI systems must be analyzed</li> </ul> <p>If the system falls under established Harmonized legislation under Annex I(A) and a post-market surveillance system is already in place, the requirements of Art. 72 can be fulfilled by integrating the template from Art. 72(3) with the existing systems/plans.</p> <p>In particular, the EU Commission will publish a template for the post-market monitoring plan by February 2026 (see Art. 72(3)). This template will describe the elements to be included in the plan as part of the technical documentation for the AI system.</p>"},{"location":"conformity/record-keeping/#monitoring-of-operations-by-deployers","title":"Monitoring of Operations by Deployers","text":"<p>Deployers of high-risk AI systems must monitor the operation of the AI system to ensure that it complies with the requirements of the AI Act. Art. 26(5) requires them to:</p> <ul> <li>Monitor systems based on the instructions for use</li> <li>If using the system (in accordance with the instructions for use) could present a risk under Art. 79(1), to inform the provider or distributor and the relevant market surveillance authorities (see Art. 3, point (26)) and suspend use of the system.</li> <li>Report any serious incidents (see Art. 3, point (49)) to the provider first, and then to importer/distributor and the relevant market surveillance authorities of that incident.</li> </ul> <p>Notifications to the providers should be made in accordance with Art. 72, which ties together the responsibilities of the deployer and the provider in the post-market monitoring of the AI system. In case the provider cannot be reached, the notification duties outlined in Art. 73 fall on the deployer.</p> <p>Special provisions apply to law enforcement authorities and financial institutions, but are out of scope for this document.</p> <p>Art. 26(6) requires the automatically generated logs to be kept for a period appropriate to the intended use of the system, but at least 6 months. If any other legal requirements apply (e.g., recording requirements under GDPR), the logs must be kept for the longer of the two periods.</p>"},{"location":"conformity/risk-management-system/","title":"Risk Management System","text":"<p>Article 9 of the EU AI Act establishes requirements for implementing a risk management system to identify, analyze, and manage risks and threats arising from the use of the AI system \"to health, safety or fundamental rights\", when used for its intended purpose or under reasonably foreseeable misuse (see Art. 3(13)). Also, it requires a risk assessment for the case in which the AI system is used outside of its intended purpose, for example by a malicious actor.</p> <p>Engineering Info</p> <p>This document does not directly reference other articles in the AI Act, since it is mostly out of scope for the engineering practices described in this project.</p>"},{"location":"conformity/risk-management-system/#non-technical-requirements","title":"Non-technical requirements","text":"<p>Even though it is named a \"system\", the risk management system introduced in the article is not necessarily a physical system, but rather a list of actions and provisions. Similarly, it is not meant as a single condition that an AI system must satisfy to be allowed to operate, but rather a continuous loop of evaluation for the AI system throughout its lifecycle.</p> <p>It might be more sensible to interpret this article not as a requirement addressed by a single software system, but rather as a guideline that should be adhered to by all personnel involved in the design and implementation of the AI system, and be applied as a \"continuous iterative process\" (see Art. 9(2)) throughout the project lifecycle.</p> <p>In general, providers must identify, evaluate, and mitigate risks that are known and \"reasonably\" foreseeable. These risks might emerge both when the system is being used as intended and when it being misused in ways that are foreseeable. Finally, providers must also mitigate risks that are identified after the system goes into production.</p> <p>While these concepts may seem vague at first, it is expected that harmonized technical standards will help engineers better understand what types of risks and circumstances they should consider.</p> <p>Since the sources and types of risks of each system are different, it is in general not feasible to automate these risk assessments. For an example for an analysis and a mitigation strategy, please have a look at a specific risk in our showcase.</p>"},{"location":"conformity/technical-documentation/","title":"Technical Documentation","text":"<p>Engineering Info</p> <p>Below we map specific engineering practices and system components from our showcase implementation to demonstrate practical automation possibilities that support EU AI Act compliance.</p> <ul> <li>Experiment Tracking:<ul> <li>Art. 11(1) &amp; Annex IV(2)(g): Logging of validation process, in particular,     characteristics of validation and test datasets</li> </ul> </li> <li>Model Registry:<ul> <li>Art. 11(1) &amp; Annex IV(2)(b): Logging the model architecture and hyperparameters makes it available for documentation</li> </ul> </li> <li>Model Cards:<ul> <li>Art. 11(1) &amp; Annex IV(1)(a,c,g): Documenting the model architecture, version and purpose, together with its user interface</li> </ul> </li> <li>Data Documentation:<ul> <li>Art. 11(1) &amp; Annex IV(2)(d,e): Creating datasheets to describe data characteristics, preprocessing, and refinement procedures</li> </ul> </li> </ul>"},{"location":"conformity/technical-documentation/#motivation","title":"Motivation","text":"<p>Providers of high-risk AI systems are required to prepare detailed technical documentation before placing the system on the market. This documentation must be kept up to date and should demonstrate the system's compliance with the AI Act's requirements. It should include a general description of the AI system, its intended purpose, design specifications, and information on performance evaluation. Small and medium-sized enterprises (SMEs) may provide this information in a simplified form, as specified by the EU.</p> <p>Thorough documentation can greatly improve users' and also practitioners' (i.e. deployers and implementers) understanding of the AI system, and thus make it easier to interact with the system, as well as diagnose any unexpected behavior that is observed during its operation.</p> <p>On the engineering level, documentation can also be a powerful tool to make the evolution of an AI project (and software engineering projects in general) transparent, and to explain architectural and design decisions that, when studied, can be used to onboard new practitioners more quickly, and to re-evaluate central decisions during the project's lifecycle.</p>"},{"location":"conformity/technical-documentation/#non-technical-requirements","title":"Non-technical requirements","text":"<p>There are a number of important features of the AI system that need to be documented as per Annex IV of the AI Act.</p> <p>Firstly, model metadata such as purpose, name of the provider, and versions of software used to run it need to be given (like a Software Bill of Materials or SBOM).</p> <p>Furthermore, the technical aspects of the system must be documented, like model architecture, algorithms used to train it, and human oversight measures to ensure safe operation in production.</p> <p>There are many different ways of writing and distributing technical documentation. In the most bare sense, documentation can be compiled into a single document, and shared with the relevant audiences as a single source of truth about the AI system.</p> <p>On the other hand, it is often advantageous to host documentation on a website, giving access to features like full-text search, styling and text markup, easily accessible tables of contents and document sections, and shared access without having to distribute the documents by hand.</p> <p>While not mandatory, documentation is also a suitable place to keep a record of engineering decisions made during the design and implementation of the AI system. Commonly called architectural decision records or, in a less rigorous version, design documents, such records can serve as a red thread through the development history and lifecycle, and give transparency about the development process to implementers, and optionally users of the system.</p>"},{"location":"conformity/technical-documentation/#annex-iv-technical-documentation-details","title":"Annex IV: Technical Documentation Details","text":"<p>Annex IV provides a comprehensive list of elements that must be included in the technical documentation referred to in Article 11. This includes detailed descriptions of the AI system's design specifications, algorithms, training data sets, risk management systems, validation and testing procedures, performance metrics, and cybersecurity measures. The annex ensures that all relevant information is available to assess the system's compliance with the AI Act.</p>"},{"location":"engineering-practice/","title":"Engineering Practices","text":"Engineering Practice Related AI Act Articles Bias Mitigation Data and Data Governance Accuracy, Robustness and Cybersecurity Containerization Accuracy, Robustness and Cybersecurity Data Quality Data and Data Governance Data Versioning Data and Data Governance Record-Keeping Experiment Tracking Technical Documentation Record-Keeping Transparency and Provision of Information to the Deployers Human Oversight Accuracy, Robustness and Cybersecurity Explainability Technical Documentation Transparency and Provision of Information to the Deployers Human Oversight Inference Log Record-Keeping Model Cards Technical Documentation Transparency and Provision of Information to the Deployers Model Monitoring Human Oversight Accuracy, Robustness and Cybersecurity Model Registry Technical Documentation Transparency and Provision of Information to the Deployers Model Serving Accuracy, Robustness and Cybersecurity Operational Monitoring Transparency and Provision of Information to the Deployers Human Oversight Accuracy, Robustness and Cybersecurity Orchestration Record-Keeping"},{"location":"engineering-practice/containerization/","title":"Containerization","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <ul> <li>Art. 15 (Accuracy, Robustness and Cybersecurity), in particular:<ul> <li>Art. 15(1), containers provide a consistent and isolated runtime environment</li> <li>Art. 15(4), containers allow for redundant, reproducible and reliable application execution</li> <li>Art. 15(5), containers provide an isolated runtime environment that limit the access of attackers and unauthorized third parties to the underlying hardware</li> </ul> </li> </ul>","tags":["Art. 15"]},{"location":"engineering-practice/containerization/#motivation","title":"Motivation","text":"<p>Deploying and running applications in a reproducible and reliable manner is a crucial part of the software development lifecycle. Inconsistencies in the runtime environment can lead to unexpected behavior, bugs, and security vulnerabilities. AI systems prove no exception to this rule, as they often rely on complex dependencies and configurations that can vary across different runtime environments.</p> <p>Containerization provides a solution to these challenges by encapsulating applications and their runtime dependencies into isolated environments, called containers.</p> <p>By controlling the runtime environment, containerization can improve the resilience of AI systems against inconsistencies, as required by Art. 15(4).</p> <p>Since containers provide a uniform execution model, monitoring their lifecycle and performance is easier. Failed containers can be restarted automatically, and their logs can be collected and analyzed to identify potential issues, in line with the fault tolerance requirements of the AI Act.</p> <p>Scaling of AI systems is also simplified with containerization, as containers can be easily replicated and distributed across multiple nodes. This addresses the redundancy requirements of Art. 15(4), as multiple instances of a containerized application can be run in parallel to ensure high availability and fault tolerance.</p>","tags":["Art. 15"]},{"location":"engineering-practice/containerization/#implementation-notes","title":"Implementation Notes","text":"","tags":["Art. 15"]},{"location":"engineering-practice/containerization/#applicability","title":"Applicability","text":"<p>Containerization can be applied at multiple stages of the machine learning lifecycle, including:</p> <ul> <li>MLOps infrastructure: tooling for data versioning, model training, and deployment</li> <li>Data processing: running data pipelines in isolated environments</li> <li>Model training: training models in isolated environments</li> <li>Model serving: deploying models as microservices</li> </ul>","tags":["Art. 15"]},{"location":"engineering-practice/containerization/#traceability-and-reproducibility","title":"Traceability and Reproducibility","text":"<p>Special care should be exercises to ensure the traceability and reproducibility of the containerized applications by always using versioned container images, if possible with immutable version tags or image digests (e.g., <code>my-image:1.0.0</code> or even <code>my-image@sha256:...</code>, instead of <code>my-image:latest</code> or <code>my-image</code>). This practice, akin to version locking for package dependencies, ensures that the same version of the container image is used for each deployment, reducing the risk of inconsistencies, especially in distributed environments.</p> <p>Avoid updating existing tags when building container images, as this can lead to confusion and inconsistencies.</p> <p>Similarly, container builds should be as reproducible as possible. The following techniques can help you to achieve this goal:</p> <ul> <li>Use versioned base images, ideally pinning to a specific image digest.</li> <li>Specify exact versions of dependencies (e.g., by using lock files or version constraints, if supported by your package manager).</li> <li>Avoid using <code>latest</code> tags for dependencies, as they can lead to unexpected changes in behavior.</li> <li>Use build arguments to parameterize the build process, if needed.</li> <li>Use multi-stage builds to separate build and runtime dependencies, if applicable.</li> <li>Use labels/annotations to add metadata to the image, e.g., the source code revision identifier, build date, and maintainers (the OpenContainers annotations spec provides guidance on predefined annotation keys).</li> </ul>","tags":["Art. 15"]},{"location":"engineering-practice/containerization/#security-considerations","title":"Security Considerations","text":"<p>While this document does not cover security aspects of containerization in detail, it is important to consider the following points:</p> <ul> <li>Use minimal base images to reduce the attack surface.</li> <li>Use container vulnerability scanning tools to identify known vulnerabilities in the base images and dependencies.</li> <li>Regularly update base images and dependencies to patch known vulnerabilities.</li> <li>Minimize runtime privileges by using non-root users and limiting container capabilities.</li> </ul>","tags":["Art. 15"]},{"location":"engineering-practice/containerization/#key-technologies","title":"Key Technologies","text":"<p>Container Engines:</p> <ul> <li>Docker</li> <li>Podman</li> </ul> <p>Container Orchestration:</p> <ul> <li>Docker Compose for small-scale deployments</li> <li>Kubernetes or OpenShift for large-scale, multi-node deployments</li> </ul>","tags":["Art. 15"]},{"location":"engineering-practice/containerization/#container-security","title":"Container Security","text":"<p>Note</p> <p>This list only includes open-source software.</p> <p>The field of container security is rapidly evolving, and new tools and techniques are constantly being developed, also as part of commercial solutions.</p> <ul> <li>Snyk Open Source</li> <li>Clair</li> <li>Grype</li> <li>Trivy</li> </ul>","tags":["Art. 15"]},{"location":"engineering-practice/experiment-tracking/","title":"Experiment tracking","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <ul> <li>Art. 11(1) in conjunction with Annex IV (Technical Documentation), in particular:<ul> <li>Annex IV(3): Documentation of accuracy measures on a reference dataset as an expected level of accuracy</li> </ul> </li> <li>Art. 12, in particular:<ul> <li>Art. 12(2)(a): Backtracking to used training data</li> </ul> </li> <li>Art. 13(3)(b)(ii), Art. 15(3) experiment tracking captures used performance metrics and levels of accuracy</li> <li>Art. 14(4)(a), understand the limitation of the underlying model by         interpreting performance on reference data</li> </ul>","tags":["Art. 11","Art. 12","Art. 13","Art. 14","Art. 15"]},{"location":"engineering-practice/experiment-tracking/#motivation","title":"Motivation","text":"<p>AI models are typically trained on large amounts of data, using lots of compute resources. The outputs that a trained model generates depend on many factors, like which version of a dataset was used (for more information, refer to the data versioning section), what configurable attributes (also called hyperparameters) were used for the training, but also training techniques used like batching, which optimization target, and many more.</p> <p>The sum of all choices for the training of an AI model is often called an experiment. Consequently, with so many moving parts and choices in an experiment, extensive documentation is needed to make training workflows transparent to practitioners, decision makers, and users alike. Moreover, tracking all this information is necessary to achieve reproducibility of an experiment.</p> <p>A lot of ML/AI projects therefore use some form of experiment tracking solution to help with visualizing experiments, evaluating model performance, and comparing the performance between different experiments.</p>","tags":["Art. 11","Art. 12","Art. 13","Art. 14","Art. 15"]},{"location":"engineering-practice/experiment-tracking/#implementation-notes","title":"Implementation notes","text":"<p>A vital aspect of experiment tracking software is to diligently mark the outputs (artifacts) of your training workflows to ensure reproducibility and a good overview on the situation. This typically includes, but is not limited to,</p> <ul> <li>versions of training and evaluation data, either raw or pre-processed, ideally in conjunction with data versioning,</li> <li>hyperparameters giving as much information as possible in order to accurately reproduce experiments across platforms,</li> <li>metrics and statistics giving information about the performance of the newly trained model.</li> </ul> <p>Additionally, many experiment trackers contain functionality to directly upload (or \"push\") models and training artifacts to remote storage, establishing a link between written artifacts and model versions for easy accessibility. This aspect is important when setting up a model registry to keep track of the history of your trained models. In general, many experiment trackers can also function as model registries at the same time.</p>","tags":["Art. 11","Art. 12","Art. 13","Art. 14","Art. 15"]},{"location":"engineering-practice/experiment-tracking/#key-technologies","title":"Key technologies","text":"<p>The following is a selection of available tools supporting the practice. It is not meant to be exhaustive and we are not affiliated with any of the listed tools.</p> <ul> <li> <p>MLflow</p> <p>MLflow is an open-source experiment tracking platform that stores data and model artifacts, (hyper)parameters, and visualizes model performance in different stages of the ML training lifecycle. It features a number of pre-configured tracking plugins for popular machine learning libraries called autologgers, which allow the collection of metrics and configuration with minimal setup. In addition, MLflow comes with a UI that can be used to visualize metadata and results across experiments.</p> </li> <li> <p>Weights &amp; Biases</p> <p>Weights &amp; Biases (or WandB) is a managed service for experiment tracking, metrics and metadata logging, and storing model and data artifacts.</p> </li> <li> <p>neptune.ai</p> <p>neptune.ai is another managed experiment tracking service for logging, visualizing, and monitoring metrics both in a training run and across multiple runs. It supports both managed and on-premise deployments, and offers special functionality for large language models (LLMs).</p> </li> <li> <p>ClearML</p> <p>ClearML is an open-source experiment tracking and orchestration platform that allows for the management of experiments, data, and models. It features a web-based UI for visualizing metrics and metadata, and supports integration with popular machine learning libraries.</p> </li> <li> <p>Comet</p> <p>Comet offers a managed experiment tracking service that allows for the logging and visualization of metrics, hyperparameters, and artifacts. It features a web-based UI for visualizing metrics and metadata, and supports integration with popular machine learning libraries.</p> </li> </ul>","tags":["Art. 11","Art. 12","Art. 13","Art. 14","Art. 15"]},{"location":"engineering-practice/explainability/","title":"Explainability","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <ul> <li>Art. 11(1) in conjunction with Annex IV (Technical Documentation), in particular:<ul> <li>Annex IV(3): Explainability techniques can be part of the instructions for use to the deployer</li> </ul> </li> <li>Art. 13 (Transparency and Provision of Information to Deployers), in particular:<ul> <li>Art. 13(1): Explainability techniques contribute to a transparent system operation</li> <li>Art. 13(3)(b)(iv): Explainability techniques directly provide information relevant to explain the system's output</li> <li>Art. 13(3)(d): Explainability techniques can be used in the human oversight process to interpret the system's behavior</li> </ul> </li> <li>Art. 14 (Human Oversight), in particular:<ul> <li>Art. 14(3)(a): automated XAI features can be built directly into     the system to generate explanations</li> <li>Art. 14(4)(c): explainability approaches provide the necessary information     to enable human interpretation of system outputs</li> <li>Art. 14(4)(d): automated explanations provide the basis for humans     to decide whether to use or disregard outputs</li> </ul> </li> <li>Art. 86(3) (Right to Explanation of Individual Decision-Making)</li> </ul>","tags":["Art. 11","Art. 13","Art. 14","Art. 86","Annex IV"]},{"location":"engineering-practice/explainability/#motivation","title":"Motivation","text":"<p>Although there is no explicit demand for using explainability methods within the AI Act, they can aid compliance with the regulation, by providing insights into the system's decision-making process.</p> <p>In particular, these explanation approaches can assist the operator of the system in interpreting the system's behavior, as part of the human oversight process (see Art. 14) and allow affected third parties to request an explanation of the system's output (see Art. 86(3)). As part of the instructions for use to the deployer (see Art. 13(3)(b)(iv)), they can be used to provide information relevant to explain the system's output.</p> <p>Furthermore, specific methods are easily available through existing software packages and utilizing them could be considered as best practice (see the upcoming ISO/IEC DTS 6254 technical specification, which gives an overview of the taxonomy and techniques for explainability).</p>","tags":["Art. 11","Art. 13","Art. 14","Art. 86","Annex IV"]},{"location":"engineering-practice/explainability/#implementation-notes","title":"Implementation Notes","text":"<p>See the showcase for an example how explainability techniques can be integrated into the AI system, with a possibility for users to request explanations for any given model prediction.</p> <p>One important consideration when implementing explainability techniques is way in which the explanations are presented to the user, which has a direct impact on the requirement for human oversight in the AI Act.</p>","tags":["Art. 11","Art. 13","Art. 14","Art. 86","Annex IV"]},{"location":"engineering-practice/explainability/#explainability-techniques","title":"Explainability Techniques","text":"","tags":["Art. 11","Art. 13","Art. 14","Art. 86","Annex IV"]},{"location":"engineering-practice/explainability/#inherently-interpretable-models","title":"Inherently interpretable models","text":"<p>Some machine-learning models and algorithms are inherently interpretable, meaning their prediction outputs can be interpreted in terms of humanly understandable concepts. One such example is the k-nearest-neighbor (KNN) classification algorithm, where the resulting model classifies any datapoint as a weighted sum of its most similar (i.e. \"nearest\", for a specific definition of near) points in a given feature space.</p> <p>Inherent interpretability usually requires a well-established mathematical theory on the problem and the used algorithms in question. Sometimes, this limits the feasibility of such solutions, since \"black-box\" approaches that are not directly interpretable compare favorably in terms of compute performance and accuracy.</p>","tags":["Art. 11","Art. 13","Art. 14","Art. 86","Annex IV"]},{"location":"engineering-practice/explainability/#post-hoc-explanations","title":"Post-hoc explanations","text":"<p>For models that are not directly interpretable (\"black-box\" models) like deep neural networks, gradient boosting algorithms, etc., post-hoc explainability provides a means to evaluate the model decisions after they made a prediction.</p> <p>Post-hoc methods can provide explanations on two distinct levels, global and local. Local methods explain the model's behavior for a single instance, while global methods provide an overview of the model's behavior across an entire dataset.</p> <p>Post-hoc methods are more widely applicable in general, since they do not require direct interpretability of the model. One possible angle of post-hoc explainability is measuring feature importance, which gives an idea about which input features influence the model's decision, and how big their individual influences are (with SHAP being a commonly used technique from this class, see below).</p> <p>A second class of post-hoc explainability methods are surrogate models, which use inherently interpretable models that approximate the behavior of a black-box model in the neighborhood of a given input (one widely used technique is LIME, see below).</p> <p>When offering explanations for predictions to the users of an AI system, it is important to present the values concise and in a sensible way. One such option is to render the results in a visualization (plot), which tends to work well for feature importance scores such as SHAP explanations.</p>","tags":["Art. 11","Art. 13","Art. 14","Art. 86","Annex IV"]},{"location":"engineering-practice/explainability/#empirical-analysis-methods","title":"Empirical analysis methods","text":"<p>Empirical analysis methods are a class of post-hoc explainability techniques that rely on the empirical evaluation of the model's behavior. Techniques like statistical analysis of the model's prediction errors or the response to modifications of the system (e.g., in ablation experiments, where performance is analyzed before and after the removal of a part of the model, such as an input feature) fall into this category.</p> <p>Another empirical technique is the use of challenge datasets, which are datasets that are specifically designed to test the limits of a model's performance on, without the goal of testing the model's generalization performance or being representative of the targeted domain.</p>","tags":["Art. 11","Art. 13","Art. 14","Art. 86","Annex IV"]},{"location":"engineering-practice/explainability/#key-technologies","title":"Key Technologies","text":"<ul> <li>The <code>shap</code> Python package, implements the SHAP (SHapley Additive exPlanations) method</li> <li>The <code>lime</code> Python package, another popular model-agnostic explainability method (Local Interpretable Model-agnostic Explanations)</li> <li>Intrinsically explainable models:<ul> <li>The <code>interpret</code> Python package providing implementations for such glassbox models</li> </ul> </li> <li>The Alibi Explain (<code>alibi</code>) Python package, which provides a collection of black-/white-box, local and global explainability methods</li> </ul>","tags":["Art. 11","Art. 13","Art. 14","Art. 86","Annex IV"]},{"location":"engineering-practice/explainability/#resources","title":"Resources","text":"<ul> <li>As a primer, appliedAI TransferLab series on Explainable AI     and the accompanying training</li> <li>Molnar (2025) - Interpretable Machine Learning, a comprehensive book on interpretability and explainability methods</li> <li>The chapter on explainability in Mucs\u00e1nyi, et al. (2023) - Trustworthy Machine Learning</li> <li> The upcoming revision of the ISO/IEC DTS 6254 standard describes approaches and methods used to achieve explainability objectives.</li> </ul>","tags":["Art. 11","Art. 13","Art. 14","Art. 86","Annex IV"]},{"location":"engineering-practice/inference-log/","title":"Inference Log","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <p>Implementing an inference log will help you in achieving compliance with the following regulations:</p> <ul> <li>Art. 12 (Record-Keeping), in particular:<ul> <li>Art. 12(1), since the inference log enables the recording of events</li> <li>Art. 12(2), since the inference log allows the identification of potentially harmful situations and facilitates the post-market monitoring</li> </ul> </li> <li>Art. 19 (Automatically Generated Logs)</li> <li>Art. 26 (Obligations of Deployers of High-Risk AI Systems), in particular:<ul> <li>Art. 26(5) (Monitoring of the AI system's operation by the deployer)</li> <li>Art. 26(6) (Keeping of system logs by the deployer)</li> </ul> </li> <li>Art. 72 (Post-Market Monitoring)</li> </ul>","tags":["Art. 12"]},{"location":"engineering-practice/inference-log/#motivation","title":"Motivation","text":"<p>An inference log is a permanent record of all inferences made by the AI system, including the input and output data, the model used, and relevant additional metadata.</p> <p>The inference log serves as the basis for monitoring the AI system's operation, ensuring that it behaves as intended and complies with legal and ethical requirements.</p> <p>Logging of inference data should allow for the reconstruction of the AI system's decision-making process, including the input data, the model used, and the output data. This is essential for understanding the AI system's behavior and for identifying and addressing any issues that may arise.</p> <p>In addition to these auditability and traceability requirements, the inference log can also be used for other purposes, such as:</p> <ul> <li>Model performance monitoring: The inference log can be used to track the performance of the AI system over time, allowing for the identification of any degradation in performance or changes in the input data distribution.</li> <li>Model retraining: The inference log can serve a source of data for retraining the AI system, allowing for continuous improvement of the model.</li> </ul>","tags":["Art. 12"]},{"location":"engineering-practice/inference-log/#implementation-notes","title":"Implementation Notes","text":"<p>When it comes to implementing an inference log, there are several key considerations to keep in mind:</p> <ul> <li>Data structure: The inference log should be designed to accommodate the specific data types and structures used in the AI system. This may include JSON or JSONB fields for input and output data, as well as additional metadata. Evolution of the data schema should be considered, as the AI system and its input and outputs may change over time.</li> <li>Data retention: The inference log should be designed to accommodate the data retention requirements of the AI system (e.g., the retention periods set out in Art. 19 of the AI Act, or any other legal or regulatory requirements, such as under the GDPR).</li> <li>Data protection and privacy: Access to the inference log should be restricted to authorized personnel only, and the data should be protected against unauthorized access or tampering. This may include encryption of sensitive data, as well as access controls and audit trails.</li> <li>Performance and scalability: Since every inference made by the AI system will be logged, the inference log should be designed to handle the foreseeable load (both in terms of data rate and volume) and to support efficient querying and analysis. This may include the use of indexing, partitioning, or other techniques to optimize performance.</li> </ul> <p>Especially for LLM applications, a variety of existing tools exist that provide tracing and logging capabilities.</p> <p>See the showcase for an example how to implement and integrate an inference log into an AI system.</p>","tags":["Art. 12"]},{"location":"engineering-practice/inference-log/#key-technologies","title":"Key Technologies","text":"","tags":["Art. 12"]},{"location":"engineering-practice/inference-log/#data-observability","title":"Data Observability","text":"<ul> <li>whylogs, an open-source library for data logging</li> <li>Seldon Core, an open-source platform for deploying and managing machine learning models on Kubernetes, implements a data flow paradigm that facilitates the logging of inference data</li> </ul>","tags":["Art. 12"]},{"location":"engineering-practice/inference-log/#custom-implementation","title":"Custom Implementation","text":"<ul> <li>Any database or storage solution that supports the required data structure<ul> <li>The showcase implementation uses PostgreSQL</li> <li>Other choice include ElasticSearch, MongoDB, or SQLite</li> <li>For high-performance workloads, an event-based architecture using a message broker (e.g., Kafka or RabbitMQ) and a stream processing framework (e.g., Apache Flink or Apache Spark) may be more suitable to asynchronously log the inference data</li> </ul> </li> <li>Open Inference Protocol specification, as a standardized data structure for the input and output data</li> </ul>","tags":["Art. 12"]},{"location":"engineering-practice/inference-log/#llm-tracing-and-observability","title":"LLM Tracing and Observability","text":"<p>Note</p> <p>The field of LLM tracing and observability is rapidly evolving, so this list may not be exhaustive.</p> <ul> <li>MLFlow Tracing, LLM tracing functionality is part of the MLflow platform</li> <li>Langfuse, an open-source LLM engineering platform</li> <li>Langtrace, an open-source LLM observability tool based on the OpenTelemetry standard</li> <li>Langchain Tracing</li> <li>Phoenix, an open-source LLM observability tool, based on OpenTelemetry</li> <li>Tracely by Evidently (see above), a LLM application tracing tool based on OpenTelemetry</li> </ul>","tags":["Art. 12"]},{"location":"engineering-practice/inference-log/#cloud-ml-platforms","title":"Cloud ML Platforms","text":"<ul> <li>Databricks Inference Tables for monitoring models after deployment (and its Azure Databricks counterpart)</li> <li>Evidently provides tracing and dataset logging functionality as part of its paid offering</li> </ul>","tags":["Art. 12"]},{"location":"engineering-practice/model-cards/","title":"Model cards","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <p>Keeping a model card will help you in achieving compliance with the following requirements, since they are well-suited to provide or supplement the required information:</p> <ul> <li>Art. 11(1) in conjunction with Annex IV (Technical Documentation)</li> <li>Art. 13(3) (Transparency and Provision of Information to Deployers)</li> </ul>","tags":["Art. 11","Art. 13","Annex IV"]},{"location":"engineering-practice/model-cards/#rationale","title":"Rationale","text":"<p>Model cards are a somewhat standardized form of documentation that provide a comprehensive overview of an AI model, including its intended use and limitations, used datasets, evaluation results and performance metrics, and ethical considerations. The general structure of a model card encompasses the following sections:</p> <ul> <li>Model name and details</li> <li>Model owners</li> <li>Model architecture and compute infrastructure</li> <li>Intended uses (and potential limitations)</li> <li>Training procedure and parameters</li> <li>Used datasets</li> <li>Evaluation results (datasets, metrics, factors, etc.)</li> <li>Ethical considerations</li> <li>Licenses and compliance information</li> </ul> <p>This information greatly overlaps with the information required for the technical documentation of high-risk AI systems and the necessary information that should be supplied to deployers of such systems as part of the instructions for use.</p> <p>Model cards are a useful tool to increase the transparency along the value chain of an AI system, from developers and providers, to deployers, certification bodies and market authorities, and ultimately end-users.</p>","tags":["Art. 11","Art. 13","Annex IV"]},{"location":"engineering-practice/model-cards/#implementation-notes","title":"Implementation Notes","text":"<p>While no single universal format for model cards exists, the YAML-based format used by Hugging Face is a good starting point. This format strikes a good balance between ease of creation and possibility for automated processing.</p> <p>Parts of the information in a model card can be generated automatically from the model metadata, such as the model's architecture, training data, and evaluation results, using appropriate libraries and tools. Experiment tracking tools, workflow orchestrators, and data versioning tools can serve as the authoritative source for this metadata.</p> <p>Since the information in a model card is tied to a specific model version, it is important to ensure that the model card is appropriately versioned, such that a clear link between a model version and its accompanying model card can be established. This can be achieved by storing the model card as an artifact alongside the model in an experiment log, or by using a version control system to track the model card.</p>","tags":["Art. 11","Art. 13","Annex IV"]},{"location":"engineering-practice/model-cards/#key-technologies","title":"Key Technologies","text":"<ul> <li>Hugging Face Model Cards, which provides a standard YAML format for model cards<ul> <li>Markdown template for Hugging Face model cards</li> </ul> </li> <li>Model Card Toolkit, a Python library for automatic creation of model cards</li> <li>The <code>skops</code> Python library, which can create Hugging Face model cards for scikit-learn models</li> </ul>","tags":["Art. 11","Art. 13","Annex IV"]},{"location":"engineering-practice/model-cards/#resources","title":"Resources","text":"<ul> <li>Mitchell, et al. (2018) - Model Cards for Model Reporting, the original research paper on model cards</li> <li>Ozoani, et al. (2022) - Model Card Guidebook, Hugging Face</li> <li>Model Cards Explained, provides explanations and examples of model cards for various Google AI models</li> </ul>","tags":["Art. 11","Art. 13","Annex IV"]},{"location":"engineering-practice/model-monitoring/","title":"Model Monitoring","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <p>Implementing a model monitoring solution will help you in achieving compliance with the following regulations:</p> <ul> <li>Art. 12 (Record-Keeping), in particular:<ul> <li>Art. 12(1), allows detecting and recording incidents related to model degradation</li> </ul> </li> <li>Art. 14 (Human Oversight), in particular:<ul> <li>Art. 14(4)(a), automated tracking of drift and performance degradation     helps to understand the capacities of the system during its lifetime</li> <li>Art. 14(4)(e), observing degradation overtime enables to intervene and     initialize a retraining of the model</li> </ul> </li> <li>Art. 15 (Accuracy, Robustness and Cybersecurity), in particular:<ul> <li>Art. 15(4), resilience and robustness through continuous monitoring of model performance</li> </ul> </li> <li>Art. 26(5) (Monitoring of the AI system's operation by the deployer), to ensure a model is operating according to the instructions for use</li> <li>Art. 72(2) (Post-market Monitoring), since a model monitoring solution allows for the continuous monitoring of the AI system's performance and compliance with legal requirements.</li> </ul>","tags":["Art. 12","Art. 14","Art. 15","Art. 26","Art. 72"]},{"location":"engineering-practice/model-monitoring/#motivation","title":"Motivation","text":"<p>Monitoring is important to ensure that an AI system continues to operate as expected (that is, safely and reliably) after its deployment to production. In practice, an AI system's performance can degrade over time for a number of reasons:</p> <ul> <li>Covariate shift, where the distribution of inference inputs is sufficiently dissimilar from the training data.</li> <li>Label shift, where the distribution of inference outputs is sufficiently dissimilar from the training data.</li> <li>Concept drift, where the relationship between inference inputs and outputs changes over time.</li> <li>Previously unseen data, which can lead to unexpected behavior of the model.</li> <li>Data quality issues, such as missing or corrupted inference input data.</li> <li>Adversarial attacks, where the model is intentionally abused by malicious actors.</li> <li>Badly calibrated models, which can lead to overconfident predictions, especially in the case of out-of-distribution data.</li> </ul> <p>Identifying these degradations early is key to mitigate accuracy and safety concerns due to inaccuracies of deployed AI systems, especially high-risk systems.</p> <p>Monitoring is the first step in a feedback loop that can be used to trigger retraining of the model or other corrective actions. By setting appropriate thresholds for the monitored metrics, the monitoring system can also be used to trigger alerts when the model's performance drops below an acceptable level.</p> <p>Note that besides the ML-specific aspects, monitoring is also important for the overall system performance, see the page on operational monitoring.</p>","tags":["Art. 12","Art. 14","Art. 15","Art. 26","Art. 72"]},{"location":"engineering-practice/model-monitoring/#implementation-notes","title":"Implementation Notes","text":"<p>See the showcase implementation for a concrete example of how to implement a model monitoring solution using the NannyML library.</p> <p>Model monitoring is a complex task that requires a combination of different techniques and tools. The following diagram illustrates the key components of a model monitoring solution:</p> <pre><code>flowchart LR\n    subgraph monitoring[Model Monitoring]\n        direction LR\n        B[Model Performance Estimation]\n        C[Data Drift Detection]\n        qual[Data Quality Monitoring]\n    end\n\n    A@{ shape: db, label: \"Inference Log\"}\n    A --&gt; monitoring\n    monitoring --&gt; D[Reporting]\n    monitoring --&gt; E[Alerting]\n    monitoring --&gt; F[Model Retraining]\n</code></pre>","tags":["Art. 12","Art. 14","Art. 15","Art. 26","Art. 72"]},{"location":"engineering-practice/model-monitoring/#model-performance-estimation","title":"Model Performance Estimation","text":"<p>For supervised learning applications, the availability of labeled data poses a significant challenge. When collecting previously unseen inference data in production (see the page on inference logging), it is often not possible to collect the corresponding labels (either due to cost limitation of manual labeling or because the true labels are not known until a later point in time).</p> <p>Algorithms that can estimate a model's performance in the absence of ground truth data can help in these scenarios. See the section on model monitoring in the implementation notes for an example of classification performance estimation without ground truth data using the NannyML library.</p>","tags":["Art. 12","Art. 14","Art. 15","Art. 26","Art. 72"]},{"location":"engineering-practice/model-monitoring/#data-drift-detection","title":"Data Drift Detection","text":"<p>Data drift detection is a key component of model monitoring. It allows to identify when the distribution of inference inputs or outputs changes over time, which can lead to degraded performance.</p>","tags":["Art. 12","Art. 14","Art. 15","Art. 26","Art. 72"]},{"location":"engineering-practice/model-monitoring/#data-quality-monitoring","title":"Data Quality Monitoring","text":"<p>Identifying data quality issues is essential for ensuring the reliability of the model's predictions. Data quality monitoring can be used to identify issues such as missing or corrupted data, outliers, and other anomalies in the inference data.</p> <p>Techniques for data quality monitoring include:</p> <ul> <li>Statistical tests, such as the Kolmogorov-Smirnov and chi-squared tests, to identify changes in the distribution of the data.</li> <li>Anomaly detection algorithms, such as isolation forests, to identify outliers in the data.</li> <li>Data validation techniques, such as schema validation, to ensure that the data conforms to the expected format and structure. (see the page on data quality for more details)</li> </ul>","tags":["Art. 12","Art. 14","Art. 15","Art. 26","Art. 72"]},{"location":"engineering-practice/model-monitoring/#reporting","title":"Reporting","text":"<p>The reporting component of a model monitoring solution is responsible for generating overview reports on the model's performance and data drift. These reports can be used to track the model's performance over time and identify trends or anomalies and trigger alerts when the model's performance drops below an acceptable level (see the next section).</p> <p>Reports can be generated on a regular basis (e.g., daily, weekly, monthly) or on-demand, ideally as part of the common workflow orchestration approach.</p>","tags":["Art. 12","Art. 14","Art. 15","Art. 26","Art. 72"]},{"location":"engineering-practice/model-monitoring/#alerting","title":"Alerting","text":"<p>When a model's performance drops below a certain threshold or significant data drift occurs, it is essential to alert the responsible team for further action.</p> <p>The concrete implementation of the alerting system depends on the organization's requirements and the available infrastructure. Examples include sending an email or Slack message, creating a ticket in a ticketing system, or invoking a webhook.</p> <p>Alerting middlewares, such as Prometheus Alertmanager or PagerDuty, can be used to manage alerts and notifications.</p>","tags":["Art. 12","Art. 14","Art. 15","Art. 26","Art. 72"]},{"location":"engineering-practice/model-monitoring/#key-technologies","title":"Key Technologies","text":"<ul> <li>NannyML, used in the showcase implementation</li> <li>Evidently, for model performance and data quality monitoring</li> <li>Alibi Detect, for outlier and drift detection</li> </ul>","tags":["Art. 12","Art. 14","Art. 15","Art. 26","Art. 72"]},{"location":"engineering-practice/model-monitoring/#additional-resources","title":"Additional Resources","text":"<ul> <li>The appliedAI TransferLab workshop on practical anomaly detection and the accompanying material</li> <li>Evidently AI's Model Monitoring in Production guide</li> </ul>","tags":["Art. 12","Art. 14","Art. 15","Art. 26","Art. 72"]},{"location":"engineering-practice/model-registry/","title":"Model registry","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <ul> <li>Art. 11(1) in conjunction with Annex IV (Technical Documentation), in particular:<ul> <li>Annex IV(2)(a): Description of the methods used for development of the system</li> <li>Annex IV(2)(c): Description of the system and overall processing architecture</li> <li>Annex IV(3): Description of the data and model lifecycle management</li> <li>Annex IV(6): References to model versions can help in describing relevant changes to the system through its lifecycle</li> </ul> </li> <li>Art. 13, in particular:<ul> <li>Art. 13(3)(b)(iv): Logging the model architecture and hyperparameters makes the system characteristics transparent</li> </ul> </li> <li>Art. 72 (Post-Market Monitoring by Providers): a model registry can be used to correlate post-market monitoring data with specific model versions</li> </ul>","tags":["Art. 11","Art. 72","Annex IV"]},{"location":"engineering-practice/model-registry/#motivation","title":"Motivation","text":"<p>After training and evaluation, the resulting machine learning models need to be stored for later use (e.g. for deployment in production environments). Frequently, multiple versions of a given model are trained in parallel to assess the influence of certain parameters on the quality of the model outputs.</p> <p>This, together with the requirement for reproducibility in many AI projects, necessitates a solution for managing multiple versions of models side-by-side, that is also highly available, reliable, and can be queried efficiently for any specific version.</p> <p>Model registries are designed to fulfill this need by providing a repository for storing and managing machine learning models, their metadata, and lineage. Advanced functionality can include the ability to manage the lifecycle of models, including versioning, deployment, and monitoring.</p> <p>The model registry fills a crucial role as the bridge between the development and operational phases of the machine learning lifecycle. They API is typically provided to allow for easy integration with other tools and processes, such as continuous deployment pipelines, validation activities, and triggering of workflows in an orchestration system.</p>","tags":["Art. 11","Art. 72","Annex IV"]},{"location":"engineering-practice/model-registry/#implementation-notes","title":"Implementation notes","text":"<p>Capabilities of a model registry can vary quite widely depending on how much of the stated responsibilities are handled by other tools in the practitioner's AI tool stack of choice.</p> <p>In the simplest case, a model registry can just be a local or remote storage bucket, that keeps track of model metadata and versions by indexing them and storing the information in a database or similar systems. If more responsibilities for the AI model lifecycle need to be handled, it is possible to use a tool that combines the storage of artifacts with their deployment, e.g. as a web application or a local assistant type of model.</p> <p>In any case, the model registry should be able to store the model artifacts in a versioned manner, so that it is possible to retrieve any version of a model at any time.</p>","tags":["Art. 11","Art. 72","Annex IV"]},{"location":"engineering-practice/model-registry/#key-technologies","title":"Key Technologies","text":"<p>Since the task of versioning and storing models is tightly coupled to the tracking of metadata and metrics about the model training process, many experiment tracking) solutions come with a builtin model registry that allows for models resulting from an experiment run to be tracked.</p> <p>Additionally, model registry functionality is commonly included in MLOps platforms, which are designed to cover the entire machine learning lifecycle (see the page on workflow orchestration).</p> <p>Besides bespoke model registry solutions, models can also be stored in a suitable versioned data storage system or container registry (for containerized ML models), as long as the accompanying model metadata is stored in a way that allows for easy retrieval and traceability.</p>","tags":["Art. 11","Art. 72","Annex IV"]},{"location":"engineering-practice/model-serving/","title":"Model Serving","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <ul> <li>Art. 15 (Accuracy, Robustness and Cybersecurity), in particular:<ul> <li>Art. 15(4), model serving allows to run inference on models remotely, and to deploy models with redundancy to make them more resilient.</li> </ul> </li> </ul>","tags":["Art. 15"]},{"location":"engineering-practice/model-serving/#motivation","title":"Motivation","text":"<p>Model serving is the process of deploying machine learning models to production environments, where they can be accessed and used by applications or users.</p> <p>Not every AI system involves a real-time inference component, for example batch processing systems or offline analytics. However, for those that do, model serving is a critical part of the machine learning lifecycle.</p> <p>Model serving needs to be designed to ensure that the deployed models are accurate, robust, and secure.</p>","tags":["Art. 15"]},{"location":"engineering-practice/model-serving/#implementation-notes","title":"Implementation Notes","text":"<p>Containerization is a common practice for model serving, as it allows for the deployment of models in isolated environments, ensuring consistency and reproducibility. It also enables the use of different versions of models and dependencies without conflicts.</p> <p>The models to be deployed should be obtained from a model registry, in order to preserve the traceability and reproducibility of the models. A model serving should be able to expose metadata about the model and its provenance, in order to associate this information with every prediction (see the page on inference logs).</p>","tags":["Art. 15"]},{"location":"engineering-practice/model-serving/#inference-api-design","title":"Inference API Design","text":"<p>Real-time inference APIs provide the interface for applications to interact with the deployed models.</p> <p>Designing the API for an interface is a trade-off between flexibility and usability.</p> <p>While a bespoke API can be designed for a single model (which might be tied to a specific input data schema), a more generic API can be designed to support multiple models and input data schemas.</p> <p>One such generic API is specified as the Open Inference Protocol, which is supported by several model serving frameworks. It provides API endpoints and type definitions for inference requests (with a flexible data schema), model metadata, and model management.</p>","tags":["Art. 15"]},{"location":"engineering-practice/model-serving/#key-technologies","title":"Key Technologies","text":"<ul> <li>MLServer</li> <li>BentoML</li> <li>Seldon Core</li> <li>KServe</li> </ul>","tags":["Art. 15"]},{"location":"engineering-practice/operational-monitoring/","title":"Operational Monitoring","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <p>Implementing an operational metrics solution will help you in achieving compliance with the following requirements of the AI Act:</p> <ul> <li>Art. 13 (Transparency and Provision of Information to Deployers), in particular:<ul> <li>Art. 13(3)(e), monitoring the operation of the system enables to provide statistics about the system resource usage</li> </ul> </li> <li>Art. 14 (Human Oversight), in particular:<ul> <li>Art. 14(4)(e), continuous monitoring the operation of the systems helps     to detect conditions requiring potential intervention</li> </ul> </li> <li>Art. 15 (Accuracy, Robustness and Cybersecurity), in particular:<ul> <li>Art. 15(4) robustness, monitoring and alerting can help detect and mitigate potential robustness and availability issues</li> <li>Art. 15(5) cybersecurity, since monitoring is a crucial part of threat detection</li> </ul> </li> <li>Art. 26 (Obligations of Deployers of High-Risk AI Systems), in particular:<ul> <li>Art. 26(5) (Monitoring of the AI system's operation by the deployer)</li> <li>Art. 26(6) (Keeping of system logs by the deployer)</li> </ul> </li> </ul>","tags":["Art. 14","Art. 15","Art. 26"]},{"location":"engineering-practice/operational-monitoring/#motivation","title":"Motivation","text":"<p>Besides monitoring the performance of your machine learning models, it is also important to monitor the performance of the underlying technical infrastructure and services. This includes monitoring the health of the servers, databases, and other components that support your machine learning applications.</p> <p>These operational metrics can give an indication of the overall health of the system and can help you identify potential issues before they become critical. This aligns with the AI Act obligations towards the robustness and cybersecurity of high-risk AI systems.</p>","tags":["Art. 14","Art. 15","Art. 26"]},{"location":"engineering-practice/operational-monitoring/#implementation-notes","title":"Implementation Notes","text":"<p>Relevant metrics to monitor include:</p> <ul> <li>Resource usage: CPU, GPU, memory, disk, network</li> <li>Service availability: uptime, error rates</li> <li>Latency: request/response times</li> <li>Throughput: requests per second</li> <li>Custom metrics: application-specific metrics (e.g., number of processed records, model inference times)</li> </ul> <p>While it is a crucial part of an operational monitoring solution, this page does not cover the topic of alerting. The following activities can provide a starting point to implement an alerting system:</p> <ul> <li>Determining the thresholds for the metrics</li> <li>Defining the escalation process for alerts</li> <li>Setting up the alerting channels (e.g., email, Slack, PagerDuty)</li> <li>Deploying and setting up the alerting system</li> </ul>","tags":["Art. 14","Art. 15","Art. 26"]},{"location":"engineering-practice/operational-monitoring/#key-technologies","title":"Key Technologies","text":"","tags":["Art. 14","Art. 15","Art. 26"]},{"location":"engineering-practice/operational-monitoring/#metrics-collection-and-visualization","title":"Metrics Collection and Visualization","text":"<ul> <li>Prometheus, a time-series database for event and metrics collection, storage, monitoring, and alerting<ul> <li>Many tools and frameworks can expose their operational metrics in the Prometheus format</li> </ul> </li> <li>Grafana, an open-source analytics solution for visualization of metrics</li> <li>The ELK (Elasticsearch, Logstash, Kibana) stack, in particular:<ul> <li>Elasticsearch, a distributed search and analytics engine</li> <li>Kibana, a data visualization and exploration tool for Elasticsearch</li> </ul> </li> </ul>","tags":["Art. 14","Art. 15","Art. 26"]},{"location":"engineering-practice/operational-monitoring/#alerting","title":"Alerting","text":"<ul> <li>Prometheus Alertmanager</li> <li>Grafana Alerting</li> </ul>","tags":["Art. 14","Art. 15","Art. 26"]},{"location":"engineering-practice/orchestration/","title":"Orchestration","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <ul> <li>Art. 10(2)(c): Consistent and versioned data preprocessing operations</li> <li>Art. 11(1) in conjunction with Annex IV (Technical Documentation), in particular:<ul> <li>Annex IV(2)(c): Description of the system and overall processing architecture</li> </ul> </li> <li>Art. 12, in particular:<ul> <li>Art. 12(2)(a): Reconstructing a complete model generation process through usage     of a unique run id</li> </ul> </li> </ul>","tags":["Art. 10","Art. 11","Art. 12"]},{"location":"engineering-practice/orchestration/#motivation","title":"Motivation","text":"<p>The orchestration of machine learning workflows is a critical aspect of the machine learning lifecycle.</p> <p>In essence, it concerns the automated execution of tasks such as data preprocessing, model training, evaluation, and deployment.</p> <p>A workflow orchestrator takes care to execute these tasks in the correct order, manage dependencies between tasks, and handle failures or retries as needed. It keeps a permanent record of the execution history, which is essential for reproducibility, transparency, and auditability.</p>","tags":["Art. 10","Art. 11","Art. 12"]},{"location":"engineering-practice/orchestration/#implementation-notes","title":"Implementation Notes","text":"<p>Which workflow operator to choose depends on the specific requirements for a given project, such as the complexity of the workflows, the scale of the data, and the available infrastructure. Some orchestrators are designed for specific environments (e.g., cloud-native, on-premise), while others are more flexible and can be deployed in various settings.</p> <p>When deciding on a workflow orchestrator, consider the following guiding questions:</p> <ul> <li>Does the orchestrator support the programming language and libraries you are using?<ul> <li>Some orchestrators are designed for specific languages or frameworks (e.g., Python, R, TensorFlow, PyTorch), while others allow you to mix and match different languages and libraries, even within a single workflow.</li> <li>Some orchestrators can delegate specific task types to other platforms (e.g., running an ETL job on Snowflake).</li> </ul> </li> <li>Does it enable seamless transition between development and production environments?<ul> <li>Does it support local development and testing of workflows?</li> <li>Can workflows execute both in a cloud environment and on-premise?</li> </ul> </li> <li>Does it provide built-in support for conditional execution or dynamic workflows (i.e., adding or removing subtasks based on conditions)?</li> <li>Does it support cached execution results to prevent redundant computation, either on-device and remotely?</li> <li>How does it handle task dependencies and execution order?</li> <li>How does it handle resource management?<ul> <li>Most importantly for machine learning workloads, how does it manage GPU resources?</li> </ul> </li> <li>How can you access the execution history and logs for debugging and auditing purposes?</li> <li>Does it provide built-in support for monitoring and alerting on workflow execution?</li> <li>If required by your organization, does it support role-based access control (RBAC) and other security features?</li> </ul> <p>These questions are somewhat orthogonal to the requirements of the AI Act. In essence, basically any orchestrator that allows for consistent execution of workflows and maintains a permanent record of the execution history can be used to comply with the requirements of the AI Act.</p>","tags":["Art. 10","Art. 11","Art. 12"]},{"location":"engineering-practice/orchestration/#key-technologies","title":"Key Technologies","text":"<p>Note</p> <p>Workflow orchestration tools come in various shapes and sizes, from lightweight libraries to full-fledged platforms.</p> <p>The following list is not exhaustive, but it provides a good starting point for exploring the available options and their respective features and tradeoffs.</p>","tags":["Art. 10","Art. 11","Art. 12"]},{"location":"engineering-practice/orchestration/#workflow-orchestrators","title":"Workflow Orchestrators","text":"<ul> <li>Dagster</li> <li>Apache Airflow</li> <li>Argo Workflows</li> <li>Prefect</li> <li>Flyte</li> </ul>","tags":["Art. 10","Art. 11","Art. 12"]},{"location":"engineering-practice/orchestration/#mlops-platforms","title":"MLOps Platforms","text":"<p>While the above orchestrators can be used for general-purpose workflow orchestration, there are also platforms specifically designed for machine learning operations (MLOps) that provide additional features and integrations tailored to the needs of ML workflows, not limited to just orchestration:</p> <ul> <li>Kubeflow</li> <li>Metaflow</li> <li>ZenML</li> <li>Kedro</li> <li>TFX</li> </ul>","tags":["Art. 10","Art. 11","Art. 12"]},{"location":"engineering-practice/orchestration/#platform-software-as-a-service-offerings","title":"Platform-/Software-as-a-Service Offerings","text":"<p>If you are already using a cloud provider, you may want to consider their ML pipeline orchestration products:</p> <ul> <li>AWS SageMaker Pipelines</li> <li>Azure Machine Learning Pipelines</li> <li>Google Vertex AI Pipelines</li> </ul> <p>Or, on a lower level, you can use the workflow orchestration services provided by the cloud providers:</p> <ul> <li>AWS Step Functions</li> <li>Azure Data Factory</li> <li>Google Cloud Workflows</li> <li>Databricks Workflows</li> </ul>","tags":["Art. 10","Art. 11","Art. 12"]},{"location":"engineering-practice/data-governance/","title":"Data Governance","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <ul> <li>Article 10</li> </ul> <p></p> <p>In general, data governance encompasses all methodologies for managing data throughout its entire lifecycle. With respect to the AI Act, Article 10 mandates that datasets used in developing high-risk AI systems must be of high quality, relevant, representative, free from bias, and appropriately documented to ensure fairness, accuracy, and reliability. While the article outlines certain required practices, it lacks a comprehensive definition of data governance and data management.</p> <p>To enhance implementation clarity, we distinguish between components suitable for automation (engineering practices, described in this section) and those centered on process and documentation, see Technical Documentation. This is not a simple mapping of paragraphs to either or, moreover each paragraph of Art. 10 can include both types of tasks.</p> <p>Specific to bias. Refer to the page on bias mitigation for more information.</p> <ol> <li>For the development of high-risk AI systems not using techniques involving the training of AI models, paragraphs 2 to 5 apply only to the testing data sets.</li> </ol> <p>This paragraph addresses high-risk AI systems that are developed without using techniques involving the training of AI models. These systems might rely on alternative approaches, such as rule-based systems, hard-coded algorithms, or pre-existing models that do not require additional training or updates to their parameters.</p> <ul> <li>Data Versioning</li> <li>Data Quality</li> </ul>"},{"location":"engineering-practice/data-governance/bias-mitigation/","title":"Bias mitigation","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <ul> <li>Art. 10 (Data and Data Governance), in particular:<ul> <li>Art. 10(2)(g): Appropriate measures for bias detection, prevention, and mitigation</li> <li>Art. 10(3): Assert appropriate statistical properties regarding natural persons related to the use of the high-risk AI system</li> <li>Art. 10(5): Use of special categories of personal data in bias detection and correction</li> </ul> </li> <li>Art. 15 (Accuracy, Robustness and Cybersecurity), in particular:<ul> <li>Art. 15(4), bias mitigation increases resilience against bias-related errors and inconsistencies</li> </ul> </li> </ul>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/bias-mitigation/#motivation","title":"Motivation","text":"<p>Biases are commonly considered one of the most detrimental effects of artificial intelligence (AI) use.</p> <p>Art. 10 mandates the examination, detection, prevention, and mitigation of biases present in the data that could result in a harmful impact to health, safety, or fundamental rights.</p> <p>As such, data governance activities should include practices to cover these requirements and map them to activities in the machine learning lifecycle.</p>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/bias-mitigation/#implementation-notes","title":"Implementation Notes","text":"Taxonomy of common causes of bias in machine learning. Source: Mitigating Bias in Machine Learning","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/bias-mitigation/#bias-and-fairness-analysis-techniques","title":"Bias and Fairness Analysis Techniques","text":"<ul> <li>Conduct Exploratory Data Analysis (EDA): Analyze the dataset for imbalances or patterns that may suggest bias, such as over-representation or under-representation of certain groups.</li> <li>Fairness Metrics: Calculate fairness metrics such as demographic parity/disparate impact, equal opportunity, or equalized odds to quantify bias in datasets and model outputs.</li> <li> <p>See fairlearn documentation for an introduction to commonly used metrics</p> </li> <li> <p>Diversity Analysis: Evaluate the dataset's demographic diversity, ensuring it represents all relevant populations appropriately.</p> <ul> <li>Group Representation:     Checks whether groups are proportionally represented in the dataset (calculate as fraction of the total dataset size)</li> <li>Overall Accuracy Equality:     Ensures that accuracy rates are equal across groups.</li> </ul> </li> </ul>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/bias-mitigation/#mitigation-techniques","title":"Mitigation Techniques","text":"<p>Bias mitigation techniques broadly fall into three categories, based on their applicability during the machine learning lifecycle:</p> <pre><code>flowchart LR\n    assessment[Bias Assessment]\n    audit[Auditing]\n\n    subgraph mitigation[Bias Mitigation]\n        direction TB\n\n        preproc[Preprocessing Techniques]\n        inproc[Inprocessing Techniques / Model Training]\n        postproc[Postprocessing Techniques]\n\n        preproc --&gt; inproc\n        inproc --&gt; postproc\n    end\n\n    assessment --&gt; mitigation\n    mitigation --&gt; audit</code></pre>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/bias-mitigation/#preprocessing-techniques","title":"Preprocessing Techniques","text":"<p>The goal of preprocessing techniques is to adjust the dataset before training the model to ensure a fair representation of all demographic groups in the training data.</p> <ul> <li>Resampling: Use oversampling or undersampling to balance the representation of different demographic groups.</li> <li>Synthetic data generation: Generate synthetic examples for under-represented groups to ensure better balance in the dataset.</li> <li>Reweighing: Adjust the weights of data instances to ensure fair representation across groups.</li> </ul>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/bias-mitigation/#inprocessing-techniques-model-training","title":"Inprocessing Techniques / Model Training","text":"<ul> <li>Bias-Corrected Features: Transform features to reduce correlations with sensitive attributes (e.g., gender, race).</li> <li>Fair Representations: Use fairness-aware models that explicitly optimize for fairness metrics alongside predictive accuracy.</li> </ul>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/bias-mitigation/#post-processing-techniques","title":"Post-processing Techniques","text":"<ul> <li>Outcome Adjustments: Adjust decision thresholds or outputs to ensure equitable outcomes across demographic groups.<ul> <li>Equalized Odds Postprocessing: Modifies predictions to satisfy equalized odds constraints.</li> </ul> </li> <li>Bias Mitigation Strategies: Apply fairness postprocessing methods, such as calibration by group or equalized odds adjustments.</li> </ul>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/bias-mitigation/#auditing","title":"Auditing","text":"<p>Regularly audit data to ensure it is free from systemic errors or biases.</p> <ul> <li>Segmentation Analysis: Partition the dataset based on sensitive attributes and assess performance metrics for each segment to detect disparities.</li> <li>Subgroup Fairness Checks: Compare outcomes for different demographic subgroups to identify discrepancies.</li> <li>Drift Detection: Use tools to detect data or model drift that may reintroduce bias over time.</li> </ul>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/bias-mitigation/#privacy-concerns","title":"Privacy Concerns","text":"<p>Art. 10(5) makes an exception that allow the use of special categories of personal data for bias detection and correction.</p> <p>When using this sensitive data, it is crucial to ensure that the data is handled in compliance with applicable data protection regulations, such as GDPR. The purpose for which the data is used and its scope must be clearly defined and documented. In particular, this includes deleting the sensitive data after the bias correction process is completed.</p> <p>Keep an audit trail of the data processing activities, including the use of sensitive data for bias detection and correction. A workflow orchestration tool can support this requirement by providing a clear record of the data processing steps.</p>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/bias-mitigation/#key-technologies","title":"Key Technologies","text":"<ul> <li>AI Fairness 360 (<code>aif360</code>) by IBM</li> <li>Fairlearn by Microsoft</li> <li>What-if Tool by Google</li> <li><code>imblearn</code> Python package for scikit-learn, provides tools when dealing with classification with imbalanced classes.</li> </ul>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/bias-mitigation/#further-reading","title":"Further Reading","text":"<ul> <li>European Parliamentary Research Service (2022) - Auditing the quality of datasets used in algorithmic decision-making systems, Study</li> <li>Caton, Haas (2024) - Fairness in Machine Learning: A Survey</li> <li>Barocas, Hardt, Narayanan (2023) - Fairness and Machine Learning: Limitations and Opportunities</li> <li>Mitigating Bias in Machine Learning</li> </ul>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/data-quality/","title":"Data quality","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <ul> <li>Art. 10 (Data and Data Governance), in particular:<ul> <li>Art. 10(2)(c)</li> <li>Art. 10(3)</li> </ul> </li> </ul>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/data-quality/#motivation","title":"Motivation","text":"<p>Art. 10(3) of the AI Act demands a certain quality of data used for training and evaluating models, in particular these data sets should be:</p> <ul> <li>relevant,</li> <li>sufficiently representative</li> <li>complete, and</li> <li>free of errors.</li> </ul> <p>To achieve those qualities, there are different techniques available at different steps in the system lifecycle.</p>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/data-quality/#implementation-notes","title":"Implementation Notes","text":"<p>Note that the techniques discussed in the section focus on technical approaches for ensuring data quality. They need to be accompanied by organizational and governance measures to become fully effective.</p>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/data-quality/#data-preprocessing","title":"Data Preprocessing","text":"<ul> <li>Detect and handle missing or incomplete data<ul> <li>Conduct data analysis to identify missing fields.</li> <li>Use statistical methods to assess if missing data skews results.</li> <li>Implement appropriate handling (e.g., interpolation, mean/mode imputation).</li> </ul> </li> <li>Perform data consistency checks<ul> <li>Enforce data schema for tabular data.</li> <li>Identify and remove duplicate records.</li> <li>Ensure data formats are consistent (e.g., all dates in correct format).</li> <li>Check for missing values and determine handling strategies (imputation or removal).</li> </ul> </li> <li>Keep preprocessing consistent, versioned and reproducible<ul> <li>Avoid manual processing steps, rely on data pipelines in a workflow orchestrator instead</li> </ul> </li> </ul>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/data-quality/#data-quality-validation","title":"Data Quality Validation","text":"<ul> <li>Validate data against ground truth<ul> <li>Cross-check a sample of the dataset against verified real-world sources or domain experts.</li> </ul> </li> <li>Ensure data accuracy through automated validation<ul> <li>Logical inconsistencies (e.g., negative age values).</li> <li>Outliers and anomalies using statistical methods (e.g., z-score, IQR analysis).</li> </ul> </li> <li>Produce automated data quality reports for human review and inclusion in technical documentation.</li> <li>Monitor for data drift over time<ul> <li>Set up periodic validation checks to see if the data distribution changes over time.</li> <li>Retrain models if significant drift is detected.</li> </ul> </li> </ul>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/data-quality/#key-technologies","title":"Key Technologies","text":"<ul> <li>Pandas<ul> <li>Other dataframe libraries with similar features exist, e.g., Polars, Spark <code>DataFrame</code>s</li> </ul> </li> <li>Pandera, for data quality validation</li> <li>Great Expectations / GX Core, for data quality validation</li> </ul>","tags":["Art. 10"]},{"location":"engineering-practice/data-governance/data-versioning/","title":"Data versioning","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <ul> <li>Art. 10 (Data and Data Governance), in particular:<ul> <li>Art. 10(2)(b): tracking the origin of data,</li> <li>Art. 10(2)(e): assessment of availability, quantity and suitability of the data sets, and</li> </ul> </li> </ul>","tags":["phase::data engineering","phase::modeling"]},{"location":"engineering-practice/data-governance/data-versioning/#motivation","title":"Motivation","text":"<p>In fast-moving, complex environments such as the subject of AI, it is vital to keep track of where data and information originated, not only to be transparent, or to easily share work with colleagues, but to be able to identify the root cause in the event of a problem with your system.</p> <p>In the software engineering world, distributed version control systems (VCS) have seen wide adoption because they address all of these concerns. Through a set of basic abstractions, they provide bookkeeping powers by means of unique and immutable references, distributed storage for work sharing, and a graph-based history building for detailed information keeping. Consequently, data version control or data versioning systems can be thought of as the similar approach to all data artifacts produced by your machine learning systems.</p> <p>In addition, a data version control system should come with security features such as role-based access control (RBAC) and restrict access to data to authorized personnel and systems only. This is a requirement of Art. 15(5) (measures to prevent manipulation of the training data set).</p>","tags":["phase::data engineering","phase::modeling"]},{"location":"engineering-practice/data-governance/data-versioning/#implementation-notes","title":"Implementation notes","text":"<p>To add data versioning to a machine learning project, it is important to figure out a few steps:</p> <ol> <li>Collaborative development.</li> </ol> <p>A suitable data version control system needs to accommodate multiple engineers working simultaneously on different versions of the data, and ensure that changes made by one engineer do not invalidate the work of another. This can be done for example by using a branch workflow, where each person has their own siloed copy of the data, and can make changes to it without changing the canonical version (the \"main\" branch in this model).</p> <pre><code>%%{ init: {'theme': 'base'} }%%\n---\ntitle: Branching in a (data) version control system\n---\ngitGraph\n   commit\n   commit\n   branch develop\n   checkout develop\n   commit\n   commit\n   checkout main\n   merge develop\n   commit\n   commit</code></pre> A branching data version control approach, with commits (immutable snapshots of the data) shown as dots. <ol> <li>Distributed storage.</li> </ol> <p>A data version control system should be accessible for all developers, and host the data in an off-device location to ensure easy access, fault tolerance, and availability of backups.</p> <ol> <li>Communication with users.</li> </ol> <p>In addition to availability, security, and fault tolerance, it is necessary that the data version control system can be easily interfaced with in AI application code. This usually means that a selection of API clients or SDKs is available for a variety of programming languages, which allows developers to efficiently interface with the data VCS.</p> <ol> <li>Security.</li> </ol> <p>A data version control system should implement role-based access control (RBAC) and restrict access to data to authorized personnel and systems only.</p>","tags":["phase::data engineering","phase::modeling"]},{"location":"engineering-practice/data-governance/data-versioning/#interoperability","title":"Interoperability","text":"<ul> <li>Where applicable, ensure that datasets and documentation are interoperable with standard regulatory frameworks and can be audited by authorities.<ul> <li>Standardize formats (e.g., CSV or Parquet files for tabular data)</li> <li>Centralize data storage (e.g., in a data lake or data warehouse) such that access to data sets can be given at any time, and it is not depending on data sets on local machines of ML Engineers and Data Scientists.</li> </ul> </li> </ul>","tags":["phase::data engineering","phase::modeling"]},{"location":"engineering-practice/data-governance/data-versioning/#usage-considerations","title":"Usage considerations","text":"<p>While data version control and \"regular\" version control sound similar, and the general operational model is the same, data versioning systems face unique challenges. Most importantly, data assets can become very large in size, which makes storing entire data sets for each revision infeasible because of duplication, unless very small data sets are used. Furthermore, data comes in lots of different formats, both structured (tables) or unstructured (raw text, images, videos), which makes it harder to test for changes (commonly called \"diffing\"). As such, it is a hard problem to design a data version control system capable of efficiently computing changes between data sets, while keeping the storage footprint of all data set diffs small.</p> <p>In general, the following considerations can help you decide how to make good use of your data version control system:</p> <ul> <li>You should version data whenever changes to the dataset occur that could impact its use, reproducibility, or compatibility with downstream systems.</li> <li>Use version control abstractions to keep versions separate between different developers, different experiments, or both. For example, for lakeFS (see below), use branches to separate.</li> <li>As a rule of thumb, create versions when initially uploading data, when updating (new data, corrections, imputation), or after applying preprocessing steps (evaluate based on compute vs. storage requirements).</li> <li>For other reasons (experimentation on a new model, feature engineering, subsampling, ...), data versions can also be created to simplify bookkeeping, and to provide a clear relation between different use cases and their data.</li> <li>Use scheduled jobs (garbage collection) to remove stale versions, free up storage space, and declutter the version control view.</li> <li>When available, use auditing and annotation features to attach metadata to revisions, enabling you to keep track of modifications and their authors over time.</li> </ul>","tags":["phase::data engineering","phase::modeling"]},{"location":"engineering-practice/data-governance/data-versioning/#key-technologies","title":"Key technologies","text":"<p>The following is a selection of available tools supporting the practice. It is not meant to be exhaustive and we are not affiliated with any of the listed tools.</p> <ol> <li>lakeFS</li> </ol> <p>lakeFS models a lot of its version control abstractions like the git VCS tool. It builds a linear history on commits, separates different avenues of work with branches, allows annotating data with tags, and also has supported for a merge workflow. It can be set up on either local storage or a cloud storage bucket of one of the larger providers (AWS, GCP, Azure).</p> <ol> <li>DVC</li> </ol> <p>DVC is a pure-Python command-line interface (CLI) for versioning data and model artifacts resulting from ML pipelines. In addition, it has functionality for local experiment tracking, and versioning data pipelines in git together with project source code.</p> <ol> <li>git-lfs</li> </ol> <p>Git Large File Storage (LFS) stores large data assets in remote locations, and replaces the data with text pointers in git, so that data and its usage are decoupled on the storage level. It integrates with the git command-line interface, augmenting developers' existing git workflows intuitively, and reducing the learning curve that an addition of a standalone tool would bring.</p>","tags":["phase::data engineering","phase::modeling"]},{"location":"engineering-practice/data-governance/documentation/","title":"Data documentation","text":"<p>Compliance Info</p> <p>Below we map the engineering practice to articles of the AI Act, which benefit from following the practice.</p> <ul> <li>Art. 10 (Data and Data Governance), in particular:<ul> <li>Art. 10(2), clear and structured documentation of data sets supports other data governance practices</li> </ul> </li> <li>Art. 11(1) in conjunction with Annex IV (Technical Documentation), in particular:<ul> <li>Annex IV(2)(d), datasheets for training data sets are explicitly mentioned</li> <li>Annex IV(2)(g), validation and test data sets should be documented and characterized</li> </ul> </li> <li>Art. 13 (Transparency and Provision of Information to Deployers), in particular:<ul> <li>Art. 13(3)(b)(vi), training, validation, and test data sets should be appropriately documented</li> </ul> </li> </ul>","tags":["Art. 10","Art. 11","Art. 13","Annex IV"]},{"location":"engineering-practice/data-governance/documentation/#rationale","title":"Rationale","text":"<p>As part of the overall data governance strategy, data documentation is a key practice to ensure that data sets are well understood and properly managed.</p> <p>This includes documenting the purpose of the data, its sources, curation method, its structure, and any transformations or processing that have been applied to it. Consideration of limitations, potential biases, and ethical implications of the data is also important.</p>","tags":["Art. 10","Art. 11","Art. 13","Annex IV"]},{"location":"engineering-practice/data-governance/documentation/#implementation-notes","title":"Implementation Notes","text":"<p>Most data documentation methodologies (see below) provide predefined templates or guidelines for documenting data sets, in order to ensure consistent application of these practices.</p> <p>Data documentation should be treated as a living artifact, and as such should be versioned appropriately (e.g., using a version control system like Git). Plain text formats like Markdown are well-suited for this purpose, as they are easy to read and edit, and can be easily converted to other formats (e.g., HTML or PDF) for publication.</p>","tags":["Art. 10","Art. 11","Art. 13","Annex IV"]},{"location":"engineering-practice/data-governance/documentation/#key-technologies","title":"Key Technologies","text":"<p>Several approaches and formats exist for documenting data sets, including:</p> <ul> <li>Datasheets for Datasets, proposed in Gebru, et al. (2021) - Datasheets for Datasets</li> <li>Data Cards, proposed in Pushkarna, et al. (2022) - Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI</li> <li>Data Statements</li> <li>Dataset Nutrition Labels, proposed in Holland, et al. (2018) - Data Nutrition Labels: A Framework to Drive Higher Data Quality Standards</li> </ul> <p>While these approaches differ in their concrete structure and content, they all aim to provide a comprehensive overview of the data set, including its purpose, sources, curation methods, and any limitations or ethical considerations, in line with the requirements for technical documentation in Annex IV.</p>","tags":["Art. 10","Art. 11","Art. 13","Annex IV"]},{"location":"engineering-practice/data-governance/documentation/#resources","title":"Resources","text":"<ul> <li>Section on data-focused documentation tools in the Hugging Face Landscape of ML Documentation Tools, provides an overview of existing data documentation methodologies</li> </ul>","tags":["Art. 10","Art. 11","Art. 13","Annex IV"]},{"location":"showcase/","title":"AI-Powered HR Assistant","text":"<p>Disclaimer</p> <p>This is a fictional use case created solely for demonstration and educational purposes. It is not a real deployment and should not be interpreted as a commercial or operational system. It does not represent any real recruitment system or policy and should not be used for making real-world hiring decisions.</p>"},{"location":"showcase/#use-case-salary-band-prediction-for-recruitment","title":"Use Case: Salary Band Prediction for Recruitment","text":""},{"location":"showcase/#business-context","title":"Business Context","text":"<p>HR professionals in large organizations often face the challenge of managing a high volume of applications for a single job opening. Many of these applicants are not considered suitable candidates for the role. Clearly, an efficient screening process is needed to identify the most suitable candidates quickly.</p> <p>This showcase presents a fictional HR Assistant system that demonstrates how AI can support this process while ensuring fairness, transparency, and compliance with regulations. The system we describe uses a machine learning model to screen and filter candidates. It predicts an applicant's expected (current) salary based on demographic and other attributes and matches them to predefined salary bands for a given job position.</p> <p>The underlying assumption is that candidates whose expected current income is far above or below the range for a given position are unlikely to be a good fit. Essentially, the system treats expected current income as a proxy for a candidate's suitability for a given position. However, note that this assumption is highly simplified and does not account for many real-world factors, such as career changes, relocation, or personal circumstances. Therefore, it should not be used as the sole criterion for hiring decisions.</p> <p>Recruiters can use this categorization to quickly identify candidates who meet the salary expectations for a position, enabling them to focus on the most relevant applications.</p> <p>Keep in mind that this approach is not suitable for actual use in hiring processes, as it may introduce biases and ethical concerns. The system is intended for educational purposes only, by demonstrating how to develop a high-risk AI system under the EU AI Act by adhering to engineering best practices, including fairness, transparency, and explainability.</p>"},{"location":"showcase/#intended-purpose","title":"Intended Purpose","text":"<p>The AI system is designed to:</p> <ul> <li>Predict (current) income for job candidates based on their demographic and professional attributes</li> <li>Assist recruiters by providing data-driven salary band recommendations</li> <li>Streamline applicant screening by matching applicants to predefined salary bands for a job position</li> </ul> <p>The risk classification shows that a system like this is classified as a high-risk system under the EU AI Act.</p> <p>The page on risk assessment provides an abridged analysis of the potential risks posed by the system, and outlines mitigation strategies to address them.</p>"},{"location":"showcase/#core-capabilities","title":"Core Capabilities","text":"<ol> <li> <p>Income Class Prediction</p> <ul> <li>Predicts a candidate's likely current income</li> <li>Uses multiple income band categories</li> <li>Provides confidence scores for predictions</li> </ul> </li> <li> <p>Explainable Predictions</p> <ul> <li>SHAP-based explanations for individual predictions</li> <li>Feature importance visualization</li> <li>Transparency into which factors most influence the prediction</li> </ul> </li> <li> <p>API-Based Integration</p> <ul> <li>RESTful API for easy integration with existing HR systems</li> <li>Batch prediction capabilities for processing multiple candidates</li> </ul> </li> <li> <p>Monitoring</p> <ul> <li>Continuous monitoring of model performance</li> <li>Continuous monitoring of system operation</li> </ul> </li> </ol> <p>There is a detailed overview of the system's architecture.</p>"},{"location":"showcase/#dataset-us-census-current-population-survey-cps","title":"Dataset: US Census Current Population Survey (CPS)","text":"<p>The model is trained on data from the US Census Bureau's Current Population Survey, specifically the 2024 version of the Annual Social and Economic Supplement (ASEC) data.</p>"},{"location":"showcase/#key-data-characteristics","title":"Key Data Characteristics","text":"<ul> <li>Source: Official US government census data (publicly available)</li> <li>Time Period: Recent annual data (updated yearly)</li> <li>Sample Size: Approximately 50,000 households</li> </ul>"},{"location":"showcase/#target-variable-salary-bands","title":"Target Variable: Salary Bands","text":"<p>The model predicts configurable salary bands (e.g., &lt;30K, 30-50K, 50-75K, 75-100K, &gt;100K) rather than exact salaries.</p>"},{"location":"showcase/#explore-further","title":"Explore Further","text":"<ul> <li>Risk Classification - EU AI Act compliance analysis</li> <li>Risk Assessment - Detailed risk assessment report</li> <li>System Overview - High-level overview of the system architecture</li> <li>Code README - Setup and running instructions</li> </ul> <p>Resources:</p> <ul> <li>CPS ASEC Documentation - Official census documentation</li> <li>Retiring Adult: New Datasets for Fair Machine Learning - Discusses limitations of older datasets and introduces better alternatives</li> </ul>"},{"location":"showcase/risk-assessment/","title":"Risk Assessment","text":"<p>This example of bias and discrimination risk is provided to give a first impression of what a risk analysis under Article 9 might entail. It is not an exhaustive analysis in the sense of Article 9, which would require a comprehensive, systematic identification and assessment of all risks to health, safety, and fundamental rights throughout the AI system's lifecycle.</p> <p>Risk of Bias and Discrimination</p> <ul> <li>Reasoning: A classification model that predicts salary bands can inadvertently learn biased patterns from historical or non-representative data. If the training dataset underrepresents certain groups or if it carries historical inequalities, the model might systematically discriminate. This could result in unfair exclusion of qualified candidates from certain demographic or socioeconomic backgrounds.</li> <li>Mitigation Strategies:<ol> <li>Data Quality and Diversity: Ensure the dataset is representative of different demographic groups. Perform data audits to identify and remove or mitigate biases.</li> <li>Bias Detection and Monitoring: Use fairness metrics (e.g., disparate impact ratio, equalized odds) to assess and continually monitor the model\u2019s outputs across different groups.</li> <li>Fairness-Aware Algorithms: Employ techniques such as reweighing or adversarial debiasing to reduce or eliminate learned biases.</li> <li>Human Review: Maintain a human-in-the-loop for critical decisions, ensuring that final filtering choices are not made solely by the AI.</li> </ol> </li> </ul> <p>In this sense, the risk management system must be a living process that extends beyond initial risk identification. In particular, high-risk AI systems must undergo continuous testing throughout their lifecycle, with mandatory testing before productization and release to end users (see Sections (6) to (8)).</p> <p>We highlight that the risk management system as presented in Article 9 is not directly tied to, but in fact a requirement for a lot of the engineering practices showcased in this repository, since they require prior knowledge about the relevant risks of the project.</p>"},{"location":"showcase/risk-classification/","title":"Risk Classification","text":"<p>This page outlines the risk classification process for the AI system used in the HR assistant showcase, according to the requirements of the EU AI Act.</p>"},{"location":"showcase/risk-classification/#risk-classification-process-for-the-showcase-ai-system","title":"Risk Classification Process for the Showcase AI System","text":"<ul> <li>Determine if the system is considered AI under the EU AI Act (see Article 3(1))<ul> <li>Reasoning: The system uses machine learning methods (a classification model) to estimate candidates\u2019 likely income. Machine learning is explicitly covered by the definition of AI in the EU AI Act.</li> <li>Outcome: Yes, this system is within scope because it qualifies as an AI system.</li> </ul> </li> <li>Check whether the AI system falls outside the scope of the AI Act (see Article 2)<ul> <li>Context: Under certain circumstances, some AI systems and operators are excluded from the regulatory requirements. This includes, for example, if the system is used exclusively for military purposes or for scientific research.</li> <li>Reasoning: This recruitment system is a commercial application used by recruiters and hiring managers, so it does not qualify for exemption.</li> <li>Outcome: The system is not exempt; it is still in scope.</li> </ul> </li> <li>Determine if the AI system poses an \u2018unacceptable risk\u2019<ul> <li>Reasoning: Under Article 5 of the EU AI Act, eight use cases are deemed to pose an \u2018unacceptable risk\u2019. This includes for example, systems that use manipulative or exploitative practices, social scoring, or emotion recognition in the workplace. A recruitment filter based on estimated salary ranges does not meet the criteria listed in Article 5.</li> <li>Outcome: The system does not pose an \u2018unacceptable risk.\u2019</li> </ul> </li> <li> <p>Assess whether the AI system is considered \u2018high-risk\u2019</p> <ul> <li>Reasoning: The EU AI Act (in Annex III, point 4) lists AI systems used in the employment and management of workers (including to \"filter job applications\") as high-risk. This system directly assists with filtering and selecting candidates for roles, thereby influencing employment opportunities.</li> <li>Outcome: The system falls into the high-risk category.</li> </ul> </li> <li> <p>Assess if additional transparency obligations apply (see Article 50)</p> <ul> <li>Reasoning: The system does not directly interact with humans nor does it generate synthetic content.</li> <li>Outcome: The system does not have additional transparency obligations.</li> </ul> </li> <li> <p>Assess if the system poses 'minimal risk'</p> <ul> <li>Reasoning: Since we have already determined it is high risk due to its recruitment use case, this consideration does not apply.</li> <li>Outcome: Classification remains high-risk.</li> </ul> </li> </ul>"},{"location":"showcase/risk-classification/#final-outcome","title":"Final Outcome","text":"<p>Based on the criteria outlined in the AI Act, the proposed HR assistant system qualifies as a high-risk system. Consequently, the system is subject to additional legal and compliance obligations, specifically those in Chapter III of the AI Act. These obligation span product safety requirements like risk management, data governance, documentation and traceability, transparency requirements, and human oversight. The remainder of the showcase will demonstrate how to implement these requirements in practice, using engineering best practices and tools.</p> <p>Continue reading the page on risk assessment to see an example on how to analyze and mitigate the risks associated with this system.</p>"},{"location":"showcase/system-overview/","title":"System overview","text":"<ul> <li>System Components<ul> <li>Data Layer (lakeFS + MinIO)</li> <li>Model Training Pipeline (Dagster)</li> <li>Model Registry and Tracking (MLflow)</li> <li>Deployment Infrastructure (Docker + MLServer)</li> <li>Business application (FastAPI)</li> <li>Inference Logging</li> <li>Post-deployment Monitoring</li> </ul> </li> </ul>"},{"location":"showcase/system-overview/#system-components","title":"System Components","text":""},{"location":"showcase/system-overview/#data-layer-lakefs-minio","title":"Data Layer (lakeFS + MinIO)","text":"<p>To keep track of data artifacts across the machine learning lifecycle, we use lakeFS, a distributed data version control solution. That way, we can clearly separate different versions of data, and annotate them with metadata and information, which helps to create a clear overview of the project. We choose to run lakeFS on top of MinIO, an S3-compatible object store, since that can also be used to store other artifacts than data, like trained models. It can also be deployed locally, which means this example can be run self-contained without the need for cloud infrastructure.</p> <ul> <li>Raw data is ingested and version-controlled using lakeFS.</li> <li>Processed data is also stored and versioned after preprocessing.</li> <li>MLflow artifacts (e.g. model binaries) and raw files are stored in MinIO.</li> </ul>"},{"location":"showcase/system-overview/#model-training-pipeline-dagster","title":"Model Training Pipeline (Dagster)","text":"<p>We use a machine learning pipeline approach to orchestrate the different steps (data loading, preprocessing, model training, performance evaluation, ...) as tasks in a workflow. In addition to a graph view, we get some more benefits from workflow orchestration, among them better observability (logging), scheduled jobs, caching steps and outputs, and a dashboard for powerful visualizations of pipeline runs. We each assume the data processing, model training, and serving/deployment to be independent parts of the machine learning lifecycle, and model each of them as one pipeline.</p> <p>The showcase uses Dagster as a workflow orchestration engine. Its focus on data assets makes it well suited for machine learning workflows.</p> <p>The data processing pipeline handles the following steps:</p> <ul> <li>Data preprocessing and cleaning</li> <li>Feature engineering</li> </ul> <p>The model training pipeline contains the following steps:</p> <ul> <li>Computing a train/test split</li> <li>Automatic hyperparameter search</li> <li>Logging experiment details (hyperparameters, metrics, models) to MLflow</li> </ul> <p>The serving pipeline contains these steps:</p> <ul> <li>Versioning and packaging of deployments into container images</li> <li>Giving fairness information on the model</li> <li>Preparing auxiliary models and datasets (using NannyML) to analyze performance for data drift, degradations, and error rates.</li> </ul>"},{"location":"showcase/system-overview/#model-registry-and-tracking-mlflow","title":"Model Registry and Tracking (MLflow)","text":"<p>To get an overview of all model training runs and their outcomes, we use MLflow, an open-source experiment tracking platform. It also allows us to keep track of our model artifacts in a versioned registry, providing a direct link between runs and their resulting models. In this showcase, MLflow uses MinIO for persistent storage of artifacts.</p> <p>MLflow covers the following concerns:</p> <ul> <li>Tracking of:<ul> <li>Training environment</li> <li>Model (hyper-)parameters</li> <li>Accuracy &amp; fairness metrics</li> </ul> </li> <li>A central model artifact registry</li> </ul>"},{"location":"showcase/system-overview/#deployment-infrastructure-docker-mlserver","title":"Deployment Infrastructure (Docker + MLServer)","text":"<p>To deploy our models we use container images with MLserver, an open-source machine learning inference server with standardized REST/gRPC APIs. MLServer is, at the very basic level, a combination of runtime and server components, the former of which is a modular implementation of a model serving for a specific ML framework, like scikit-learn and xgboost. The server component contains functionality to host multiple versions of the same model kind, as well as the REST/gRPC interface implementing the Open Inference Protocol.</p> <p>Additionally, it enables the following features for our serving pipeline:</p> <ul> <li>Creating a scalable inference server for a model.</li> <li>Trained models are packaged as container images and then deployed to provide a scalable inference endpoint, allowing real-time access for prediction.</li> <li>The endpoint supports the output of SHAP values to indicate fairness evaluation of predictions through a custom MLserver runtime. See the implementation notes for more details.</li> </ul>"},{"location":"showcase/system-overview/#business-application-fastapi","title":"Business application (FastAPI)","text":"<p>The business logic of the HR assistant system in the showcase is implemented as a REST API service built with FastAPI. In order to make predictions, this service calls the aforementioned inference server, and handles additional tasks around the inference request: In order to fulfill the record-keeping requirements of the AI Act, all inference requests and responses are logged to a database (see below). In addition to that, the FastAPI application provides functionality to explain any of its decisions to the user, by showing how each feature of the input data contributes to their predicted salary band. This supports the right to explanation of the system's decision making as required by the AI Act for certain classes of AI systems.</p> <p>The most important aspects of the API:</p> <ul> <li>Accepting incoming candidate data for inference.</li> <li>Sending data to the inference endpoint, either as a single or batch request.</li> <li>Returning the predicted salary band and explanation of the prediction (i.e. top contributing features). See the implementation notes for more details.</li> </ul>"},{"location":"showcase/system-overview/#inference-logging","title":"Inference Logging","text":"<p>Due to record-keeping provisions in the AI Act, the system logs all inference requests and responses to ensure traceability and an overview of failures and errors throughout its lifetime. In this example, all inference requests and responses are logged into an PostgreSQL database. This includes input features, output predictions, SHAP values explaining the output through feature contributions, and metadata.</p> <p>The implementation notes describe the data structure of the inference log and its integration with the rest of the AI system in detail.</p> <p>This inference log also serves a secondary purpose by enabling post-deployment monitoring, as described in the next section.</p>"},{"location":"showcase/system-overview/#post-deployment-monitoring","title":"Post-deployment Monitoring","text":"<p>After deployment, we use an observability tool stack to keep track of model performance, data drift, and operational metrics like HTTP errors or request latencies. This stack consists of Grafana, Prometheus, and NannyML, where the former two are responsible for streaming logs and metrics, and for visualizing them, respectively. NannyML is our data drift detection and model performance estimation tool of choice, allowing us to alert on triggers set to mark unacceptable performance losses.</p> <p>To summarize, below are the most important points of our post-deployment monitoring service:</p> <ul> <li>NannyML monitors data drift, model degradation, and performance drops post-deployment by analyzing logs and inference data.</li> <li>Prometheus collects operational metrics (e.g., latency, error rates, throughput).</li> <li>Grafana displays these metrics in real time on an operational dashboard for observability.</li> </ul>"},{"location":"showcase/implementation-notes/","title":"Implementation Notes","text":"<p>This section discusses the interesting aspects of the implementation of the showcase system.</p> <p>The following topics are covered:</p> <ul> <li>Inference Logging</li> <li>Explainability</li> <li>Model Monitoring</li> <li>Bias Detection and Mitigation</li> </ul>"},{"location":"showcase/implementation-notes/bias/","title":"Bias Detection and Mitigation","text":"<p>Note</p> <p>While the dataset used in the showcase system contains multiple protected characteristics in the context of employment access (e.g., race, sex, age, disability status, ...), the implementation focuses on detecting and mitigating bias related to sex for demonstration purposes.</p> <p>Setting appropriate thresholds and metrics for detection of unacceptable bias is a complex task that requires careful consideration of the specific context and the potential impact of the model's predictions.</p>"},{"location":"showcase/implementation-notes/bias/#bias-detection","title":"Bias Detection","text":"<p>Two main types of bias are considered in the showcase system:</p> <ul> <li>bias inherent to the dataset, which refers to the potential biases present in the training data that can lead to unfair predictions;</li> <li>bias stemming from the ML model, which refers to the potential biases introduced by the model itself during training or inference.</li> </ul> <p>Various metrics are calculated to assess the fairness of the model's predictions. These metrics are logged to the MLflow experiment log during training, in order to be able to assess the model's fairness across different versions.</p> <p>Two Python packages are used to calculate the relevant metrics:</p> <ul> <li>Fairlearn: a toolkit for assessing and mitigating unfairness in machine learning models.</li> <li>AIF360: a comprehensive toolkit for detecting and mitigating bias in AI systems.</li> </ul>"},{"location":"showcase/implementation-notes/bias/#bias-mitigation","title":"Bias Mitigation","text":"<p>As an example for a bias mitigation technique, the showcase system implements a correlation removal pre-processing step. This pre-processing transformation is implemented using the Fairlearn library.</p> <p>In line with the risk assessment for the showcase, the <code>SEX</code> feature is identified as a sensitive attribute that should be considered for bias mitigation.</p> <p>The mitigation is applied to the training data before training the model, and the model is then trained on the mitigated dataset. This helps to ensure that the model does not learn biased patterns from the training data, leading to fairer predictions, at the expense of slightly worse model accuracy.</p> <p>The following plots compare the feature importance of the <code>SEX</code> feature for models that have been trained on the original dataset and on the mitigated dataset (see the page on explainability for more details on the SHAP approach). The plots on the left show the feature importance for the unmitigated dataset, while the plots on the right show the feature importance for the mitigated dataset.</p> Unmitigated dataset Mitigated dataset <p>We can see that the mitigation reduced the importance of the <code>SEX</code> feature, which can be seen in the smaller range of the SHAP values in the violin plot for this feature. The bar plot shows that the overall importance of the <code>SEX</code> feature is also reduced so far that it is no longer among the top 10 most important features in the model.</p>"},{"location":"showcase/implementation-notes/explainability/","title":"Explainability","text":"<p>Our showcase implementation focusses on a single explainability technique, the SHAP (SHapley Additive exPlanations) approach. This game-theoretic approach allows for post-hoc explanations in the form of highlighting each input feature's contribution to the model prediction.</p>"},{"location":"showcase/implementation-notes/explainability/#custom-mlserver-runtime-with-explainability","title":"Custom MLServer Runtime with Explainability","text":"<p>This section describes the exemplary implementation of a system that automatically generates explanations for every prediction made by the underlying model. A custom inference server runtime implementation in MLserver is used to attach these SHAP explanations to every model prediction.</p> <p>This design of predictions linked with explanations ensures that every prediction made by the system can be explained at a later point in time, without the need for the original input data or model revision at the time of the explanation. Contrast this with an implementation that requires access to the deployed model revision when an explanation is requested. In such a case, the model revision would need to be available (or deployed on demand) to generate the explanation post-hoc.</p>"},{"location":"showcase/implementation-notes/explainability/#explainability-api-in-the-application","title":"Explainability API in the Application","text":"<p>The AI system exposes an API endpoint (<code>/model/explain</code>) that allows a user to request explanations for a given model prediction. Predictions are identified by their request ID, which is automatically attached to the system's response in the <code>X-Request-ID</code> HTTP header.</p> <p>For a given inference request ID, the system retrieves the corresponding input, output, and explanation data from the inference log, and returns a visual representation of the SHAP explanation for the prediction to the user:</p> <p></p>"},{"location":"showcase/implementation-notes/inference-logging/","title":"Inference Logging","text":""},{"location":"showcase/implementation-notes/inference-logging/#database-schema","title":"Database Schema","text":"<p>The database schema for the inference log should include tables for storing the following information:</p> <ul> <li>Requests: Information about the inference requests, including the input data, model used, and timestamp.</li> <li>Responses: Information about the inference responses, including the output data and timestamp.</li> <li>Errors: Information about any errors that occurred during inference, including the error message and the response data.</li> <li>Metadata: Additional metadata about the inference requests and responses.</li> </ul> <p>In order to allow for a variety of data types and structures, the input and output data should be stored as JSON or JSONB fields. The data types from the standardized Open Inference Protocol v2 REST API specification can be used as a reference for the structure of the input and output data. Using a standardized data structure will make it easier to integrate the inference log with other components of the AI system, such as the model performance monitoring component.</p> <pre><code>erDiagram\n    REQUESTS {\n        TEXT id PK\n        TIMESTAMPTZ timestamp\n        JSONB parameters\n        JSONB inputs\n        JSONB outputs\n        JSONB raw_request\n    }\n\n    RESPONSES {\n        TEXT id PK, FK\n        TEXT model_name\n        TEXT model_version\n        TIMESTAMPTZ timestamp\n        JSONB parameters\n        JSONB outputs\n        JSONB raw_response\n    }\n\n    ERRORS {\n        TEXT id PK, FK\n        TEXT error\n        JSONB response\n    }\n\n    METADATA {\n        TEXT id PK, FK\n        JSONB metadata\n    }\n\n    REQUESTS ||--|| RESPONSES : \"has\"\n    REQUESTS ||--|| ERRORS : \"has\"\n    REQUESTS ||--|| METADATA : \"has\"</code></pre> <p>Choose a database or storage solution that supports the required data structure and provides the necessary performance and scalability characteristics for the AI system's workload. Additionally, consider the data retention requirements of Art. 19 and the need for data protection and privacy (e.g., interactions with GDPR) when selecting a log storage solution.</p>"},{"location":"showcase/implementation-notes/inference-logging/#fastapi-application-middleware","title":"FastAPI Application Middleware","text":"<p>In order to ensure that all inference requests are automatically logged, the inference log should be implemented as a middleware component in the AI system's application code.</p> <p>The middleware should intercept all incoming inference requests, log the relevant data, and then pass the request on to the model (inference server) for processing.</p> <p>When using the FastAPI, the inference log can be injected as a dependency into the application's route handlers or other dependencies. By further encapsulating the interface to the model itself in another dependency, the inference log can be easily integrated into the application's request handling pipeline.</p> <pre><code>flowchart TD\n    A[Route Handler] --&gt; B[Model Inference Dependency] --&gt; C[Inference Log Dependency]</code></pre>"},{"location":"showcase/implementation-notes/model-monitoring/","title":"Model Monitoring","text":""},{"location":"showcase/implementation-notes/model-monitoring/#model-performance-estimation","title":"Model Performance Estimation","text":"<p>The NannyML library implements two approaches to model performance estimation:</p> <ul> <li>Confidence-based performance estimation (CBPE) for binary and multiclass classification tasks</li> <li>Direct Loss Estimation (DLE) for regression tasks</li> </ul> <p>Since the ML problem for the showcase is a classification task, the CBPE approach is applied.</p>"},{"location":"showcase/implementation-notes/model-monitoring/#data-drift-detection","title":"Data Drift Detection","text":"<p>NannyML implements a variety of data drift detection algorithms for univariate and multivariate data.</p> <p>TODO: How do we use the data drift detection algorithms in the showcase?</p>"},{"location":"showcase/implementation-notes/model-monitoring/#reporting","title":"Reporting","text":"<p>Both model performance estimation and data drift detection algorithm in NannyML operate on chunks of model predictions. Therefore, it is straightforward to generate periodic reports on the model's performance as part of a monitoring workflow.</p> <p>The showcase implements a simple reporting mechanism that builds a containerized monitoring dashboard of the reported performance metrics based on the prediction stored in the inference log.</p> <p>Similarly, these reports could be implemented as a scheduled workflow that generates reports on a daily, weekly, or monthly basis.</p>"}]}