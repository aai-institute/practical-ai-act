The introduction of the EU AI Act adds a new dimension of requirements to the implementation of high-risk artificial intelligence systems.
At first glance, the legal text may not clearly outline how these compliance requirements affect the practical aspects of AI system development.

This project aims to bridge that gap by offering valuable insights and practical examples to help accelerate the integration of compliance requirements into real-world projects.
It is designed as an entry point for professionals approaching the AI Act from a technical perspective. A key message we want to convey is that adhering to software engineering best practices already provides a strong foundation for achieving compliance.

To support this goal, the project will present an exemplary system architecture for a trustworthy AI system, offer concrete implementations of essential software components, and establish clear connections between engineering best practices and the specific requirements for high-risk systems under the AI Act.


## Intended Audiences

This project is intended for AI practitioners, data scientists, and engineers who are interested in implementing trustworthy AI systems under the European AI Act.

## Recommended Knowledge

-   A basic understanding of machine learning and AI concepts
-   Familiarity with Python programming
-   An understanding of the terminology in |Art. 3| of the AI Act
-   Software engineering best practices for ML:
    -   See the [Beyond Jupyter series](https://transferlab.ai/trainings/beyond-jupyter/) for an introduction

!!! warning "TODO"

    Out of scope:

    - legal advice
    - all-encompassing reference architecture
    - LLMs
    - GPAI-specific requirements under Chapter V (roadmap?), maybe link to AI Office [Code of Practice](https://digital-strategy.ec.europa.eu/en/policies/ai-code-practice)
    - machine learning theory and best practice (algorithm and software design), link beyond jupyter?


!!! warning "Roadmap"

    collect open ends
